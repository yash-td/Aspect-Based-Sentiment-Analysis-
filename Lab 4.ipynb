{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hExKCzh6doIW"
      },
      "source": [
        "# Lab 4 - Aspect-Based Sentiment Analysis\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HixoFOoCIJ7V"
      },
      "source": [
        "In this session, we demonstrate how to deal with the aspect-based sentiment analysis (ABSA). You can find the whole task description from (https://aclanthology.org/D19-1654.pdf).\n",
        "This task provides a review text dataset with aspect.\n",
        "Given a review and an aspect, we need to classify the sentiment conveyed towards that aspect on a  three-point scale:   POSITIVE, NEUTRAL, and NEGATIVE.\n",
        "This is a multi-class classification task, and it needs to analyze the text and its aspect. \n",
        "\n",
        "Same as before, we are going to use Keras Sequential API in this session. The Sequential API allows you to make models layer-by-layer. You could modify the previous models to fit in the new task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "m8fpBfhBpupy"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "from keras.layers import Lambda, GlobalAveragePooling1D, Dense, Embedding\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import LSTM, RNN, Dropout, Input, LeakyReLU, Bidirectional,Conv1D, GlobalMaxPooling1D\n",
        "from keras.layers.core import Dense\n",
        "from keras.models import Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqvPQvgvPv1W"
      },
      "source": [
        "### Downloading and preprocessing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EundMtGPpCdf"
      },
      "source": [
        "Unlike the IMDB dataset that is included and preprocessed by the Keras, the dataset we will be using is the aspect-term sentiment analysis (ATSA) dataset, which consists of 5297 labeled reviews. These are split into 4,297 reviews for training and 500 reviews for testing and validation, respectively. \n",
        "\n",
        "For ATSA, the annotators extract aspect terms in the sentences and label the sentiment polarities with respect to the  aspect  terms.   The  sentences  that  consist  of only one aspect term or multiple aspects with the same  sentiment  polarities  are  deleted.  ATSA also provides the start and end positions in a sentence for each aspect term."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "P27sPFg7f0CJ"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "def downloadfile(url):\n",
        "  rq = requests.get(url)\n",
        "  open(url.split('/')[-1], 'wb').write(rq.content)\n",
        "downloadfile('https://raw.githubusercontent.com/siat-nlp/MAMS-for-ABSA/master/data/MAMS-ATSA/raw/train.xml')\n",
        "downloadfile('https://raw.githubusercontent.com/siat-nlp/MAMS-for-ABSA/master/data/MAMS-ATSA/raw/val.xml')\n",
        "downloadfile('https://raw.githubusercontent.com/siat-nlp/MAMS-for-ABSA/master/data/MAMS-ATSA/raw/test.xml')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3Voc6S_gT2X",
        "outputId": "2bed1de7-d4c7-4567-e73c-4694e166915e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training entries: 11186\n",
            "Test entries: 1336\n"
          ]
        }
      ],
      "source": [
        "# The code is modified from https://raw.githubusercontent.com/siat-nlp/MAMS-for-ABSA/master/data_process/utils.py\n",
        "from xml.etree.ElementTree import parse\n",
        "\n",
        "def parse_sentence_term(path, lowercase=False):\n",
        "    tree = parse(path)\n",
        "    sentences = tree.getroot()\n",
        "    data = []\n",
        "    split_char = '__split__'\n",
        "    for sentence in sentences:\n",
        "        text = sentence.find('text')\n",
        "        if text is None:\n",
        "            continue\n",
        "        text = text.text\n",
        "        if lowercase:\n",
        "            text = text.lower()\n",
        "        aspectTerms = sentence.find('aspectTerms')\n",
        "        if aspectTerms is None:\n",
        "            continue\n",
        "        for aspectTerm in aspectTerms:\n",
        "            term = aspectTerm.get('term')\n",
        "            if lowercase:\n",
        "                term = term.lower()\n",
        "            polarity = aspectTerm.get('polarity')\n",
        "            start = aspectTerm.get('from')\n",
        "            end = aspectTerm.get('to')\n",
        "            piece = [text , term,  polarity , start , end]\n",
        "            data.append(piece)\n",
        "    return data\n",
        "train = parse_sentence_term(\"train.xml\",True)\n",
        "val = parse_sentence_term(\"val.xml\",True)\n",
        "test = parse_sentence_term(\"test.xml\",True)\n",
        "\n",
        "print(\"Training entries: {}\".format(len(train)))\n",
        "print(\"Test entries: {}\".format(len(test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6U4iCV9-rmay"
      },
      "source": [
        "We now can start playing around with the data, letâ€™s first see some examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-gjWRAuqg5s",
        "outputId": "8d39eabb-169f-4f72-b22d-c010b5ca4221"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SENTENCE \t ASPECT \t LABEL \t ASPECT-START-INDEX \t ASPECT-END-INDEX\n",
            "['the decor is not special at all but their food and amazing prices make up for it.', 'decor', 'negative', '4', '9']\n",
            "['the decor is not special at all but their food and amazing prices make up for it.', 'food', 'positive', '42', '46']\n",
            "['the decor is not special at all but their food and amazing prices make up for it.', 'prices', 'positive', '59', '65']\n",
            "['when tables opened up, the manager sat another party before us.', 'tables', 'neutral', '5', '11']\n",
            "['when tables opened up, the manager sat another party before us.', 'manager', 'negative', '27', '34']\n"
          ]
        }
      ],
      "source": [
        "print(\"SENTENCE \\t ASPECT \\t LABEL \\t ASPECT-START-INDEX \\t ASPECT-END-INDEX\")\n",
        "print(train[0])\n",
        "print(train[1])\n",
        "print(train[2])\n",
        "print(train[3])\n",
        "print(train[4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTRZrpcyr-4x"
      },
      "source": [
        "We could use this dataset to try an \"unknown aspect\" task, if we assume that the ASPECT, LABEL and START/END-INDEX fields are what the model must predict. But here we will attempt a simpler \"known aspect\" task: we will assume that we know ASPECT and START/END-INDEX and the model must just predict the LABEL for a given combination of aspect and sentence.\n",
        "\n",
        "First, build a vocabulary based on the training dataset. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79Ev72Kgq4XL",
        "outputId": "991f1cb3-3f84-4787-e509-6a60be0bf463"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7894\n",
            "7898\n"
          ]
        }
      ],
      "source": [
        "voc = []\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "for example in train:\n",
        "  text_tokens = text_to_word_sequence(example[0])\n",
        "  aspect_tokens = text_to_word_sequence(example[1])\n",
        "  voc.extend(aspect_tokens)\n",
        "  voc.extend(text_tokens)\n",
        "voc = sorted(set(voc))\n",
        "print(len(voc))\n",
        "\n",
        "word_index = dict()\n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNK>\"] = 2  \n",
        "word_index[\"<EOS>\"] = 3\n",
        "for w in voc:\n",
        "  word_index[w] = len(word_index)\n",
        "print(len(word_index))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tvuu4KhStqei"
      },
      "source": [
        "According to the word_index and the tokenizer function (text_to_word_sequence), we can convert the review text and aspect words to word tokens and integers separately:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMK5BPq8Yb3c",
        "outputId": "c3e7264f-4447-46d8-cacb-e0d135c1d504"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1926"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "word_index['decor']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMCH1OoDrSNR",
        "outputId": "c1cff2e7-2777-4593-c816-24f95873d551"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train_review[0]:\n",
            "['the', 'decor', 'is', 'not', 'special', 'at', 'all', 'but', 'their', 'food', 'and', 'amazing', 'prices', 'make', 'up', 'for', 'it']\n",
            "x_train_aspect[0]:\n",
            "['decor']\n",
            "x_train_review_int[0]:\n",
            "[7049, 1926, 3655, 4655, 6516, 564, 330, 1079, 7053, 2802, 393, 369, 5364, 4150, 7437, 2812, 3665]\n",
            "x_train_aspect_int[0]:\n",
            "1926\n"
          ]
        }
      ],
      "source": [
        "# Please write your code to generate the following data\n",
        "x_train_review = [text_to_word_sequence(w[0]) for w in train]\n",
        "x_train_aspect = [text_to_word_sequence(w[1]) for w in train]\n",
        "\n",
        "#x_train_review_int = [word_index.get(w[0],2) for w in train]\n",
        "x_train_review_int = [[word_index.get(word,2) for word in line] for line in x_train_review]\n",
        "\n",
        "x_train_aspect_int = [word_index.get(w[1],2) for w in train] #update by \n",
        "x_dev_review = [text_to_word_sequence(w[0]) for w in val]\n",
        "x_dev_aspect = [text_to_word_sequence(w[1]) for w in val]\n",
        "x_dev_review_int = [[word_index.get(word,2) for word in line] for line in x_dev_review]\n",
        "x_dev_aspect_int = [word_index.get(w[1],2) for w in val]\n",
        "\n",
        "x_test_review = [text_to_word_sequence(w[0]) for w in test]\n",
        "x_test_aspect = [text_to_word_sequence(w[1]) for w in test]\n",
        "x_test_review_int = [[word_index.get(word,2) for word in line] for line in x_test_review]\n",
        "x_test_aspect_int = [word_index.get(w[0],2) for w in test]\n",
        "\n",
        "\n",
        "# your code goes here:\n",
        "\n",
        "\n",
        "\n",
        "# If use the previous word_index, you can get a print result like:\n",
        "assert len(x_train_aspect) == len(train)\n",
        "assert len(x_train_aspect) == len(x_train_aspect_int)\n",
        "assert len(x_test_aspect) == len(test)\n",
        "assert len(x_test_aspect) == len(x_test_aspect_int)\n",
        "print(\"x_train_review[0]:\")\n",
        "print(x_train_review[0])\n",
        "print(\"x_train_aspect[0]:\")\n",
        "print(x_train_aspect[0])\n",
        "print(\"x_train_review_int[0]:\")\n",
        "print(x_train_review_int[0])\n",
        "print(\"x_train_aspect_int[0]:\")\n",
        "print(x_train_aspect_int[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IreFXgruZot"
      },
      "source": [
        "We use 4 to represent \"positive\", 2 for \"neutral\", and 1 for \"negative\". Then we can convert the lables to numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abIb7Fe5u3GQ",
        "outputId": "bef67a07-247b-40ca-b580-be493f62d615"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 1]\n",
            "[1 0 0]\n",
            "[1 0 0]\n",
            "[0 1 0]\n",
            "[0 0 1]\n"
          ]
        }
      ],
      "source": [
        "def label2int(dataset):\n",
        "  y = []\n",
        "  for example in dataset:\n",
        "    if example[2].lower() == \"negative\":\n",
        "      y.append([0,0,1])\n",
        "    elif example[2].lower() == \"neutral\":\n",
        "      y.append([0,1,0])\n",
        "    else:\n",
        "      # assert example[2].lower() == \"positive\"\n",
        "      y.append([1,0,0])\n",
        "  return y\n",
        "  \n",
        "y_train = label2int(train)\n",
        "y_dev = label2int(val)\n",
        "y_test = label2int(test)\n",
        "y_train = np.array(y_train)\n",
        "y_dev = np.array(y_dev)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "print(y_train[0])\n",
        "print(y_train[1])\n",
        "print(y_train[2])\n",
        "print(y_train[3])\n",
        "print(y_train[4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TnnSuspvC5b"
      },
      "source": [
        "Now we have almost done the data preprocessing. Unlike the previous lab, there are two x (review and aspect) to input the model in here. The easiest way is to combine the review and aspect into one sentence and then input it into the model. Thus we can use the previous model directly.\n",
        "\n",
        "(This means our model is similar to a simplified version of the Vo & Zhang model from the lectures: we have an input sequence containing an aspect embedding paired with the sentence word embeddings (but not separating into left & right sentence context as Vo & Zhang do)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKOiVVXQu-_I",
        "outputId": "befb7887-1639-44a6-f2fe-eff287132758"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before paded:\n",
            "['decor', '<START>', 'the', 'decor', 'is', 'not', 'special', 'at', 'all', 'but', 'their', 'food', 'and', 'amazing', 'prices', 'make', 'up', 'for', 'it']\n",
            "[1926, 1, 7049, 1926, 3655, 4655, 6516, 564, 330, 1079, 7053, 2802, 393, 369, 5364, 4150, 7437, 2812, 3665]\n",
            "After paded:\n",
            "[1926    1 7049 1926 3655 4655 6516  564  330 1079 7053 2802  393  369\n",
            " 5364 4150 7437 2812 3665    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n"
          ]
        }
      ],
      "source": [
        "# Please write your code to combine the x_*_review and x_*_aspect into the following varibles\n",
        "x_train = []\n",
        "x_train_int = []\n",
        "x_dev = []\n",
        "x_dev_int = []\n",
        "x_test = []\n",
        "x_test_int = []\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "max_len = 128\n",
        "\n",
        "for i in range(len(x_train_review)):\n",
        "  temp = (x_train_aspect[i] + ['<START>'] + x_train_review[i])\n",
        "  x_train.append(temp)\n",
        "  x_train_int.append([word_index.get(w,2) for w in temp])\n",
        "\n",
        "\n",
        "x_train_pad = pad_sequences(x_train_int, maxlen=max_len, padding='post')\n",
        "\n",
        "for i in range(len(x_dev_review)):\n",
        "  temp = (x_dev_aspect[i] + ['<START>'] + x_dev_review[i])\n",
        "  x_dev.append(temp)\n",
        "  x_dev_int.append([word_index.get(w,2) for w in temp])\n",
        "  \n",
        "x_dev_pad = pad_sequences(x_dev_int, maxlen=max_len, padding='post')\n",
        "\n",
        "\n",
        "for i in range(len(x_test_review)):\n",
        "  temp = (x_test_aspect[i] + ['<START>'] + x_test_review[i])\n",
        "  x_test.append(temp)\n",
        "  x_test_int.append([word_index.get(w,2) for w in temp])\n",
        "  \n",
        "x_test_pad = pad_sequences(x_test_int, maxlen=max_len, padding='post')\n",
        "\n",
        "\n",
        "\n",
        "# Tips: \n",
        "# 1) We can use the special token <START> to concatenate the reviews and aspects.\n",
        "# 2) After combine them, do not foget to pad the sequences.\n",
        "\n",
        "\n",
        "# Don't forget the to use np.array function to wrap the ouput of pad_sequences function, such as: x_train_pad = np.array(x_train_pad)\n",
        "# Only pad the *_int varibles\n",
        "print(\"Before paded:\")\n",
        "print(x_train[0])\n",
        "print(x_train_int[0])\n",
        "print(\"After paded:\")\n",
        "print(x_train_pad[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7OwOQw4h8RX"
      },
      "source": [
        "#Model 1: Previous models without pre-trained word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CuAHDGQ3Mmp"
      },
      "source": [
        "## Model 1-1: Neural bag of words without pre-trained word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-QzOMO_P4jc"
      },
      "source": [
        "Now we use the model2 in lab4 to deal with our task. However, the previous model works only for the binary classification task. Therefore, we need to modify the output layer to fix the multi-class problem. You can read this tutorial for more details: https://machinelearningmastery.com/multi-class-classification-tutorial-keras-deep-learning-library/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Yi04MLIvJOGZ"
      },
      "outputs": [],
      "source": [
        "class GlobalAveragePooling1DMasked(GlobalAveragePooling1D):\n",
        "    def call(self, x, mask=None):\n",
        "        if mask != None:\n",
        "            return K.sum(x, axis=1) / K.sum(mask, axis=1)\n",
        "        else:\n",
        "            return super().call(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFrCsL-NBFVL",
        "outputId": "42966f80-4be8-49ff-f853-f913eac5e4e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 128)]             0         \n",
            "                                                                 \n",
            " target_embed_layer (Embeddi  (None, 128, 100)         789800    \n",
            " ng)                                                             \n",
            "                                                                 \n",
            " global_average_pooling1d (G  (None, 100)              0         \n",
            " lobalAveragePooling1D)                                          \n",
            "                                                                 \n",
            " dense_layer (Dense)         (None, 16)                1616      \n",
            "                                                                 \n",
            " dense_layer_output (Dense)  (None, 3)                 51        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 791,467\n",
            "Trainable params: 791,467\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# your code goes here   \n",
        "# Tips: The activation function of the output layer is softmax.\n",
        "\n",
        "# your code goes here\n",
        "from keras.models import Model\n",
        "import tensorflow as tf\n",
        "VOCAB_SIZE = len(word_index)\n",
        "# The input is an array of target indices e.g. [2, 45, 7, 23,...9]\n",
        "input = Input((128,), dtype='int32')\n",
        "embedding = Embedding(VOCAB_SIZE, 100, name='target_embed_layer',\n",
        "                        \tembeddings_initializer='glorot_uniform',\n",
        "                         \tinput_length=500, mask_zero=True)(input)\n",
        "                          \n",
        "                          \n",
        "avg_pool = GlobalAveragePooling1D()(embedding) \n",
        "\n",
        "dense_layer = tf.keras.layers.Dense(units=16, name=\"dense_layer\")(avg_pool)\n",
        "output_layer = tf.keras.layers.Dense(units=3, name=\"dense_layer_output\", activation = 'sigmoid')(dense_layer)\n",
        "\n",
        "model = Model(inputs=[input], outputs=[output_layer])\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fo8v0dgtgCtq",
        "outputId": "b424dff4-8979-4aaa-ebd5-bd02e1df49e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "22/22 [==============================] - 1s 13ms/step - loss: 0.6762 - accuracy: 0.4539 - val_loss: 0.6545 - val_accuracy: 0.4932\n",
            "Epoch 2/40\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.6338 - accuracy: 0.4744 - val_loss: 0.6152 - val_accuracy: 0.4542\n",
            "Epoch 3/40\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.6051 - accuracy: 0.4509 - val_loss: 0.5998 - val_accuracy: 0.4542\n",
            "Epoch 4/40\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.5901 - accuracy: 0.4584 - val_loss: 0.5862 - val_accuracy: 0.4647\n",
            "Epoch 5/40\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5734 - accuracy: 0.4937 - val_loss: 0.5737 - val_accuracy: 0.4887\n",
            "Epoch 6/40\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5572 - accuracy: 0.5253 - val_loss: 0.5629 - val_accuracy: 0.5180\n",
            "Epoch 7/40\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5418 - accuracy: 0.5461 - val_loss: 0.5540 - val_accuracy: 0.5308\n",
            "Epoch 8/40\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5277 - accuracy: 0.5670 - val_loss: 0.5464 - val_accuracy: 0.5405\n",
            "Epoch 9/40\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5152 - accuracy: 0.5844 - val_loss: 0.5418 - val_accuracy: 0.5480\n",
            "Epoch 10/40\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5047 - accuracy: 0.5977 - val_loss: 0.5378 - val_accuracy: 0.5526\n",
            "Epoch 11/40\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4954 - accuracy: 0.6095 - val_loss: 0.5353 - val_accuracy: 0.5503\n",
            "Epoch 12/40\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4872 - accuracy: 0.6237 - val_loss: 0.5345 - val_accuracy: 0.5503\n",
            "Epoch 13/40\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4801 - accuracy: 0.6295 - val_loss: 0.5335 - val_accuracy: 0.5548\n",
            "Epoch 14/40\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4738 - accuracy: 0.6361 - val_loss: 0.5332 - val_accuracy: 0.5548\n",
            "Epoch 15/40\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4678 - accuracy: 0.6429 - val_loss: 0.5341 - val_accuracy: 0.5488\n",
            "Epoch 16/40\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4627 - accuracy: 0.6488 - val_loss: 0.5358 - val_accuracy: 0.5556\n",
            "Epoch 17/40\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4579 - accuracy: 0.6529 - val_loss: 0.5361 - val_accuracy: 0.5511\n",
            "Epoch 18/40\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4533 - accuracy: 0.6601 - val_loss: 0.5382 - val_accuracy: 0.5518\n",
            "Epoch 19/40\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4492 - accuracy: 0.6631 - val_loss: 0.5401 - val_accuracy: 0.5571\n",
            "Epoch 20/40\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4455 - accuracy: 0.6700 - val_loss: 0.5424 - val_accuracy: 0.5586\n",
            "Epoch 21/40\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4416 - accuracy: 0.6748 - val_loss: 0.5450 - val_accuracy: 0.5601\n",
            "Epoch 22/40\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4381 - accuracy: 0.6782 - val_loss: 0.5477 - val_accuracy: 0.5586\n",
            "Epoch 23/40\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4347 - accuracy: 0.6835 - val_loss: 0.5514 - val_accuracy: 0.5601\n",
            "Epoch 24/40\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4320 - accuracy: 0.6897 - val_loss: 0.5549 - val_accuracy: 0.5556\n",
            "Epoch 25/40\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4285 - accuracy: 0.6923 - val_loss: 0.5569 - val_accuracy: 0.5616\n",
            "Epoch 26/40\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4255 - accuracy: 0.6964 - val_loss: 0.5608 - val_accuracy: 0.5608\n",
            "Epoch 27/40\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4227 - accuracy: 0.7003 - val_loss: 0.5652 - val_accuracy: 0.5556\n",
            "Epoch 28/40\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4204 - accuracy: 0.7053 - val_loss: 0.5690 - val_accuracy: 0.5593\n",
            "Epoch 29/40\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4173 - accuracy: 0.7078 - val_loss: 0.5729 - val_accuracy: 0.5586\n",
            "Epoch 30/40\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4148 - accuracy: 0.7086 - val_loss: 0.5770 - val_accuracy: 0.5556\n",
            "Epoch 31/40\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4124 - accuracy: 0.7131 - val_loss: 0.5818 - val_accuracy: 0.5563\n",
            "Epoch 32/40\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4103 - accuracy: 0.7157 - val_loss: 0.5871 - val_accuracy: 0.5548\n",
            "Epoch 33/40\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4073 - accuracy: 0.7199 - val_loss: 0.5915 - val_accuracy: 0.5518\n",
            "Epoch 34/40\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4055 - accuracy: 0.7213 - val_loss: 0.5967 - val_accuracy: 0.5473\n",
            "Epoch 35/40\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4028 - accuracy: 0.7275 - val_loss: 0.6004 - val_accuracy: 0.5480\n",
            "Epoch 36/40\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4005 - accuracy: 0.7264 - val_loss: 0.6058 - val_accuracy: 0.5533\n",
            "Epoch 37/40\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3986 - accuracy: 0.7293 - val_loss: 0.6122 - val_accuracy: 0.5465\n",
            "Epoch 38/40\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3963 - accuracy: 0.7326 - val_loss: 0.6179 - val_accuracy: 0.5443\n",
            "Epoch 39/40\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3946 - accuracy: 0.7334 - val_loss: 0.6221 - val_accuracy: 0.5435\n",
            "Epoch 40/40\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3923 - accuracy: 0.7381 - val_loss: 0.6291 - val_accuracy: 0.5495\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(x_train_pad,\n",
        "                    y_train,\n",
        "                    epochs=40,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(x_dev_pad, y_dev),\n",
        "                    verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8mlq5Owbpt_",
        "outputId": "4ea5ffce-bca9-41dd-95a9-5481f998e1b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "42/42 [==============================] - 0s 4ms/step - loss: 0.6445 - accuracy: 0.5614\n"
          ]
        }
      ],
      "source": [
        "results = model.evaluate(x_test_pad, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FBpTc_rXGvQ"
      },
      "source": [
        "The accuracy of lab3 model2 in this task is around 46%. If you use the \"glorot_uniform\" initialization method, the accuracy can reach around 55%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iafDTygK28fv"
      },
      "source": [
        "##  Model 1-2: CNN or LSTM without pre-trained word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2TnuiKb2-vE"
      },
      "source": [
        "Please try one more model (CNN or LSTM) without pre-trained word embeddings in here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXjbq6WcJosQ",
        "outputId": "e0278b62-b9a9-42e7-80e9-acb5411caeca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 128)]             0         \n",
            "                                                                 \n",
            " target_embed_layer (Embeddi  (None, 128, 100)         789800    \n",
            " ng)                                                             \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 4)                 1680      \n",
            "                                                                 \n",
            " dense_layer (Dense)         (None, 16)                80        \n",
            "                                                                 \n",
            " dense_layer_output (Dense)  (None, 3)                 51        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 791,611\n",
            "Trainable params: 791,611\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Try CNN or LSTM without pre-trained word embeddings in here:\n",
        "\n",
        "# your code goes here   \n",
        "# Tips: The activation function of the output layer is softmax.\n",
        "\n",
        "# your code goes here\n",
        "from keras.models import Model\n",
        "import tensorflow as tf\n",
        "VOCAB_SIZE = len(word_index)\n",
        "# The input is an array of target indices e.g. [2, 45, 7, 23,...9]\n",
        "input = Input((128,), dtype='int32')\n",
        "embedding = Embedding(VOCAB_SIZE, 100, name='target_embed_layer',\n",
        "                        \tembeddings_initializer='glorot_uniform',\n",
        "                         \tinput_length=500, mask_zero=True)(input)\n",
        "                          \n",
        "lstm = tf.keras.layers.LSTM(4)(embedding)\n",
        "\n",
        "dense_layer = tf.keras.layers.Dense(units=16, name=\"dense_layer\")(lstm)\n",
        "output_layer = tf.keras.layers.Dense(units=3, name=\"dense_layer_output\", activation = 'sigmoid')(dense_layer)\n",
        "\n",
        "model1_2 = Model(inputs=[input], outputs=[output_layer])\n",
        "model1_2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model1_2.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKG35AkGqvA6",
        "outputId": "740ffae9-2c0d-40fe-cc57-1943d979f809"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "22/22 [==============================] - 9s 89ms/step - loss: 0.6797 - accuracy: 0.3687 - val_loss: 0.6617 - val_accuracy: 0.4264\n",
            "Epoch 2/40\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.6414 - accuracy: 0.4548 - val_loss: 0.6249 - val_accuracy: 0.4542\n",
            "Epoch 3/40\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.6136 - accuracy: 0.4529 - val_loss: 0.6112 - val_accuracy: 0.4535\n",
            "Epoch 4/40\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.6006 - accuracy: 0.4525 - val_loss: 0.6039 - val_accuracy: 0.4580\n",
            "Epoch 5/40\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.5870 - accuracy: 0.4616 - val_loss: 0.5974 - val_accuracy: 0.4625\n",
            "Epoch 6/40\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.5730 - accuracy: 0.4642 - val_loss: 0.5929 - val_accuracy: 0.4640\n",
            "Epoch 7/40\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.5591 - accuracy: 0.4767 - val_loss: 0.5899 - val_accuracy: 0.4572\n",
            "Epoch 8/40\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.5473 - accuracy: 0.4802 - val_loss: 0.5886 - val_accuracy: 0.4647\n",
            "Epoch 9/40\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.5376 - accuracy: 0.4924 - val_loss: 0.5851 - val_accuracy: 0.4625\n",
            "Epoch 10/40\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.5297 - accuracy: 0.5048 - val_loss: 0.5831 - val_accuracy: 0.4715\n",
            "Epoch 11/40\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.5227 - accuracy: 0.5178 - val_loss: 0.5825 - val_accuracy: 0.4737\n",
            "Epoch 12/40\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.5147 - accuracy: 0.5367 - val_loss: 0.5812 - val_accuracy: 0.4797\n",
            "Epoch 13/40\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.5062 - accuracy: 0.5617 - val_loss: 0.5784 - val_accuracy: 0.4872\n",
            "Epoch 14/40\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.4964 - accuracy: 0.5826 - val_loss: 0.5757 - val_accuracy: 0.4940\n",
            "Epoch 15/40\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.4858 - accuracy: 0.6080 - val_loss: 0.5785 - val_accuracy: 0.5150\n",
            "Epoch 16/40\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.4737 - accuracy: 0.6224 - val_loss: 0.5764 - val_accuracy: 0.5203\n",
            "Epoch 17/40\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.4616 - accuracy: 0.6399 - val_loss: 0.5805 - val_accuracy: 0.5375\n",
            "Epoch 18/40\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.4526 - accuracy: 0.6494 - val_loss: 0.5799 - val_accuracy: 0.5420\n",
            "Epoch 19/40\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.4393 - accuracy: 0.6606 - val_loss: 0.5803 - val_accuracy: 0.5435\n",
            "Epoch 20/40\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.4299 - accuracy: 0.6700 - val_loss: 0.5920 - val_accuracy: 0.5495\n",
            "Epoch 21/40\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.4213 - accuracy: 0.6767 - val_loss: 0.5986 - val_accuracy: 0.5428\n",
            "Epoch 22/40\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.4129 - accuracy: 0.6859 - val_loss: 0.6018 - val_accuracy: 0.5473\n",
            "Epoch 23/40\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.4052 - accuracy: 0.6907 - val_loss: 0.6093 - val_accuracy: 0.5495\n",
            "Epoch 24/40\n",
            "22/22 [==============================] - 0s 22ms/step - loss: 0.3984 - accuracy: 0.6970 - val_loss: 0.6165 - val_accuracy: 0.5443\n",
            "Epoch 25/40\n",
            "22/22 [==============================] - 1s 23ms/step - loss: 0.3949 - accuracy: 0.6995 - val_loss: 0.6219 - val_accuracy: 0.5518\n",
            "Epoch 26/40\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.3878 - accuracy: 0.7071 - val_loss: 0.6256 - val_accuracy: 0.5473\n",
            "Epoch 27/40\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.3814 - accuracy: 0.7136 - val_loss: 0.6254 - val_accuracy: 0.5586\n",
            "Epoch 28/40\n",
            "22/22 [==============================] - 1s 23ms/step - loss: 0.3782 - accuracy: 0.7154 - val_loss: 0.6369 - val_accuracy: 0.5495\n",
            "Epoch 29/40\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3739 - accuracy: 0.7230 - val_loss: 0.6374 - val_accuracy: 0.5413\n",
            "Epoch 30/40\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.3703 - accuracy: 0.7258 - val_loss: 0.6316 - val_accuracy: 0.5571\n",
            "Epoch 31/40\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3605 - accuracy: 0.7373 - val_loss: 0.6352 - val_accuracy: 0.5661\n",
            "Epoch 32/40\n",
            "22/22 [==============================] - 1s 29ms/step - loss: 0.3485 - accuracy: 0.7485 - val_loss: 0.6424 - val_accuracy: 0.5653\n",
            "Epoch 33/40\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.3388 - accuracy: 0.7583 - val_loss: 0.6516 - val_accuracy: 0.5646\n",
            "Epoch 34/40\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.3309 - accuracy: 0.7645 - val_loss: 0.6529 - val_accuracy: 0.5638\n",
            "Epoch 35/40\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.3237 - accuracy: 0.7728 - val_loss: 0.6658 - val_accuracy: 0.5676\n",
            "Epoch 36/40\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.3194 - accuracy: 0.7773 - val_loss: 0.6723 - val_accuracy: 0.5706\n",
            "Epoch 37/40\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 0.3129 - accuracy: 0.7838"
          ]
        }
      ],
      "source": [
        "history = model1_2.fit(x_train_pad,\n",
        "                    y_train,\n",
        "                    epochs=40,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(x_dev_pad, y_dev),\n",
        "                    verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZvKw8dEoXrSk",
        "outputId": "c3d2b71a-0f73-48da-b25c-960fc36d9a2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "42/42 [==============================] - 0s 4ms/step - loss: 0.7560 - accuracy: 0.5546\n"
          ]
        }
      ],
      "source": [
        "results = model1_2.evaluate(x_test_pad, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUPuaVqufHv2"
      },
      "source": [
        "CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LbmVMarfHLd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7aeb1cd-0a8a-461e-d969-9314ab281aa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 128)]             0         \n",
            "                                                                 \n",
            " embed_layer (Embedding)     (None, 128, 100)          789800    \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 121, 100)          80100     \n",
            "                                                                 \n",
            " global_max_pooling1d (Globa  (None, 100)              0         \n",
            " lMaxPooling1D)                                                  \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                1616      \n",
            "                                                                 \n",
            " dense_layer_output (Dense)  (None, 3)                 51        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 871,567\n",
            "Trainable params: 871,567\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from keras.layers.pooling import GlobalMaxPooling1D\n",
        "hidden_layer=16\n",
        "\n",
        "from keras.models import Model\n",
        "\n",
        "input = Input((128,), dtype='int32')\n",
        "embedding = Embedding(VOCAB_SIZE, 100, name='embed_layer',\n",
        "                        \tembeddings_initializer='glorot_uniform',\n",
        "                         \tinput_length=500, mask_zero=True)(input)\n",
        "conv = tf.keras.layers.Conv1D(100,8)(embedding)\n",
        "pool = GlobalMaxPooling1D()(conv)\n",
        "hidden = Dense(16)(pool)\n",
        "output_layer = tf.keras.layers.Dense(units=3, name=\"dense_layer_output\", activation = 'sigmoid')(hidden)\n",
        "\n",
        "model1_cnn = Model(inputs=[input], outputs=[output_layer])\n",
        "model1_cnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model1_cnn.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gb0AXM6NgtWn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ebabd6a-e26d-4c88-f49c-5b23101b7681"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "22/22 [==============================] - 10s 398ms/step - loss: 0.6475 - accuracy: 0.4017 - val_loss: 0.6187 - val_accuracy: 0.4535\n",
            "Epoch 2/40\n",
            "22/22 [==============================] - 9s 388ms/step - loss: 0.6020 - accuracy: 0.4593 - val_loss: 0.5883 - val_accuracy: 0.4812\n",
            "Epoch 3/40\n",
            "22/22 [==============================] - 9s 386ms/step - loss: 0.5585 - accuracy: 0.5275 - val_loss: 0.5423 - val_accuracy: 0.5706\n",
            "Epoch 4/40\n",
            "22/22 [==============================] - 9s 391ms/step - loss: 0.4891 - accuracy: 0.6483 - val_loss: 0.4882 - val_accuracy: 0.6321\n",
            "Epoch 5/40\n",
            "22/22 [==============================] - 9s 391ms/step - loss: 0.4291 - accuracy: 0.7120 - val_loss: 0.4756 - val_accuracy: 0.6471\n",
            "Epoch 6/40\n",
            "22/22 [==============================] - 8s 385ms/step - loss: 0.3899 - accuracy: 0.7389 - val_loss: 0.4770 - val_accuracy: 0.6449\n",
            "Epoch 7/40\n",
            "22/22 [==============================] - 9s 416ms/step - loss: 0.3611 - accuracy: 0.7675 - val_loss: 0.4865 - val_accuracy: 0.6419\n",
            "Epoch 8/40\n",
            "22/22 [==============================] - 8s 387ms/step - loss: 0.3380 - accuracy: 0.7852 - val_loss: 0.4993 - val_accuracy: 0.6441\n",
            "Epoch 9/40\n",
            "22/22 [==============================] - 8s 386ms/step - loss: 0.3178 - accuracy: 0.8022 - val_loss: 0.5126 - val_accuracy: 0.6396\n",
            "Epoch 10/40\n",
            "22/22 [==============================] - 8s 385ms/step - loss: 0.2998 - accuracy: 0.8153 - val_loss: 0.5231 - val_accuracy: 0.6366\n",
            "Epoch 11/40\n",
            "22/22 [==============================] - 8s 386ms/step - loss: 0.2829 - accuracy: 0.8225 - val_loss: 0.5402 - val_accuracy: 0.6336\n",
            "Epoch 12/40\n",
            "22/22 [==============================] - 9s 389ms/step - loss: 0.2693 - accuracy: 0.8359 - val_loss: 0.5544 - val_accuracy: 0.6441\n",
            "Epoch 13/40\n",
            "22/22 [==============================] - 8s 386ms/step - loss: 0.2545 - accuracy: 0.8469 - val_loss: 0.5675 - val_accuracy: 0.6381\n",
            "Epoch 14/40\n",
            "22/22 [==============================] - 12s 549ms/step - loss: 0.2396 - accuracy: 0.8580 - val_loss: 0.5883 - val_accuracy: 0.6366\n",
            "Epoch 15/40\n",
            "22/22 [==============================] - 9s 420ms/step - loss: 0.2278 - accuracy: 0.8648 - val_loss: 0.6068 - val_accuracy: 0.6366\n",
            "Epoch 16/40\n",
            "22/22 [==============================] - 9s 402ms/step - loss: 0.2151 - accuracy: 0.8762 - val_loss: 0.6250 - val_accuracy: 0.6336\n",
            "Epoch 17/40\n",
            "22/22 [==============================] - 9s 415ms/step - loss: 0.2034 - accuracy: 0.8813 - val_loss: 0.6399 - val_accuracy: 0.6344\n",
            "Epoch 18/40\n",
            "22/22 [==============================] - 9s 412ms/step - loss: 0.1932 - accuracy: 0.8877 - val_loss: 0.6569 - val_accuracy: 0.6374\n",
            "Epoch 19/40\n",
            "22/22 [==============================] - 8s 385ms/step - loss: 0.1848 - accuracy: 0.8955 - val_loss: 0.6734 - val_accuracy: 0.6411\n",
            "Epoch 20/40\n",
            "22/22 [==============================] - 9s 389ms/step - loss: 0.1736 - accuracy: 0.9010 - val_loss: 0.6883 - val_accuracy: 0.6396\n",
            "Epoch 21/40\n",
            "22/22 [==============================] - 9s 387ms/step - loss: 0.1651 - accuracy: 0.9059 - val_loss: 0.7109 - val_accuracy: 0.6404\n",
            "Epoch 22/40\n",
            "22/22 [==============================] - 8s 383ms/step - loss: 0.1578 - accuracy: 0.9112 - val_loss: 0.7200 - val_accuracy: 0.6366\n",
            "Epoch 23/40\n",
            "22/22 [==============================] - 8s 386ms/step - loss: 0.1477 - accuracy: 0.9179 - val_loss: 0.7465 - val_accuracy: 0.6351\n",
            "Epoch 24/40\n",
            "22/22 [==============================] - 8s 386ms/step - loss: 0.1418 - accuracy: 0.9213 - val_loss: 0.7614 - val_accuracy: 0.6404\n",
            "Epoch 25/40\n",
            "22/22 [==============================] - 8s 385ms/step - loss: 0.1354 - accuracy: 0.9229 - val_loss: 0.7708 - val_accuracy: 0.6381\n",
            "Epoch 26/40\n",
            "22/22 [==============================] - 8s 384ms/step - loss: 0.1287 - accuracy: 0.9279 - val_loss: 0.7882 - val_accuracy: 0.6381\n",
            "Epoch 27/40\n",
            "22/22 [==============================] - 9s 388ms/step - loss: 0.1217 - accuracy: 0.9330 - val_loss: 0.8020 - val_accuracy: 0.6366\n",
            "Epoch 28/40\n",
            "22/22 [==============================] - 9s 387ms/step - loss: 0.1142 - accuracy: 0.9393 - val_loss: 0.8214 - val_accuracy: 0.6374\n",
            "Epoch 29/40\n",
            "22/22 [==============================] - 8s 385ms/step - loss: 0.1081 - accuracy: 0.9420 - val_loss: 0.8428 - val_accuracy: 0.6276\n",
            "Epoch 30/40\n",
            "22/22 [==============================] - 9s 394ms/step - loss: 0.1019 - accuracy: 0.9464 - val_loss: 0.8565 - val_accuracy: 0.6329\n",
            "Epoch 31/40\n",
            "22/22 [==============================] - 9s 396ms/step - loss: 0.0959 - accuracy: 0.9503 - val_loss: 0.8776 - val_accuracy: 0.6314\n",
            "Epoch 32/40\n",
            "22/22 [==============================] - 9s 391ms/step - loss: 0.0900 - accuracy: 0.9581 - val_loss: 0.8923 - val_accuracy: 0.6299\n",
            "Epoch 33/40\n",
            "22/22 [==============================] - 8s 385ms/step - loss: 0.0837 - accuracy: 0.9610 - val_loss: 0.9174 - val_accuracy: 0.6254\n",
            "Epoch 34/40\n",
            "22/22 [==============================] - 8s 384ms/step - loss: 0.0782 - accuracy: 0.9659 - val_loss: 0.9323 - val_accuracy: 0.6321\n",
            "Epoch 35/40\n",
            "22/22 [==============================] - 8s 385ms/step - loss: 0.0730 - accuracy: 0.9671 - val_loss: 0.9631 - val_accuracy: 0.6216\n",
            "Epoch 36/40\n",
            "22/22 [==============================] - 8s 385ms/step - loss: 0.0699 - accuracy: 0.9716 - val_loss: 0.9867 - val_accuracy: 0.6254\n",
            "Epoch 37/40\n",
            "22/22 [==============================] - 9s 386ms/step - loss: 0.0662 - accuracy: 0.9698 - val_loss: 0.9990 - val_accuracy: 0.6231\n",
            "Epoch 38/40\n",
            "22/22 [==============================] - 8s 386ms/step - loss: 0.0600 - accuracy: 0.9734 - val_loss: 1.0154 - val_accuracy: 0.6246\n",
            "Epoch 39/40\n",
            "22/22 [==============================] - 8s 386ms/step - loss: 0.0566 - accuracy: 0.9786 - val_loss: 1.0336 - val_accuracy: 0.6224\n",
            "Epoch 40/40\n",
            "22/22 [==============================] - 8s 384ms/step - loss: 0.0530 - accuracy: 0.9789 - val_loss: 1.0627 - val_accuracy: 0.6201\n"
          ]
        }
      ],
      "source": [
        "history = model1_cnn.fit(x_train_pad,\n",
        "                    y_train,\n",
        "                    epochs=40,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(x_dev_pad, y_dev),\n",
        "                    verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kslsZ-oNXxWa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b08f28a-dc71-4a80-d318-9b30ec065fe3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "42/42 [==============================] - 0s 10ms/step - loss: 1.1167 - accuracy: 0.6078\n"
          ]
        }
      ],
      "source": [
        "results = model1_cnn.evaluate(x_test_pad, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--020hfG6rN2"
      },
      "source": [
        "# Model 2: Using pre-trained word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GdY2-64YG1B"
      },
      "source": [
        "### Preparing pre-trained word embeddings (GLOVE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4gBeOyi4gkM"
      },
      "source": [
        "The Embedding layer can be used to load a pre-trained word embedding model. We are going to use GloVe embeddings, which you can read about it here (https://nlp.stanford.edu/projects/glove/). GloVe stands for \"Global Vectors for Word Representation\". It's a somewhat popular embedding technique based on factorizing a matrix of word co-occurence statistics. You can download GloVe and we can seed the Keras Embedding layer with weights from the pre-trained embedding for the words in your dataset.\n",
        "First, we need to read GloVe and map words to GloVe:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "f_PypdqG9Iis"
      },
      "outputs": [],
      "source": [
        "def readGloveFile(gloveFile):\n",
        "    with open(gloveFile, 'r') as f:\n",
        "        wordToGlove = {}  \n",
        "        wordToIndex = {}  \n",
        "        indexToWord = {}  \n",
        "\n",
        "        for line in f:\n",
        "            record = line.strip().split()\n",
        "            token = record[0] \n",
        "            wordToGlove[token] = np.array(record[1:], dtype=np.float64) \n",
        "            \n",
        "        tokens = sorted(wordToGlove.keys())\n",
        "        for idx, tok in enumerate(tokens):\n",
        "            kerasIdx = idx + 1  \n",
        "            wordToIndex[tok] = kerasIdx \n",
        "            indexToWord[kerasIdx] = tok \n",
        "\n",
        "    return wordToIndex, indexToWord, wordToGlove"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcIZ3dq59bCh"
      },
      "source": [
        "Now, we create our pre-trained Embedding layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "gembn7VM3ex8"
      },
      "outputs": [],
      "source": [
        "from keras.initializers import Constant\n",
        "import tensorflow as tf\n",
        "lstm = tf.keras.layers.LSTM(4)\n",
        "def createPretrainedEmbeddingLayer(wordToGlove, wordToIndex, isTrainable):\n",
        "    vocabLen = len(wordToIndex) + 1  \n",
        "    embDim = next(iter(wordToGlove.values())).shape[0]  \n",
        "   \n",
        "    embeddingMatrix = np.zeros((vocabLen, embDim))  \n",
        "    for word, index in wordToIndex.items():\n",
        "        embeddingMatrix[index, :] = wordToGlove[word] \n",
        "\n",
        "    embeddingLayer = Embedding(vocabLen, embDim, embeddings_initializer=Constant(embeddingMatrix), trainable=isTrainable, name='GloVe_Embeddings')\n",
        "    return embeddingLayer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8OC1wuctdFvA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d101147b-147d-4f7d-c0d9-351d6db50ff1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-09 13:57:00--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2022-03-09 13:57:00--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2022-03-09 13:57:00--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: â€˜glove.6B.zipâ€™\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.08MB/s    in 2m 40s  \n",
            "\n",
            "2022-03-09 13:59:40 (5.14 MB/s) - â€˜glove.6B.zipâ€™ saved [862182613/862182613]\n",
            "\n",
            "Archive:  /content/glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip '/content/glove.6B.zip'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGxciLK4-xOr"
      },
      "source": [
        "We freeze the weights. To create the model: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "PZCPUM0W_Drc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1537498b-445e-481a-9b53-c5b9bb8256a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of Embedding:  300\n"
          ]
        }
      ],
      "source": [
        "# wordToIndex,indexToWord,wordToGlove=readGloveFile('/content/glove.6B.50d.txt')\n",
        "# wordToIndex,indexToWord,wordToGlove=readGloveFile('/content/glove.6B.100d.txt')\n",
        "wordToIndex,indexToWord,wordToGlove=readGloveFile('/content/glove.6B.300d.txt')\n",
        "\n",
        "# vocabLen = len(wordToIndex) + 1 \n",
        "\n",
        "EMBED_SIZE = next(iter(wordToGlove.values())).shape[0]\n",
        "print('Size of Embedding: ',EMBED_SIZE)\n",
        "\n",
        "embeddingLayer=createPretrainedEmbeddingLayer(wordToGlove,wordToIndex,isTrainable=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RyeDPimMW7c"
      },
      "source": [
        "### Convert the data to GLOVE word index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_X6nC57NGXv"
      },
      "source": [
        "The index in our vocabulary is different from that in GLOVE. For example, the word \"you\" corresponds to 394475 in GLOVE, while it corresponds to another index in our vocabulary. Thus we can not directly use the index data in the last section. We convert them from text tokens to GLOVE word index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ODUoIts1NM6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4f4d1d2-58c8-471b-92b1-f26b32cb7d5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "394475\n",
            "you\n",
            "7856\n"
          ]
        }
      ],
      "source": [
        "print(wordToIndex[\"you\"])\n",
        "print(indexToWord[394475])\n",
        "print(word_index[\"you\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "1MmZq5MT1ZB2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59488f62-d736-45ca-ab30-79ec5878a0b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "372306\n"
          ]
        }
      ],
      "source": [
        "print(wordToIndex[\"unk\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "p037PLDyNXhV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3d879e1-40b5-4d7c-b298-d87c2bc2bb3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train_review_glove[0]:\n",
            "[357266, 118926, 192973, 264550, 338995, 62065, 51582, 87775, 357354, 151204, 54718, 53201, 292136, 231458, 373317, 151349, 193716]\n",
            "x_train_aspect_glove[0]:\n",
            "151204\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.python.framework import device\n",
        "# Please write your code to generate the following data\n",
        "x_train_review_glove = [[wordToIndex.get(v,372306) for v in w] for w in x_train_review]\n",
        "x_train_aspect_glove = [wordToIndex.get(w[1],372306) for w in train]\n",
        "\n",
        "\n",
        "x_dev_review_glove = [[wordToIndex.get(v,372306) for v in w] for w in x_dev_review]\n",
        "x_dev_aspect_glove = [wordToIndex.get(w[1],372306) for w in val]\n",
        "\n",
        "x_test_review_glove = [[wordToIndex.get(v,372306) for v in w] for w in x_test_review]\n",
        "x_test_aspect_glove = [wordToIndex.get(w[1],372306) for w in test]\n",
        "\n",
        "# You should get a print result like:\n",
        "assert len(x_train_review_glove) == len(train)\n",
        "assert len(x_train_aspect_glove) == len(x_train_aspect_int)\n",
        "assert len(x_test_review_glove) == len(test)\n",
        "assert len(x_test_aspect_glove) == len(x_test_aspect_int)\n",
        "print(\"x_train_review_glove[0]:\")\n",
        "print(x_train_review_glove[0])\n",
        "print(\"x_train_aspect_glove[0]:\")\n",
        "print(x_test_aspect_glove[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "wbFC6etAzG5U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "661468b9-bd5c-4d3a-ed36-639dec8608da"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'food'"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "indexToWord[151204]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z--QOlb1vXtN"
      },
      "source": [
        "As before, we concatenate the tweets and topics for fitting in the previous model. Let us do it again for GLOVE version variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-D_szbDQ5X5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfe39452-fa00-4451-f866-b3d8fd161e76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before paded:\n",
            "[118926, 1, 357266, 118926, 192973, 264550, 338995, 62065, 51582, 87775, 357354, 151204, 54718, 53201, 292136, 231458, 373317, 151349, 193716]\n",
            "After paded:\n",
            "[118926      1 357266 118926 192973 264550 338995  62065  51582  87775\n",
            " 357354 151204  54718  53201 292136 231458 373317 151349 193716      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0]\n"
          ]
        }
      ],
      "source": [
        "# Please write your code to combine the x_*_review_glove and x_*_aspect_glove into the following varibles\n",
        "x_train_glove =[]\n",
        "x_dev_glove = []\n",
        "x_test_glove = []\n",
        "\n",
        "def combine_x(a,b):\n",
        "  c = []\n",
        "  for i in range(len(b)):\n",
        "    c.append([a[i]] + [1,] + b[i])\n",
        "  return c\n",
        "# Tips: \n",
        "# 1) There is no <START> token in GLOVE. Here we can use integer 1 to concatenate.\n",
        "# 2) After combine them, do not foget to pad the sequences.\n",
        "\n",
        "#######################TODO: REMOVE FOLLOWING CODE####################\n",
        "\n",
        "x_train_glove = combine_x(x_train_aspect_glove,x_train_review_glove)\n",
        "x_dev_glove   = combine_x(x_dev_aspect_glove  ,x_dev_review_glove)\n",
        "x_test_glove  = combine_x(x_test_aspect_glove ,x_test_review_glove)\n",
        "\n",
        "\n",
        "x_train_pad_glove = keras.preprocessing.sequence.pad_sequences(x_train_glove,\n",
        "                                                        value=0,\n",
        "                                                        padding='post',\n",
        "                                                        maxlen=128)\n",
        "\n",
        "x_dev_pad_glove = keras.preprocessing.sequence.pad_sequences(x_dev_glove,\n",
        "                                                       value=0,\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=128)\n",
        "\n",
        "x_test_pad_glove = keras.preprocessing.sequence.pad_sequences(x_test_glove,\n",
        "                                                       value=0,\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=128)\n",
        "x_train_pad_glove = np.array(x_train_pad_glove)\n",
        "x_dev_pad_glove = np.array(x_dev_pad_glove)\n",
        "x_test_pad_glove = np.array(x_test_pad_glove)\n",
        "#######################TODO: REMOVE ABOVE CODE####################\n",
        "\n",
        "# Don't forget the to use np.array function to wrap the ouput of pad_sequences function, such as: x_train_pad = np.array(x_train_pad)\n",
        "# Only pad the *_int varibles\n",
        "print(\"Before paded:\")\n",
        "print(x_train_glove[0])\n",
        "print(\"After paded:\")\n",
        "print(x_train_pad_glove[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdZ4nl08vp9A"
      },
      "source": [
        "## Model 2-1: Neural bag of words using pre-trained word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gyCwXFj_R5w"
      },
      "source": [
        "We use model3-1 in lab4 to deal with this task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VICS9rY8C7KH"
      },
      "outputs": [],
      "source": [
        "# your code goes here\n",
        "# Tips: Do not misuse the training data\n",
        "\n",
        "# your code goes here\n",
        "from keras.models import Model\n",
        "import tensorflow as tf\n",
        "VOCAB_SIZE = len(word_index)\n",
        "# The input is an array of target indices e.g. [2, 45, 7, 23,...9]\n",
        "input = Input((128,), dtype='int32')\n",
        "embedding = createPretrainedEmbeddingLayer(wordToGlove,wordToIndex,isTrainable=False)(input)\n",
        "                                                    \n",
        "avg_pool = GlobalAveragePooling1D()(embedding) \n",
        "\n",
        "dense_layer = tf.keras.layers.Dense(units=16, name=\"dense_layer\")(avg_pool)\n",
        "output_layer = tf.keras.layers.Dense(units=3, name=\"dense_layer_output\", activation = 'sigmoid')(dense_layer)\n",
        "\n",
        "model2_1 = Model(inputs=[input], outputs=[output_layer])\n",
        "model2_1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2_1.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W61sH8Uu41PO",
        "outputId": "033957bc-1935-490b-b718-6053e892ed66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 128)]             0         \n",
            "                                                                 \n",
            " GloVe_Embeddings (Embedding  (None, 128, 300)         120000300 \n",
            " )                                                               \n",
            "                                                                 \n",
            " global_average_pooling1d (G  (None, 300)              0         \n",
            " lobalAveragePooling1D)                                          \n",
            "                                                                 \n",
            " dense_layer (Dense)         (None, 16)                4816      \n",
            "                                                                 \n",
            " dense_layer_output (Dense)  (None, 3)                 51        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 120,005,167\n",
            "Trainable params: 4,867\n",
            "Non-trainable params: 120,000,300\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aTWUWQS2VFd"
      },
      "source": [
        "The accuracy is around 56%. In this version, the \"glorot_uniform\" initialization method does not improve model performance significantly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jiir3XiSZzCA",
        "outputId": "81582b85-3f96-4bea-eca4-47ce139c0b06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.5544 - accuracy: 0.5341 - val_loss: 0.5557 - val_accuracy: 0.5368\n",
            "Epoch 2/40\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.5539 - accuracy: 0.5335 - val_loss: 0.5547 - val_accuracy: 0.5368\n",
            "Epoch 3/40\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.5534 - accuracy: 0.5362 - val_loss: 0.5550 - val_accuracy: 0.5398\n",
            "Epoch 4/40\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.5530 - accuracy: 0.5359 - val_loss: 0.5543 - val_accuracy: 0.5375\n",
            "Epoch 5/40\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.5524 - accuracy: 0.5346 - val_loss: 0.5535 - val_accuracy: 0.5443\n",
            "Epoch 6/40\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.5520 - accuracy: 0.5365 - val_loss: 0.5546 - val_accuracy: 0.5405\n",
            "Epoch 7/40\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.5519 - accuracy: 0.5375 - val_loss: 0.5529 - val_accuracy: 0.5450\n",
            "Epoch 8/40\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.5513 - accuracy: 0.5384 - val_loss: 0.5529 - val_accuracy: 0.5405\n",
            "Epoch 9/40\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.5509 - accuracy: 0.5381 - val_loss: 0.5527 - val_accuracy: 0.5405\n",
            "Epoch 10/40\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.5505 - accuracy: 0.5371 - val_loss: 0.5521 - val_accuracy: 0.5450\n",
            "Epoch 11/40\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.5503 - accuracy: 0.5384 - val_loss: 0.5525 - val_accuracy: 0.5443\n",
            "Epoch 12/40\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.5499 - accuracy: 0.5404 - val_loss: 0.5521 - val_accuracy: 0.5473\n",
            "Epoch 13/40\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.5498 - accuracy: 0.5405 - val_loss: 0.5520 - val_accuracy: 0.5480\n",
            "Epoch 14/40\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.5493 - accuracy: 0.5392 - val_loss: 0.5513 - val_accuracy: 0.5473\n",
            "Epoch 15/40\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.5489 - accuracy: 0.5433 - val_loss: 0.5520 - val_accuracy: 0.5398\n",
            "Epoch 16/40\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.5493 - accuracy: 0.5415 - val_loss: 0.5511 - val_accuracy: 0.5458\n",
            "Epoch 17/40\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.5484 - accuracy: 0.5434 - val_loss: 0.5510 - val_accuracy: 0.5473\n",
            "Epoch 18/40\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.5481 - accuracy: 0.5414 - val_loss: 0.5506 - val_accuracy: 0.5473\n",
            "Epoch 19/40\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.5479 - accuracy: 0.5427 - val_loss: 0.5505 - val_accuracy: 0.5450\n",
            "Epoch 20/40\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.5476 - accuracy: 0.5423 - val_loss: 0.5506 - val_accuracy: 0.5473\n",
            "Epoch 21/40\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.5474 - accuracy: 0.5429 - val_loss: 0.5507 - val_accuracy: 0.5428\n",
            "Epoch 22/40\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.5471 - accuracy: 0.5431 - val_loss: 0.5502 - val_accuracy: 0.5480\n",
            "Epoch 23/40\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.5470 - accuracy: 0.5438 - val_loss: 0.5501 - val_accuracy: 0.5488\n",
            "Epoch 24/40\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.5467 - accuracy: 0.5443 - val_loss: 0.5498 - val_accuracy: 0.5473\n",
            "Epoch 25/40\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.5466 - accuracy: 0.5434 - val_loss: 0.5496 - val_accuracy: 0.5511\n",
            "Epoch 26/40\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.5465 - accuracy: 0.5465 - val_loss: 0.5511 - val_accuracy: 0.5495\n",
            "Epoch 27/40\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.5462 - accuracy: 0.5451 - val_loss: 0.5503 - val_accuracy: 0.5488\n",
            "Epoch 28/40\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.5460 - accuracy: 0.5452 - val_loss: 0.5498 - val_accuracy: 0.5488\n",
            "Epoch 29/40\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.5458 - accuracy: 0.5460 - val_loss: 0.5496 - val_accuracy: 0.5511\n",
            "Epoch 30/40\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.5456 - accuracy: 0.5434 - val_loss: 0.5492 - val_accuracy: 0.5548\n",
            "Epoch 31/40\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.5454 - accuracy: 0.5478 - val_loss: 0.5506 - val_accuracy: 0.5518\n",
            "Epoch 32/40\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.5452 - accuracy: 0.5469 - val_loss: 0.5492 - val_accuracy: 0.5488\n",
            "Epoch 33/40\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.5452 - accuracy: 0.5462 - val_loss: 0.5498 - val_accuracy: 0.5473\n",
            "Epoch 34/40\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.5448 - accuracy: 0.5467 - val_loss: 0.5495 - val_accuracy: 0.5518\n",
            "Epoch 35/40\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.5450 - accuracy: 0.5482 - val_loss: 0.5497 - val_accuracy: 0.5533\n",
            "Epoch 36/40\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.5447 - accuracy: 0.5480 - val_loss: 0.5496 - val_accuracy: 0.5503\n",
            "Epoch 37/40\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.5446 - accuracy: 0.5471 - val_loss: 0.5493 - val_accuracy: 0.5480\n",
            "Epoch 38/40\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.5444 - accuracy: 0.5487 - val_loss: 0.5493 - val_accuracy: 0.5563\n",
            "Epoch 39/40\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.5442 - accuracy: 0.5476 - val_loss: 0.5499 - val_accuracy: 0.5488\n",
            "Epoch 40/40\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.5440 - accuracy: 0.5472 - val_loss: 0.5491 - val_accuracy: 0.5526\n"
          ]
        }
      ],
      "source": [
        "history = model2_1.fit(x_train_pad_glove,\n",
        "                    y_train,\n",
        "                    epochs=40,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(x_dev_pad_glove, y_dev),\n",
        "                    verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results2_1 = model2_1.evaluate(x_test_pad_glove, y_test)\n",
        "print(\"Loss: {} and Accuracy: {}\".format(results2_1[0], results2_1[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tG1UaJXp8CuO",
        "outputId": "e1e8cb06-41a2-460d-cc6e-c475cfb40b4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "42/42 [==============================] - 0s 2ms/step - loss: 0.5397 - accuracy: 0.5689\n",
            "Loss: 0.5397483110427856 and Accuracy: 0.56886225938797\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZ1KWFKvcagS"
      },
      "source": [
        "##  Model 2-2: CNN or LSTM with pre-trained word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYubGfP-2VEL"
      },
      "source": [
        "Please try one more model (CNN or LSTM) with pre-trained word embeddings in here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeLTrJ-3zKBj"
      },
      "outputs": [],
      "source": [
        "# your code goes here\n",
        "\n",
        "# Try CNN or LSTM without pre-trained word embeddings in here:\n",
        "\n",
        "# your code goes here   \n",
        "# Tips: The activation function of the output layer is softmax.\n",
        "\n",
        "# your code goes here\n",
        "from keras.models import Model\n",
        "import tensorflow as tf\n",
        "VOCAB_SIZE = len(word_index)\n",
        "# The input is an array of target indices e.g. [2, 45, 7, 23,...9]\n",
        "input = Input((128,), dtype='int32')\n",
        "embedding = createPretrainedEmbeddingLayer(wordToGlove,wordToIndex,isTrainable=False)(input)\n",
        "                          \n",
        "lstm = tf.keras.layers.LSTM(4)(embedding)\n",
        "\n",
        "dense_layer = tf.keras.layers.Dense(units=16, name=\"dense_layer\")(lstm)\n",
        "output_layer = tf.keras.layers.Dense(units=3, name=\"dense_layer_output\", activation = 'sigmoid')(dense_layer)\n",
        "\n",
        "model2_2 = Model(inputs=[input], outputs=[output_layer])\n",
        "model2_2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2_2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IrVbU_A44jV",
        "outputId": "24e87301-832c-43d2-e475-2a57228edf91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 128)]             0         \n",
            "                                                                 \n",
            " GloVe_Embeddings (Embedding  (None, 128, 300)         120000300 \n",
            " )                                                               \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 4)                 4880      \n",
            "                                                                 \n",
            " dense_layer (Dense)         (None, 16)                80        \n",
            "                                                                 \n",
            " dense_layer_output (Dense)  (None, 3)                 51        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 120,005,311\n",
            "Trainable params: 5,011\n",
            "Non-trainable params: 120,000,300\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQQX08Z1dJ92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4914c14d-4640-4ec0-b183-62c84b29ce26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "22/22 [==============================] - 4s 113ms/step - loss: 0.6851 - accuracy: 0.4479 - val_loss: 0.6757 - val_accuracy: 0.4535\n",
            "Epoch 2/40\n",
            "22/22 [==============================] - 2s 92ms/step - loss: 0.6662 - accuracy: 0.4507 - val_loss: 0.6546 - val_accuracy: 0.4535\n",
            "Epoch 3/40\n",
            "22/22 [==============================] - 2s 94ms/step - loss: 0.6441 - accuracy: 0.4507 - val_loss: 0.6317 - val_accuracy: 0.4535\n",
            "Epoch 4/40\n",
            "22/22 [==============================] - 2s 90ms/step - loss: 0.6250 - accuracy: 0.4507 - val_loss: 0.6197 - val_accuracy: 0.4535\n",
            "Epoch 5/40\n",
            "22/22 [==============================] - 2s 90ms/step - loss: 0.6204 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 6/40\n",
            "22/22 [==============================] - 2s 91ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 7/40\n",
            "22/22 [==============================] - 2s 97ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 8/40\n",
            "22/22 [==============================] - 2s 100ms/step - loss: 0.6200 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 9/40\n",
            "22/22 [==============================] - 2s 96ms/step - loss: 0.6200 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 10/40\n",
            "22/22 [==============================] - 2s 95ms/step - loss: 0.6200 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 11/40\n",
            "22/22 [==============================] - 2s 95ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 12/40\n",
            "22/22 [==============================] - 2s 102ms/step - loss: 0.6200 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 13/40\n",
            "22/22 [==============================] - 2s 96ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 14/40\n",
            "22/22 [==============================] - 2s 96ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 15/40\n",
            "22/22 [==============================] - 2s 95ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 16/40\n",
            "22/22 [==============================] - 2s 95ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 17/40\n",
            "22/22 [==============================] - 2s 95ms/step - loss: 0.6200 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 18/40\n",
            "22/22 [==============================] - 2s 96ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 19/40\n",
            "22/22 [==============================] - 3s 116ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 20/40\n",
            "22/22 [==============================] - 3s 115ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 21/40\n",
            "22/22 [==============================] - 2s 103ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 22/40\n",
            "22/22 [==============================] - 2s 113ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 23/40\n",
            "22/22 [==============================] - 2s 101ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 24/40\n",
            "22/22 [==============================] - 2s 98ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 25/40\n",
            "22/22 [==============================] - 2s 100ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 26/40\n",
            "22/22 [==============================] - 2s 101ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 27/40\n",
            "22/22 [==============================] - 2s 102ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 28/40\n",
            "22/22 [==============================] - 2s 101ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 29/40\n",
            "22/22 [==============================] - 2s 103ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 30/40\n",
            "22/22 [==============================] - 2s 103ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 31/40\n",
            "22/22 [==============================] - 2s 100ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 32/40\n",
            "22/22 [==============================] - 2s 102ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 33/40\n",
            "22/22 [==============================] - 2s 102ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 34/40\n",
            "22/22 [==============================] - 2s 99ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 35/40\n",
            "22/22 [==============================] - 2s 101ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 36/40\n",
            "22/22 [==============================] - 2s 102ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 37/40\n",
            "22/22 [==============================] - 2s 101ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 38/40\n",
            "22/22 [==============================] - 2s 98ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 39/40\n",
            "22/22 [==============================] - 2s 97ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 40/40\n",
            "22/22 [==============================] - 2s 102ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n"
          ]
        }
      ],
      "source": [
        "history = model2_2.fit(x_train_pad_glove,\n",
        "                    y_train,\n",
        "                    epochs=40,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(x_dev_pad_glove, y_dev),\n",
        "                    verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results2_2 = model2_2.evaluate(x_test_pad_glove, y_test)\n",
        "print(\"Loss: {} and Accuracy: {}\".format(results2_1[0], results2_1[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jN8LB2A8BTPj",
        "outputId": "86f36079-1df4-4365-b782-0ec9cc8fac97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "42/42 [==============================] - 0s 10ms/step - loss: 0.6192 - accuracy: 0.4543\n",
            "Loss: 0.5397483110427856 and Accuracy: 0.56886225938797\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SEZIl_GdWbG"
      },
      "outputs": [],
      "source": [
        "from keras.layers.pooling import GlobalMaxPooling1D\n",
        "hidden_layer=16\n",
        "\n",
        "from keras.models import Model\n",
        "\n",
        "input = Input((128,), dtype='int32')\n",
        "embedding = createPretrainedEmbeddingLayer(wordToGlove,wordToIndex,isTrainable=False)(input)\n",
        "conv = tf.keras.layers.Conv1D(100,8)(embedding)\n",
        "pool = GlobalMaxPooling1D()(conv)\n",
        "hidden = Dense(16)(pool)\n",
        "output_layer = tf.keras.layers.Dense(units=3, name=\"dense_layer_output\", activation = 'sigmoid')(hidden)\n",
        "\n",
        "model2_cnn = Model(inputs=[input], outputs=[output_layer])\n",
        "model2_cnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2_cnn.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whAxoUcm46zG",
        "outputId": "9c9ade2b-6d17-4fbc-ee2a-c67d2c282668"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 128)]             0         \n",
            "                                                                 \n",
            " GloVe_Embeddings (Embedding  (None, 128, 300)         120000300 \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 121, 100)          240100    \n",
            "                                                                 \n",
            " global_max_pooling1d (Globa  (None, 100)              0         \n",
            " lMaxPooling1D)                                                  \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                1616      \n",
            "                                                                 \n",
            " dense_layer_output (Dense)  (None, 3)                 51        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 120,242,067\n",
            "Trainable params: 241,767\n",
            "Non-trainable params: 120,000,300\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYcGWO4ideE6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e888504-6601-4153-ea56-d159c1ea3477"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "22/22 [==============================] - 2s 98ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 2/40\n",
            "22/22 [==============================] - 2s 98ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 3/40\n",
            "22/22 [==============================] - 2s 97ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 4/40\n",
            "22/22 [==============================] - 2s 95ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 5/40\n",
            "22/22 [==============================] - 2s 96ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 6/40\n",
            "22/22 [==============================] - 2s 95ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 7/40\n",
            "22/22 [==============================] - 2s 95ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 8/40\n",
            "22/22 [==============================] - 2s 96ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 9/40\n",
            "22/22 [==============================] - 2s 96ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 10/40\n",
            "22/22 [==============================] - 2s 94ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 11/40\n",
            "22/22 [==============================] - 2s 93ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 12/40\n",
            "22/22 [==============================] - 2s 95ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 13/40\n",
            "22/22 [==============================] - 2s 96ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 14/40\n",
            "22/22 [==============================] - 2s 97ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 15/40\n",
            "22/22 [==============================] - 2s 98ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 16/40\n",
            "22/22 [==============================] - 2s 96ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 17/40\n",
            "22/22 [==============================] - 2s 96ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 18/40\n",
            "22/22 [==============================] - 2s 94ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 19/40\n",
            "22/22 [==============================] - 2s 95ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 20/40\n",
            "22/22 [==============================] - 2s 95ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 21/40\n",
            "22/22 [==============================] - 2s 97ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 22/40\n",
            "22/22 [==============================] - 2s 95ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 23/40\n",
            "22/22 [==============================] - 2s 94ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 24/40\n",
            "22/22 [==============================] - 2s 95ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 25/40\n",
            "22/22 [==============================] - 2s 95ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 26/40\n",
            "22/22 [==============================] - 2s 95ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 27/40\n",
            "22/22 [==============================] - 2s 94ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 28/40\n",
            "22/22 [==============================] - 2s 96ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 29/40\n",
            "22/22 [==============================] - 2s 97ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 30/40\n",
            "22/22 [==============================] - 2s 98ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 31/40\n",
            "22/22 [==============================] - 2s 97ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 32/40\n",
            "22/22 [==============================] - 2s 96ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 33/40\n",
            "22/22 [==============================] - 2s 96ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 34/40\n",
            "22/22 [==============================] - 2s 95ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 35/40\n",
            "22/22 [==============================] - 2s 98ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 36/40\n",
            "22/22 [==============================] - 2s 98ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 37/40\n",
            "22/22 [==============================] - 2s 96ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 38/40\n",
            "22/22 [==============================] - 2s 96ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 39/40\n",
            "22/22 [==============================] - 2s 94ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
            "Epoch 40/40\n",
            "22/22 [==============================] - 2s 95ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n"
          ]
        }
      ],
      "source": [
        "history = model2_2.fit(x_train_pad_glove,\n",
        "                    y_train,\n",
        "                    epochs=40,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(x_dev_pad, y_dev),\n",
        "                    verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results2_cnn = model2_cnn.evaluate(x_test_pad_glove, y_test)\n",
        "print(\"Loss: {} and Accuracy: {}\".format(results2_1[0], results2_1[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CacVrEpDBWeS",
        "outputId": "66aa7a1d-472c-413a-9c94-1ad67d1af1e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "42/42 [==============================] - 1s 18ms/step - loss: 0.8397 - accuracy: 0.3308\n",
            "Loss: 0.5397483110427856 and Accuracy: 0.56886225938797\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-bZ5SCHiIMl"
      },
      "source": [
        "#  Model 3: Model with multiple-input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G85QM3lSV7qp"
      },
      "source": [
        "Model 1 and 2 are copied from lab4. We build new models in this section. \n",
        "\n",
        "In models 1 and 2, we combine the reviews and aspects to input into the models. In model 3, we separately input these two data into the model and use different layers to analyze them. \n",
        "\n",
        "(This will give us a model similar to a simplified version of the Xue & Li model from the lectures - we have a separate paths through the network for the aspect embedding and the sentence, being combined - but we don't have to use gating like Xue & Li)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ztiFcOWuA0xH"
      },
      "outputs": [],
      "source": [
        "# Padding review and aspect separately\n",
        "x_train_review_pad_glove = keras.preprocessing.sequence.pad_sequences(x_train_review_glove,\n",
        "                                                        value=0,\n",
        "                                                        padding='post',\n",
        "                                                        maxlen=128)\n",
        "\n",
        "x_dev_review_pad_glove = keras.preprocessing.sequence.pad_sequences(x_dev_review_glove,\n",
        "                                                       value=0,\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=128)\n",
        "\n",
        "x_test_review_pad_glove = keras.preprocessing.sequence.pad_sequences(x_test_review_glove,\n",
        "                                                       value=0,\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=128)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "la = []\n",
        "for i in range(len(x_train_aspect_glove)):\n",
        "  la.append([x_train_aspect_glove[i]])"
      ],
      "metadata": {
        "id": "yNga1VlARXvB"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lb = []\n",
        "for i in range(len(x_dev_aspect_glove)):\n",
        "  lb.append([x_dev_aspect_glove[i]])"
      ],
      "metadata": {
        "id": "CDoUOSyVRs7B"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lc = []\n",
        "for i in range(len(x_test_aspect_glove)):\n",
        "  lc.append([x_test_aspect_glove[i]])"
      ],
      "metadata": {
        "id": "uM16-DwHRtYI"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_aspect_pad_glove = keras.preprocessing.sequence.pad_sequences(la,\n",
        "                                                       value=0,\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=16)\n",
        "\n",
        "x_dev_aspect_pad_glove = keras.preprocessing.sequence.pad_sequences(lb,\n",
        "                                                       value=0,\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=16)\n",
        "\n",
        "x_test_aspect_pad_glove = keras.preprocessing.sequence.pad_sequences(lc,\n",
        "                                                       value=0,\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=16)"
      ],
      "metadata": {
        "id": "vRFRJsWvPsGR"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting our data ready for the  model inputs, hence converting them to numpy arrays."
      ],
      "metadata": {
        "id": "-kGr67__-FJ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_review_pad_glove = np.array(x_train_review_pad_glove)\n",
        "x_dev_review_pad_glove = np.array(x_dev_review_pad_glove)\n",
        "x_test_review_pad_glove = np.array(x_test_review_pad_glove)\n",
        "x_train_aspect_pad_glove = np.array(x_train_aspect_pad_glove)\n",
        "x_dev_aspect_pad_glove = np.array(x_dev_aspect_pad_glove)\n",
        "x_test_aspect_pad_glove = np.array(x_test_aspect_pad_glove)"
      ],
      "metadata": {
        "id": "7If7qUEHv0Xf"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExgX8bxpVgps"
      },
      "source": [
        "## Model 3-1 Neural bag of words model with multiple-input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-TN6yup01mC"
      },
      "source": [
        "Model 3-1 needs you to modify the model 2-1 to be compatible with multiple-input.\n",
        "You could find some tutorial examples from (https://www.pyimagesearch.com/2019/02/04/keras-multiple-inputs-and-mixed-data/).\n",
        "\n",
        "Please print the model summary and visualize it using vis_utils."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.initializers import Constant\n",
        "\n",
        "def createPretrainedEmbeddingLayer(wordToGlove, wordToIndex, isTrainable):\n",
        "    vocabLen = len(wordToIndex) + 1  \n",
        "    embDim = next(iter(wordToGlove.values())).shape[0]  \n",
        "   \n",
        "    embeddingMatrix = np.zeros((vocabLen, embDim))  \n",
        "    for word, index in wordToIndex.items():\n",
        "        embeddingMatrix[index, :] = wordToGlove[word] \n",
        "\n",
        "    embeddingLayer = Embedding(vocabLen, embDim, embeddings_initializer=Constant(embeddingMatrix), trainable=isTrainable)\n",
        "    return embeddingLayer"
      ],
      "metadata": {
        "id": "AGGNczjn6Sx7"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTgD_gMzXa1z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4914965b-6cfc-460a-e2db-db7de299687b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_7\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_6 (InputLayer)           [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " input_7 (InputLayer)           [(None, 16)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 128, 300)     120000300   ['input_6[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, 16, 300)      120000300   ['input_7[0][0]']                \n",
            "                                                                                                  \n",
            " global_average_pooling1d_maske  (None, 300)         0           ['embedding[0][0]']              \n",
            " d (GlobalAveragePooling1DMaske                                                                   \n",
            " d)                                                                                               \n",
            "                                                                                                  \n",
            " global_average_pooling1d_maske  (None, 300)         0           ['embedding_1[0][0]']            \n",
            " d_1 (GlobalAveragePooling1DMas                                                                   \n",
            " ked)                                                                                             \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 16)           4816        ['global_average_pooling1d_masked\n",
            "                                                                 [0][0]']                         \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 16)           4816        ['global_average_pooling1d_masked\n",
            "                                                                 _1[0][0]']                       \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 32)           0           ['dense_2[0][0]',                \n",
            "                                                                  'dense_3[0][0]']                \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 3)            99          ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 240,010,331\n",
            "Trainable params: 9,731\n",
            "Non-trainable params: 240,000,600\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# your code goes here\n",
        "\n",
        "from keras.layers import Input, Embedding, Dense, concatenate\n",
        "from keras.models import Model\n",
        "from keras.layers.core import Dense, Reshape\n",
        "\n",
        "# define two sets of inputs\n",
        "A = Input(shape=(128,), dtype='int32')\n",
        "B = Input(shape=(16,), dtype='int32')\n",
        "\n",
        "\n",
        "# the first branch operates on the first input\n",
        "glove_embeddingA = createPretrainedEmbeddingLayer(wordToGlove,wordToIndex,isTrainable=False)(A)\n",
        "globalavgpoolinglayerA = GlobalAveragePooling1DMasked()(glove_embeddingA)\n",
        "hidden1 = Dense(16)(globalavgpoolinglayerA)\n",
        "\n",
        "x = Model(inputs=A, outputs=hidden1)\n",
        "\n",
        "# the second branch opreates on the second input\n",
        "glove_embeddingB = createPretrainedEmbeddingLayer(wordToGlove,wordToIndex,isTrainable=False)(B)\n",
        "globalavgpoolinglayerB = GlobalAveragePooling1DMasked()(glove_embeddingB)\n",
        "hidden2 = Dense(16)(globalavgpoolinglayerB)\n",
        "\n",
        "y = Model(inputs=B, outputs=hidden2)\n",
        "\n",
        "# combine the output of the two branches\n",
        "combined = concatenate([x.output, y.output])\n",
        "\n",
        "\n",
        "z = Dense(3, activation='softmax')(combined)\n",
        "\n",
        "model31 = Model(inputs=[x.input, y.input], outputs=z)\n",
        "model31.compile(optimizer='adam', loss='binary_crossentropy', metrics=[\"accuracy\"])\n",
        "model31.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlJbDRHeAMIh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "ad1a637f-c18f-47d6-ce1a-7f8d752f731f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg height=\"392pt\" viewBox=\"0.00 0.00 1002.00 470.00\" width=\"835pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(.8333 .8333) rotate(0) translate(4 466)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" points=\"-4,4 -4,-466 998,-466 998,4 -4,4\" stroke=\"transparent\"/>\n<!-- 139918925682960 -->\n<g class=\"node\" id=\"node1\">\n<title>139918925682960</title>\n<polygon fill=\"none\" points=\"77,-415.5 77,-461.5 407,-461.5 407,-415.5 77,-415.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"117\" y=\"-446.3\">input_6</text>\n<polyline fill=\"none\" points=\"77,-438.5 157,-438.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"117\" y=\"-423.3\">InputLayer</text>\n<polyline fill=\"none\" points=\"157,-415.5 157,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"186\" y=\"-446.3\">input:</text>\n<polyline fill=\"none\" points=\"157,-438.5 215,-438.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"186\" y=\"-423.3\">output:</text>\n<polyline fill=\"none\" points=\"215,-415.5 215,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"263\" y=\"-434.8\">[(None, 128)]</text>\n<polyline fill=\"none\" points=\"311,-415.5 311,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"359\" y=\"-434.8\">[(None, 128)]</text>\n</g>\n<!-- 139918926296144 -->\n<g class=\"node\" id=\"node3\">\n<title>139918926296144</title>\n<polygon fill=\"none\" points=\"69,-332.5 69,-378.5 415,-378.5 415,-332.5 69,-332.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"111\" y=\"-363.3\">embedding</text>\n<polyline fill=\"none\" points=\"69,-355.5 153,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"111\" y=\"-340.3\">Embedding</text>\n<polyline fill=\"none\" points=\"153,-332.5 153,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"182\" y=\"-363.3\">input:</text>\n<polyline fill=\"none\" points=\"153,-355.5 211,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"182\" y=\"-340.3\">output:</text>\n<polyline fill=\"none\" points=\"211,-332.5 211,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"254.5\" y=\"-351.8\">(None, 128)</text>\n<polyline fill=\"none\" points=\"298,-332.5 298,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"356.5\" y=\"-351.8\">(None, 128, 300)</text>\n</g>\n<!-- 139918925682960&#45;&gt;139918926296144 -->\n<g class=\"edge\" id=\"edge1\">\n<title>139918925682960-&gt;139918926296144</title>\n<path d=\"M242,-415.3799C242,-407.1745 242,-397.7679 242,-388.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"245.5001,-388.784 242,-378.784 238.5001,-388.784 245.5001,-388.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139916254371216 -->\n<g class=\"node\" id=\"node2\">\n<title>139916254371216</title>\n<polygon fill=\"none\" points=\"590,-415.5 590,-461.5 906,-461.5 906,-415.5 590,-415.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"630\" y=\"-446.3\">input_7</text>\n<polyline fill=\"none\" points=\"590,-438.5 670,-438.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"630\" y=\"-423.3\">InputLayer</text>\n<polyline fill=\"none\" points=\"670,-415.5 670,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"699\" y=\"-446.3\">input:</text>\n<polyline fill=\"none\" points=\"670,-438.5 728,-438.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"699\" y=\"-423.3\">output:</text>\n<polyline fill=\"none\" points=\"728,-415.5 728,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"772.5\" y=\"-434.8\">[(None, 16)]</text>\n<polyline fill=\"none\" points=\"817,-415.5 817,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"861.5\" y=\"-434.8\">[(None, 16)]</text>\n</g>\n<!-- 139916101716240 -->\n<g class=\"node\" id=\"node4\">\n<title>139916101716240</title>\n<polygon fill=\"none\" points=\"576,-332.5 576,-378.5 920,-378.5 920,-332.5 576,-332.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"624\" y=\"-363.3\">embedding_1</text>\n<polyline fill=\"none\" points=\"576,-355.5 672,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"624\" y=\"-340.3\">Embedding</text>\n<polyline fill=\"none\" points=\"672,-332.5 672,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"701\" y=\"-363.3\">input:</text>\n<polyline fill=\"none\" points=\"672,-355.5 730,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"701\" y=\"-340.3\">output:</text>\n<polyline fill=\"none\" points=\"730,-332.5 730,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"770\" y=\"-351.8\">(None, 16)</text>\n<polyline fill=\"none\" points=\"810,-332.5 810,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"865\" y=\"-351.8\">(None, 16, 300)</text>\n</g>\n<!-- 139916254371216&#45;&gt;139916101716240 -->\n<g class=\"edge\" id=\"edge2\">\n<title>139916254371216-&gt;139916101716240</title>\n<path d=\"M748,-415.3799C748,-407.1745 748,-397.7679 748,-388.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"751.5001,-388.784 748,-378.784 744.5001,-388.784 751.5001,-388.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139916135043024 -->\n<g class=\"node\" id=\"node5\">\n<title>139916135043024</title>\n<polygon fill=\"none\" points=\"0,-249.5 0,-295.5 484,-295.5 484,-249.5 0,-249.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"111\" y=\"-280.3\">global_average_pooling1d_masked</text>\n<polyline fill=\"none\" points=\"0,-272.5 222,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"111\" y=\"-257.3\">GlobalAveragePooling1DMasked</text>\n<polyline fill=\"none\" points=\"222,-249.5 222,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"251\" y=\"-280.3\">input:</text>\n<polyline fill=\"none\" points=\"222,-272.5 280,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"251\" y=\"-257.3\">output:</text>\n<polyline fill=\"none\" points=\"280,-249.5 280,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"338.5\" y=\"-268.8\">(None, 128, 300)</text>\n<polyline fill=\"none\" points=\"397,-249.5 397,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"440.5\" y=\"-268.8\">(None, 300)</text>\n</g>\n<!-- 139918926296144&#45;&gt;139916135043024 -->\n<g class=\"edge\" id=\"edge3\">\n<title>139918926296144-&gt;139916135043024</title>\n<path d=\"M242,-332.3799C242,-324.1745 242,-314.7679 242,-305.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"245.5001,-305.784 242,-295.784 238.5001,-305.784 245.5001,-305.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139916158134288 -->\n<g class=\"node\" id=\"node6\">\n<title>139916158134288</title>\n<polygon fill=\"none\" points=\"502,-249.5 502,-295.5 994,-295.5 994,-249.5 502,-249.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"620.5\" y=\"-280.3\">global_average_pooling1d_masked_1</text>\n<polyline fill=\"none\" points=\"502,-272.5 739,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"620.5\" y=\"-257.3\">GlobalAveragePooling1DMasked</text>\n<polyline fill=\"none\" points=\"739,-249.5 739,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"768\" y=\"-280.3\">input:</text>\n<polyline fill=\"none\" points=\"739,-272.5 797,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"768\" y=\"-257.3\">output:</text>\n<polyline fill=\"none\" points=\"797,-249.5 797,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"852\" y=\"-268.8\">(None, 16, 300)</text>\n<polyline fill=\"none\" points=\"907,-249.5 907,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"950.5\" y=\"-268.8\">(None, 300)</text>\n</g>\n<!-- 139916101716240&#45;&gt;139916158134288 -->\n<g class=\"edge\" id=\"edge4\">\n<title>139916101716240-&gt;139916158134288</title>\n<path d=\"M748,-332.3799C748,-324.1745 748,-314.7679 748,-305.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"751.5001,-305.784 748,-295.784 744.5001,-305.784 751.5001,-305.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139916158132944 -->\n<g class=\"node\" id=\"node7\">\n<title>139916158132944</title>\n<polygon fill=\"none\" points=\"196.5,-166.5 196.5,-212.5 485.5,-212.5 485.5,-166.5 196.5,-166.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"228.5\" y=\"-197.3\">dense_2</text>\n<polyline fill=\"none\" points=\"196.5,-189.5 260.5,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"228.5\" y=\"-174.3\">Dense</text>\n<polyline fill=\"none\" points=\"260.5,-166.5 260.5,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"289.5\" y=\"-197.3\">input:</text>\n<polyline fill=\"none\" points=\"260.5,-189.5 318.5,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"289.5\" y=\"-174.3\">output:</text>\n<polyline fill=\"none\" points=\"318.5,-166.5 318.5,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"362\" y=\"-185.8\">(None, 300)</text>\n<polyline fill=\"none\" points=\"405.5,-166.5 405.5,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"445.5\" y=\"-185.8\">(None, 16)</text>\n</g>\n<!-- 139916135043024&#45;&gt;139916158132944 -->\n<g class=\"edge\" id=\"edge5\">\n<title>139916135043024-&gt;139916158132944</title>\n<path d=\"M269.577,-249.3799C280.6407,-240.1043 293.5354,-229.2936 305.2999,-219.4304\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"307.813,-221.8908 313.2275,-212.784 303.3157,-216.5266 307.813,-221.8908\" stroke=\"#000000\"/>\n</g>\n<!-- 139918919234832 -->\n<g class=\"node\" id=\"node8\">\n<title>139918919234832</title>\n<polygon fill=\"none\" points=\"553.5,-166.5 553.5,-212.5 842.5,-212.5 842.5,-166.5 553.5,-166.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"585.5\" y=\"-197.3\">dense_3</text>\n<polyline fill=\"none\" points=\"553.5,-189.5 617.5,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"585.5\" y=\"-174.3\">Dense</text>\n<polyline fill=\"none\" points=\"617.5,-166.5 617.5,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"646.5\" y=\"-197.3\">input:</text>\n<polyline fill=\"none\" points=\"617.5,-189.5 675.5,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"646.5\" y=\"-174.3\">output:</text>\n<polyline fill=\"none\" points=\"675.5,-166.5 675.5,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"719\" y=\"-185.8\">(None, 300)</text>\n<polyline fill=\"none\" points=\"762.5,-166.5 762.5,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"802.5\" y=\"-185.8\">(None, 16)</text>\n</g>\n<!-- 139916158134288&#45;&gt;139918919234832 -->\n<g class=\"edge\" id=\"edge6\">\n<title>139916158134288-&gt;139918919234832</title>\n<path d=\"M734.0722,-249.3799C728.8606,-240.7286 722.8445,-230.7419 717.2374,-221.4341\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"720.1847,-219.5438 712.0265,-212.784 714.1886,-223.1559 720.1847,-219.5438\" stroke=\"#000000\"/>\n</g>\n<!-- 139916254291600 -->\n<g class=\"node\" id=\"node9\">\n<title>139916254291600</title>\n<polygon fill=\"none\" points=\"302.5,-83.5 302.5,-129.5 685.5,-129.5 685.5,-83.5 302.5,-83.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"345.5\" y=\"-114.3\">concatenate</text>\n<polyline fill=\"none\" points=\"302.5,-106.5 388.5,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"345.5\" y=\"-91.3\">Concatenate</text>\n<polyline fill=\"none\" points=\"388.5,-83.5 388.5,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"417.5\" y=\"-114.3\">input:</text>\n<polyline fill=\"none\" points=\"388.5,-106.5 446.5,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"417.5\" y=\"-91.3\">output:</text>\n<polyline fill=\"none\" points=\"446.5,-83.5 446.5,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"526\" y=\"-102.8\">[(None, 16), (None, 16)]</text>\n<polyline fill=\"none\" points=\"605.5,-83.5 605.5,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"645.5\" y=\"-102.8\">(None, 32)</text>\n</g>\n<!-- 139916158132944&#45;&gt;139916254291600 -->\n<g class=\"edge\" id=\"edge7\">\n<title>139916158132944-&gt;139916254291600</title>\n<path d=\"M383.619,-166.3799C401.8624,-156.4832 423.3273,-144.8388 442.4547,-134.4625\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"444.2235,-137.4849 451.3444,-129.6399 440.8856,-131.3319 444.2235,-137.4849\" stroke=\"#000000\"/>\n</g>\n<!-- 139918919234832&#45;&gt;139916254291600 -->\n<g class=\"edge\" id=\"edge8\">\n<title>139918919234832-&gt;139916254291600</title>\n<path d=\"M641.4455,-166.4901C616.2411,-156.2353 586.3831,-144.0872 560.15,-133.414\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"561.2462,-130.0814 550.6645,-129.5547 558.6081,-136.5653 561.2462,-130.0814\" stroke=\"#000000\"/>\n</g>\n<!-- 139916158242576 -->\n<g class=\"node\" id=\"node10\">\n<title>139916158242576</title>\n<polygon fill=\"none\" points=\"357,-.5 357,-46.5 631,-46.5 631,-.5 357,-.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"389\" y=\"-31.3\">dense_4</text>\n<polyline fill=\"none\" points=\"357,-23.5 421,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"389\" y=\"-8.3\">Dense</text>\n<polyline fill=\"none\" points=\"421,-.5 421,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"450\" y=\"-31.3\">input:</text>\n<polyline fill=\"none\" points=\"421,-23.5 479,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"450\" y=\"-8.3\">output:</text>\n<polyline fill=\"none\" points=\"479,-.5 479,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"519\" y=\"-19.8\">(None, 32)</text>\n<polyline fill=\"none\" points=\"559,-.5 559,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"595\" y=\"-19.8\">(None, 3)</text>\n</g>\n<!-- 139916254291600&#45;&gt;139916158242576 -->\n<g class=\"edge\" id=\"edge9\">\n<title>139916254291600-&gt;139916158242576</title>\n<path d=\"M494,-83.3799C494,-75.1745 494,-65.7679 494,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"497.5001,-56.784 494,-46.784 490.5001,-56.784 497.5001,-56.784\" stroke=\"#000000\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "from IPython.display import SVG\n",
        "from keras.utils import vis_utils\n",
        "SVG(vis_utils.model_to_dot(model31, show_shapes=True, show_layer_names=True, dpi=60).create(prog='dot', format='svg'))  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVl_cKt903Z9"
      },
      "source": [
        "Train and evaluate your model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEzz9deDF2rc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45dd42b9-da1c-4653-c14d-056b3a38742f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "22/22 [==============================] - 2s 32ms/step - loss: 0.6571 - accuracy: 0.4455 - val_loss: 0.6278 - val_accuracy: 0.4535\n",
            "Epoch 2/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.6169 - accuracy: 0.4507 - val_loss: 0.6059 - val_accuracy: 0.4535\n",
            "Epoch 3/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.6026 - accuracy: 0.4508 - val_loss: 0.5943 - val_accuracy: 0.4542\n",
            "Epoch 4/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.5916 - accuracy: 0.4575 - val_loss: 0.5820 - val_accuracy: 0.4752\n",
            "Epoch 5/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.5801 - accuracy: 0.4914 - val_loss: 0.5694 - val_accuracy: 0.5203\n",
            "Epoch 6/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.5684 - accuracy: 0.5326 - val_loss: 0.5574 - val_accuracy: 0.5511\n",
            "Epoch 7/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.5574 - accuracy: 0.5448 - val_loss: 0.5463 - val_accuracy: 0.5653\n",
            "Epoch 8/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.5476 - accuracy: 0.5645 - val_loss: 0.5367 - val_accuracy: 0.5758\n",
            "Epoch 9/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.5391 - accuracy: 0.5768 - val_loss: 0.5284 - val_accuracy: 0.5848\n",
            "Epoch 10/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.5320 - accuracy: 0.5813 - val_loss: 0.5217 - val_accuracy: 0.5856\n",
            "Epoch 11/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.5262 - accuracy: 0.5847 - val_loss: 0.5162 - val_accuracy: 0.5953\n",
            "Epoch 12/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.5212 - accuracy: 0.5881 - val_loss: 0.5116 - val_accuracy: 0.5983\n",
            "Epoch 13/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.5171 - accuracy: 0.5890 - val_loss: 0.5081 - val_accuracy: 0.5983\n",
            "Epoch 14/200\n",
            "22/22 [==============================] - 1s 30ms/step - loss: 0.5137 - accuracy: 0.5969 - val_loss: 0.5047 - val_accuracy: 0.6066\n",
            "Epoch 15/200\n",
            "22/22 [==============================] - 1s 41ms/step - loss: 0.5105 - accuracy: 0.5978 - val_loss: 0.5020 - val_accuracy: 0.6111\n",
            "Epoch 16/200\n",
            "22/22 [==============================] - 1s 41ms/step - loss: 0.5077 - accuracy: 0.6047 - val_loss: 0.4996 - val_accuracy: 0.6126\n",
            "Epoch 17/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.5052 - accuracy: 0.6075 - val_loss: 0.4974 - val_accuracy: 0.6149\n",
            "Epoch 18/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.5029 - accuracy: 0.6120 - val_loss: 0.4955 - val_accuracy: 0.6194\n",
            "Epoch 19/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.5009 - accuracy: 0.6133 - val_loss: 0.4937 - val_accuracy: 0.6231\n",
            "Epoch 20/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4989 - accuracy: 0.6159 - val_loss: 0.4917 - val_accuracy: 0.6269\n",
            "Epoch 21/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.4971 - accuracy: 0.6171 - val_loss: 0.4906 - val_accuracy: 0.6299\n",
            "Epoch 22/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4950 - accuracy: 0.6196 - val_loss: 0.4887 - val_accuracy: 0.6321\n",
            "Epoch 23/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4934 - accuracy: 0.6206 - val_loss: 0.4879 - val_accuracy: 0.6321\n",
            "Epoch 24/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4918 - accuracy: 0.6229 - val_loss: 0.4860 - val_accuracy: 0.6374\n",
            "Epoch 25/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4903 - accuracy: 0.6229 - val_loss: 0.4850 - val_accuracy: 0.6359\n",
            "Epoch 26/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4887 - accuracy: 0.6256 - val_loss: 0.4840 - val_accuracy: 0.6366\n",
            "Epoch 27/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4876 - accuracy: 0.6261 - val_loss: 0.4828 - val_accuracy: 0.6389\n",
            "Epoch 28/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4862 - accuracy: 0.6284 - val_loss: 0.4817 - val_accuracy: 0.6411\n",
            "Epoch 29/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4850 - accuracy: 0.6304 - val_loss: 0.4809 - val_accuracy: 0.6404\n",
            "Epoch 30/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4838 - accuracy: 0.6296 - val_loss: 0.4802 - val_accuracy: 0.6441\n",
            "Epoch 31/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4825 - accuracy: 0.6300 - val_loss: 0.4792 - val_accuracy: 0.6426\n",
            "Epoch 32/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4814 - accuracy: 0.6313 - val_loss: 0.4784 - val_accuracy: 0.6441\n",
            "Epoch 33/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4804 - accuracy: 0.6316 - val_loss: 0.4778 - val_accuracy: 0.6441\n",
            "Epoch 34/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4794 - accuracy: 0.6337 - val_loss: 0.4771 - val_accuracy: 0.6486\n",
            "Epoch 35/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4785 - accuracy: 0.6317 - val_loss: 0.4769 - val_accuracy: 0.6471\n",
            "Epoch 36/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4777 - accuracy: 0.6342 - val_loss: 0.4758 - val_accuracy: 0.6509\n",
            "Epoch 37/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4769 - accuracy: 0.6379 - val_loss: 0.4760 - val_accuracy: 0.6441\n",
            "Epoch 38/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4761 - accuracy: 0.6365 - val_loss: 0.4756 - val_accuracy: 0.6434\n",
            "Epoch 39/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4754 - accuracy: 0.6377 - val_loss: 0.4742 - val_accuracy: 0.6479\n",
            "Epoch 40/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4745 - accuracy: 0.6379 - val_loss: 0.4737 - val_accuracy: 0.6547\n",
            "Epoch 41/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4741 - accuracy: 0.6400 - val_loss: 0.4733 - val_accuracy: 0.6569\n",
            "Epoch 42/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4733 - accuracy: 0.6406 - val_loss: 0.4742 - val_accuracy: 0.6456\n",
            "Epoch 43/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4726 - accuracy: 0.6387 - val_loss: 0.4729 - val_accuracy: 0.6524\n",
            "Epoch 44/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4720 - accuracy: 0.6412 - val_loss: 0.4725 - val_accuracy: 0.6502\n",
            "Epoch 45/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4715 - accuracy: 0.6406 - val_loss: 0.4720 - val_accuracy: 0.6554\n",
            "Epoch 46/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4709 - accuracy: 0.6415 - val_loss: 0.4721 - val_accuracy: 0.6502\n",
            "Epoch 47/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4703 - accuracy: 0.6432 - val_loss: 0.4714 - val_accuracy: 0.6547\n",
            "Epoch 48/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4699 - accuracy: 0.6446 - val_loss: 0.4718 - val_accuracy: 0.6502\n",
            "Epoch 49/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4693 - accuracy: 0.6460 - val_loss: 0.4709 - val_accuracy: 0.6554\n",
            "Epoch 50/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4690 - accuracy: 0.6460 - val_loss: 0.4708 - val_accuracy: 0.6517\n",
            "Epoch 51/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4686 - accuracy: 0.6465 - val_loss: 0.4708 - val_accuracy: 0.6524\n",
            "Epoch 52/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.4679 - accuracy: 0.6462 - val_loss: 0.4704 - val_accuracy: 0.6532\n",
            "Epoch 53/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4677 - accuracy: 0.6472 - val_loss: 0.4702 - val_accuracy: 0.6539\n",
            "Epoch 54/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4670 - accuracy: 0.6479 - val_loss: 0.4704 - val_accuracy: 0.6517\n",
            "Epoch 55/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4669 - accuracy: 0.6477 - val_loss: 0.4698 - val_accuracy: 0.6524\n",
            "Epoch 56/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4663 - accuracy: 0.6481 - val_loss: 0.4700 - val_accuracy: 0.6494\n",
            "Epoch 57/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4663 - accuracy: 0.6488 - val_loss: 0.4696 - val_accuracy: 0.6562\n",
            "Epoch 58/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4657 - accuracy: 0.6485 - val_loss: 0.4698 - val_accuracy: 0.6449\n",
            "Epoch 59/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4655 - accuracy: 0.6490 - val_loss: 0.4691 - val_accuracy: 0.6584\n",
            "Epoch 60/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4654 - accuracy: 0.6504 - val_loss: 0.4692 - val_accuracy: 0.6562\n",
            "Epoch 61/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4646 - accuracy: 0.6500 - val_loss: 0.4694 - val_accuracy: 0.6486\n",
            "Epoch 62/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.4644 - accuracy: 0.6505 - val_loss: 0.4699 - val_accuracy: 0.6464\n",
            "Epoch 63/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.4641 - accuracy: 0.6508 - val_loss: 0.4690 - val_accuracy: 0.6524\n",
            "Epoch 64/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4638 - accuracy: 0.6513 - val_loss: 0.4689 - val_accuracy: 0.6471\n",
            "Epoch 65/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4635 - accuracy: 0.6509 - val_loss: 0.4689 - val_accuracy: 0.6547\n",
            "Epoch 66/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4633 - accuracy: 0.6505 - val_loss: 0.4696 - val_accuracy: 0.6441\n",
            "Epoch 67/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.4628 - accuracy: 0.6519 - val_loss: 0.4690 - val_accuracy: 0.6532\n",
            "Epoch 68/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4627 - accuracy: 0.6516 - val_loss: 0.4685 - val_accuracy: 0.6524\n",
            "Epoch 69/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4623 - accuracy: 0.6523 - val_loss: 0.4685 - val_accuracy: 0.6517\n",
            "Epoch 70/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4623 - accuracy: 0.6534 - val_loss: 0.4694 - val_accuracy: 0.6479\n",
            "Epoch 71/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.4622 - accuracy: 0.6496 - val_loss: 0.4682 - val_accuracy: 0.6569\n",
            "Epoch 72/200\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.4616 - accuracy: 0.6547 - val_loss: 0.4685 - val_accuracy: 0.6532\n",
            "Epoch 73/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4616 - accuracy: 0.6532 - val_loss: 0.4683 - val_accuracy: 0.6509\n",
            "Epoch 74/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.4612 - accuracy: 0.6545 - val_loss: 0.4689 - val_accuracy: 0.6464\n",
            "Epoch 75/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4613 - accuracy: 0.6547 - val_loss: 0.4688 - val_accuracy: 0.6517\n",
            "Epoch 76/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4610 - accuracy: 0.6550 - val_loss: 0.4685 - val_accuracy: 0.6502\n",
            "Epoch 77/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4605 - accuracy: 0.6550 - val_loss: 0.4685 - val_accuracy: 0.6479\n",
            "Epoch 78/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4604 - accuracy: 0.6547 - val_loss: 0.4684 - val_accuracy: 0.6517\n",
            "Epoch 79/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4601 - accuracy: 0.6559 - val_loss: 0.4686 - val_accuracy: 0.6471\n",
            "Epoch 80/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4601 - accuracy: 0.6564 - val_loss: 0.4683 - val_accuracy: 0.6517\n",
            "Epoch 81/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4601 - accuracy: 0.6568 - val_loss: 0.4683 - val_accuracy: 0.6486\n",
            "Epoch 82/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4595 - accuracy: 0.6559 - val_loss: 0.4686 - val_accuracy: 0.6494\n",
            "Epoch 83/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4594 - accuracy: 0.6565 - val_loss: 0.4682 - val_accuracy: 0.6532\n",
            "Epoch 84/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4591 - accuracy: 0.6564 - val_loss: 0.4684 - val_accuracy: 0.6479\n",
            "Epoch 85/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.4592 - accuracy: 0.6562 - val_loss: 0.4686 - val_accuracy: 0.6486\n",
            "Epoch 86/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4588 - accuracy: 0.6566 - val_loss: 0.4680 - val_accuracy: 0.6486\n",
            "Epoch 87/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4590 - accuracy: 0.6551 - val_loss: 0.4681 - val_accuracy: 0.6502\n",
            "Epoch 88/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4584 - accuracy: 0.6581 - val_loss: 0.4685 - val_accuracy: 0.6509\n",
            "Epoch 89/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4586 - accuracy: 0.6573 - val_loss: 0.4687 - val_accuracy: 0.6524\n",
            "Epoch 90/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4585 - accuracy: 0.6574 - val_loss: 0.4684 - val_accuracy: 0.6502\n",
            "Epoch 91/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4584 - accuracy: 0.6578 - val_loss: 0.4680 - val_accuracy: 0.6517\n",
            "Epoch 92/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4581 - accuracy: 0.6593 - val_loss: 0.4680 - val_accuracy: 0.6494\n",
            "Epoch 93/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4577 - accuracy: 0.6593 - val_loss: 0.4682 - val_accuracy: 0.6456\n",
            "Epoch 94/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4577 - accuracy: 0.6578 - val_loss: 0.4682 - val_accuracy: 0.6502\n",
            "Epoch 95/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4574 - accuracy: 0.6583 - val_loss: 0.4683 - val_accuracy: 0.6502\n",
            "Epoch 96/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4575 - accuracy: 0.6573 - val_loss: 0.4684 - val_accuracy: 0.6486\n",
            "Epoch 97/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.4573 - accuracy: 0.6591 - val_loss: 0.4684 - val_accuracy: 0.6464\n",
            "Epoch 98/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4570 - accuracy: 0.6607 - val_loss: 0.4681 - val_accuracy: 0.6509\n",
            "Epoch 99/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4571 - accuracy: 0.6589 - val_loss: 0.4689 - val_accuracy: 0.6494\n",
            "Epoch 100/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4572 - accuracy: 0.6576 - val_loss: 0.4685 - val_accuracy: 0.6517\n",
            "Epoch 101/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.4569 - accuracy: 0.6593 - val_loss: 0.4685 - val_accuracy: 0.6517\n",
            "Epoch 102/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4565 - accuracy: 0.6599 - val_loss: 0.4682 - val_accuracy: 0.6547\n",
            "Epoch 103/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.4566 - accuracy: 0.6598 - val_loss: 0.4684 - val_accuracy: 0.6479\n",
            "Epoch 104/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4565 - accuracy: 0.6586 - val_loss: 0.4685 - val_accuracy: 0.6524\n",
            "Epoch 105/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4563 - accuracy: 0.6599 - val_loss: 0.4689 - val_accuracy: 0.6502\n",
            "Epoch 106/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4563 - accuracy: 0.6605 - val_loss: 0.4683 - val_accuracy: 0.6517\n",
            "Epoch 107/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4559 - accuracy: 0.6603 - val_loss: 0.4700 - val_accuracy: 0.6509\n",
            "Epoch 108/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4562 - accuracy: 0.6600 - val_loss: 0.4692 - val_accuracy: 0.6539\n",
            "Epoch 109/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4558 - accuracy: 0.6602 - val_loss: 0.4696 - val_accuracy: 0.6524\n",
            "Epoch 110/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4558 - accuracy: 0.6604 - val_loss: 0.4688 - val_accuracy: 0.6524\n",
            "Epoch 111/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4555 - accuracy: 0.6606 - val_loss: 0.4686 - val_accuracy: 0.6547\n",
            "Epoch 112/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4556 - accuracy: 0.6606 - val_loss: 0.4692 - val_accuracy: 0.6539\n",
            "Epoch 113/200\n",
            "22/22 [==============================] - 1s 29ms/step - loss: 0.4554 - accuracy: 0.6623 - val_loss: 0.4688 - val_accuracy: 0.6524\n",
            "Epoch 114/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.4552 - accuracy: 0.6621 - val_loss: 0.4684 - val_accuracy: 0.6517\n",
            "Epoch 115/200\n",
            "22/22 [==============================] - 1s 30ms/step - loss: 0.4553 - accuracy: 0.6615 - val_loss: 0.4684 - val_accuracy: 0.6524\n",
            "Epoch 116/200\n",
            "22/22 [==============================] - 1s 29ms/step - loss: 0.4549 - accuracy: 0.6634 - val_loss: 0.4717 - val_accuracy: 0.6486\n",
            "Epoch 117/200\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.4552 - accuracy: 0.6610 - val_loss: 0.4686 - val_accuracy: 0.6524\n",
            "Epoch 118/200\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.4548 - accuracy: 0.6601 - val_loss: 0.4692 - val_accuracy: 0.6486\n",
            "Epoch 119/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.4547 - accuracy: 0.6621 - val_loss: 0.4691 - val_accuracy: 0.6509\n",
            "Epoch 120/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.4548 - accuracy: 0.6594 - val_loss: 0.4686 - val_accuracy: 0.6539\n",
            "Epoch 121/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.4546 - accuracy: 0.6619 - val_loss: 0.4689 - val_accuracy: 0.6547\n",
            "Epoch 122/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.4544 - accuracy: 0.6629 - val_loss: 0.4687 - val_accuracy: 0.6532\n",
            "Epoch 123/200\n",
            "22/22 [==============================] - 1s 29ms/step - loss: 0.4543 - accuracy: 0.6642 - val_loss: 0.4706 - val_accuracy: 0.6502\n",
            "Epoch 124/200\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.4545 - accuracy: 0.6618 - val_loss: 0.4694 - val_accuracy: 0.6502\n",
            "Epoch 125/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.4543 - accuracy: 0.6615 - val_loss: 0.4689 - val_accuracy: 0.6471\n",
            "Epoch 126/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.4540 - accuracy: 0.6630 - val_loss: 0.4686 - val_accuracy: 0.6547\n",
            "Epoch 127/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.4539 - accuracy: 0.6617 - val_loss: 0.4689 - val_accuracy: 0.6532\n",
            "Epoch 128/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4540 - accuracy: 0.6627 - val_loss: 0.4696 - val_accuracy: 0.6502\n",
            "Epoch 129/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4541 - accuracy: 0.6623 - val_loss: 0.4694 - val_accuracy: 0.6562\n",
            "Epoch 130/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4541 - accuracy: 0.6631 - val_loss: 0.4692 - val_accuracy: 0.6532\n",
            "Epoch 131/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4538 - accuracy: 0.6624 - val_loss: 0.4692 - val_accuracy: 0.6524\n",
            "Epoch 132/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4535 - accuracy: 0.6639 - val_loss: 0.4691 - val_accuracy: 0.6554\n",
            "Epoch 133/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4538 - accuracy: 0.6632 - val_loss: 0.4693 - val_accuracy: 0.6532\n",
            "Epoch 134/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.4536 - accuracy: 0.6636 - val_loss: 0.4693 - val_accuracy: 0.6532\n",
            "Epoch 135/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4537 - accuracy: 0.6632 - val_loss: 0.4695 - val_accuracy: 0.6471\n",
            "Epoch 136/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4534 - accuracy: 0.6643 - val_loss: 0.4693 - val_accuracy: 0.6539\n",
            "Epoch 137/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4533 - accuracy: 0.6647 - val_loss: 0.4699 - val_accuracy: 0.6509\n",
            "Epoch 138/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4531 - accuracy: 0.6640 - val_loss: 0.4694 - val_accuracy: 0.6517\n",
            "Epoch 139/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4530 - accuracy: 0.6643 - val_loss: 0.4696 - val_accuracy: 0.6554\n",
            "Epoch 140/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4532 - accuracy: 0.6651 - val_loss: 0.4694 - val_accuracy: 0.6547\n",
            "Epoch 141/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4534 - accuracy: 0.6639 - val_loss: 0.4696 - val_accuracy: 0.6494\n",
            "Epoch 142/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4532 - accuracy: 0.6638 - val_loss: 0.4700 - val_accuracy: 0.6517\n",
            "Epoch 143/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.4530 - accuracy: 0.6649 - val_loss: 0.4694 - val_accuracy: 0.6539\n",
            "Epoch 144/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4528 - accuracy: 0.6633 - val_loss: 0.4701 - val_accuracy: 0.6502\n",
            "Epoch 145/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4531 - accuracy: 0.6638 - val_loss: 0.4695 - val_accuracy: 0.6547\n",
            "Epoch 146/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4531 - accuracy: 0.6656 - val_loss: 0.4700 - val_accuracy: 0.6562\n",
            "Epoch 147/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4526 - accuracy: 0.6635 - val_loss: 0.4696 - val_accuracy: 0.6554\n",
            "Epoch 148/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4526 - accuracy: 0.6637 - val_loss: 0.4695 - val_accuracy: 0.6554\n",
            "Epoch 149/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4523 - accuracy: 0.6642 - val_loss: 0.4703 - val_accuracy: 0.6524\n",
            "Epoch 150/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4522 - accuracy: 0.6648 - val_loss: 0.4696 - val_accuracy: 0.6539\n",
            "Epoch 151/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.4524 - accuracy: 0.6640 - val_loss: 0.4699 - val_accuracy: 0.6502\n",
            "Epoch 152/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4521 - accuracy: 0.6657 - val_loss: 0.4702 - val_accuracy: 0.6547\n",
            "Epoch 153/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4520 - accuracy: 0.6653 - val_loss: 0.4698 - val_accuracy: 0.6509\n",
            "Epoch 154/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4520 - accuracy: 0.6655 - val_loss: 0.4700 - val_accuracy: 0.6509\n",
            "Epoch 155/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4521 - accuracy: 0.6639 - val_loss: 0.4699 - val_accuracy: 0.6532\n",
            "Epoch 156/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4522 - accuracy: 0.6674 - val_loss: 0.4696 - val_accuracy: 0.6517\n",
            "Epoch 157/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4521 - accuracy: 0.6638 - val_loss: 0.4698 - val_accuracy: 0.6532\n",
            "Epoch 158/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4518 - accuracy: 0.6641 - val_loss: 0.4699 - val_accuracy: 0.6547\n",
            "Epoch 159/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4519 - accuracy: 0.6653 - val_loss: 0.4699 - val_accuracy: 0.6524\n",
            "Epoch 160/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4518 - accuracy: 0.6663 - val_loss: 0.4700 - val_accuracy: 0.6524\n",
            "Epoch 161/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4517 - accuracy: 0.6650 - val_loss: 0.4707 - val_accuracy: 0.6502\n",
            "Epoch 162/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4519 - accuracy: 0.6643 - val_loss: 0.4702 - val_accuracy: 0.6532\n",
            "Epoch 163/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4516 - accuracy: 0.6656 - val_loss: 0.4701 - val_accuracy: 0.6547\n",
            "Epoch 164/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.4515 - accuracy: 0.6670 - val_loss: 0.4702 - val_accuracy: 0.6539\n",
            "Epoch 165/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4514 - accuracy: 0.6662 - val_loss: 0.4707 - val_accuracy: 0.6494\n",
            "Epoch 166/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4515 - accuracy: 0.6650 - val_loss: 0.4703 - val_accuracy: 0.6524\n",
            "Epoch 167/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4516 - accuracy: 0.6652 - val_loss: 0.4701 - val_accuracy: 0.6524\n",
            "Epoch 168/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4516 - accuracy: 0.6665 - val_loss: 0.4706 - val_accuracy: 0.6584\n",
            "Epoch 169/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4515 - accuracy: 0.6659 - val_loss: 0.4702 - val_accuracy: 0.6547\n",
            "Epoch 170/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4512 - accuracy: 0.6679 - val_loss: 0.4704 - val_accuracy: 0.6524\n",
            "Epoch 171/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4510 - accuracy: 0.6667 - val_loss: 0.4703 - val_accuracy: 0.6547\n",
            "Epoch 172/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4512 - accuracy: 0.6670 - val_loss: 0.4707 - val_accuracy: 0.6547\n",
            "Epoch 173/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4511 - accuracy: 0.6656 - val_loss: 0.4704 - val_accuracy: 0.6554\n",
            "Epoch 174/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4509 - accuracy: 0.6663 - val_loss: 0.4706 - val_accuracy: 0.6532\n",
            "Epoch 175/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4510 - accuracy: 0.6665 - val_loss: 0.4704 - val_accuracy: 0.6554\n",
            "Epoch 176/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4508 - accuracy: 0.6662 - val_loss: 0.4710 - val_accuracy: 0.6547\n",
            "Epoch 177/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4509 - accuracy: 0.6673 - val_loss: 0.4715 - val_accuracy: 0.6502\n",
            "Epoch 178/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4510 - accuracy: 0.6669 - val_loss: 0.4712 - val_accuracy: 0.6539\n",
            "Epoch 179/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4506 - accuracy: 0.6660 - val_loss: 0.4705 - val_accuracy: 0.6532\n",
            "Epoch 180/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4509 - accuracy: 0.6664 - val_loss: 0.4707 - val_accuracy: 0.6524\n",
            "Epoch 181/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.4509 - accuracy: 0.6667 - val_loss: 0.4711 - val_accuracy: 0.6539\n",
            "Epoch 182/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4509 - accuracy: 0.6680 - val_loss: 0.4704 - val_accuracy: 0.6554\n",
            "Epoch 183/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4505 - accuracy: 0.6667 - val_loss: 0.4710 - val_accuracy: 0.6554\n",
            "Epoch 184/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4508 - accuracy: 0.6652 - val_loss: 0.4717 - val_accuracy: 0.6547\n",
            "Epoch 185/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4506 - accuracy: 0.6678 - val_loss: 0.4718 - val_accuracy: 0.6502\n",
            "Epoch 186/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4505 - accuracy: 0.6663 - val_loss: 0.4713 - val_accuracy: 0.6547\n",
            "Epoch 187/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4505 - accuracy: 0.6672 - val_loss: 0.4708 - val_accuracy: 0.6562\n",
            "Epoch 188/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4504 - accuracy: 0.6684 - val_loss: 0.4709 - val_accuracy: 0.6539\n",
            "Epoch 189/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4501 - accuracy: 0.6672 - val_loss: 0.4711 - val_accuracy: 0.6547\n",
            "Epoch 190/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4505 - accuracy: 0.6672 - val_loss: 0.4718 - val_accuracy: 0.6524\n",
            "Epoch 191/200\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4504 - accuracy: 0.6678 - val_loss: 0.4716 - val_accuracy: 0.6562\n",
            "Epoch 192/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4505 - accuracy: 0.6669 - val_loss: 0.4715 - val_accuracy: 0.6554\n",
            "Epoch 193/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4507 - accuracy: 0.6671 - val_loss: 0.4720 - val_accuracy: 0.6509\n",
            "Epoch 194/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4505 - accuracy: 0.6670 - val_loss: 0.4710 - val_accuracy: 0.6539\n",
            "Epoch 195/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4502 - accuracy: 0.6678 - val_loss: 0.4712 - val_accuracy: 0.6539\n",
            "Epoch 196/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4499 - accuracy: 0.6680 - val_loss: 0.4711 - val_accuracy: 0.6562\n",
            "Epoch 197/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4502 - accuracy: 0.6700 - val_loss: 0.4712 - val_accuracy: 0.6554\n",
            "Epoch 198/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4501 - accuracy: 0.6683 - val_loss: 0.4715 - val_accuracy: 0.6509\n",
            "Epoch 199/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4501 - accuracy: 0.6682 - val_loss: 0.4718 - val_accuracy: 0.6569\n",
            "Epoch 200/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4503 - accuracy: 0.6669 - val_loss: 0.4715 - val_accuracy: 0.6547\n",
            "42/42 [==============================] - 0s 3ms/step - loss: 0.4711 - accuracy: 0.6482\n",
            "Loss: 0.47112593054771423 and Accuracy: 0.6482036113739014\n"
          ]
        }
      ],
      "source": [
        "# your code goes here\n",
        "history31 = model31.fit([x_train_review_pad_glove, x_train_aspect_pad_glove],\n",
        "                    y_train,\n",
        "                    epochs=200,\n",
        "                    batch_size=512,\n",
        "                    validation_data=([x_dev_review_pad_glove, x_dev_aspect_pad_glove], y_dev),\n",
        "                    verbose=1)\n",
        "\n",
        "results31 = model31.evaluate([x_test_review_pad_glove, x_test_aspect_pad_glove], y_test)\n",
        "\n",
        "print(\"Loss: {} and Accuracy: {}\".format(results31[0], results31[1]))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zR55MxEtBKBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0npTvFuVt5R"
      },
      "source": [
        "## Model 3-2 CNN or LSTM model with multiple-input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCp9khOn0D2-"
      },
      "source": [
        "Modify the previous CNN or LSTM model to be compatible with multiple-input, similar to model 3-1. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Js5yZqmYywev",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70845e0d-9ab2-4598-a5a5-c236b0726ad0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 16)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 128, 300)     120000300   ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, 16, 300)      120000300   ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  (None, 128)          219648      ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " lstm_2 (LSTM)                  (None, 16)           20288       ['embedding_1[0][0]']            \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 16)           2064        ['lstm_1[0][0]']                 \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 16)           272         ['lstm_2[0][0]']                 \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 32)           0           ['dense[0][0]',                  \n",
            "                                                                  'dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 3)            99          ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 240,242,971\n",
            "Trainable params: 242,371\n",
            "Non-trainable params: 240,000,600\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# your code goes here\n",
        "# Tips: The activation function of the output layer is softmax.\n",
        "from keras.layers import Input, Embedding, Dense, concatenate, LSTM\n",
        "from keras.models import Model\n",
        "from keras.layers.core import Dense, Reshape\n",
        "\n",
        "# define two sets of inputs\n",
        "inputA = Input(shape=(128,), dtype='int32')\n",
        "inputB = Input(shape=(16,), dtype='int32')\n",
        "\n",
        "\n",
        "# the first branch operates on the first input\n",
        "glove_embeddingA = createPretrainedEmbeddingLayer(wordToGlove,wordToIndex,isTrainable=False)(inputA)\n",
        "lstmA = LSTM(128)(glove_embeddingA)\n",
        "hidden1 = Dense(16)(lstmA)\n",
        "\n",
        "x = Model(inputs=inputA, outputs=hidden1)\n",
        "\n",
        "# the second branch opreates on the second input\n",
        "glove_embeddingB = createPretrainedEmbeddingLayer(wordToGlove,wordToIndex,isTrainable=False)(inputB)\n",
        "lstmB = LSTM(16)(glove_embeddingB)\n",
        "hidden2 = Dense(16)(lstmB)\n",
        "\n",
        "y = Model(inputs=inputB, outputs=hidden2)\n",
        "\n",
        "# combine the output of the two branches\n",
        "combined = concatenate([x.output, y.output])\n",
        "\n",
        "\n",
        "z = Dense(3, activation='softmax')(combined)\n",
        "\n",
        "model3b = Model(inputs=[x.input, y.input], outputs=z)\n",
        "model3b.compile(optimizer='adam', loss='binary_crossentropy', metrics=[\"accuracy\"])\n",
        "model3b.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import SVG\n",
        "from keras.utils import vis_utils\n",
        "SVG(vis_utils.model_to_dot(model3b, show_shapes=True, show_layer_names=True, dpi=60).create(prog='dot', format='svg'))  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "id": "A9jHEFqB60Ja",
        "outputId": "8c6f679d-97e6-4c7d-a79a-4a96676e0e75"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg height=\"392pt\" viewBox=\"0.00 0.00 716.00 470.00\" width=\"597pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(.8333 .8333) rotate(0) translate(4 466)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" points=\"-4,4 -4,-466 712,-466 712,4 -4,4\" stroke=\"transparent\"/>\n<!-- 140408155232080 -->\n<g class=\"node\" id=\"node1\">\n<title>140408155232080</title>\n<polygon fill=\"none\" points=\"8,-415.5 8,-461.5 338,-461.5 338,-415.5 8,-415.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"48\" y=\"-446.3\">input_1</text>\n<polyline fill=\"none\" points=\"8,-438.5 88,-438.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"48\" y=\"-423.3\">InputLayer</text>\n<polyline fill=\"none\" points=\"88,-415.5 88,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"117\" y=\"-446.3\">input:</text>\n<polyline fill=\"none\" points=\"88,-438.5 146,-438.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"117\" y=\"-423.3\">output:</text>\n<polyline fill=\"none\" points=\"146,-415.5 146,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"194\" y=\"-434.8\">[(None, 128)]</text>\n<polyline fill=\"none\" points=\"242,-415.5 242,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"290\" y=\"-434.8\">[(None, 128)]</text>\n</g>\n<!-- 140409998172688 -->\n<g class=\"node\" id=\"node3\">\n<title>140409998172688</title>\n<polygon fill=\"none\" points=\"0,-332.5 0,-378.5 346,-378.5 346,-332.5 0,-332.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"42\" y=\"-363.3\">embedding</text>\n<polyline fill=\"none\" points=\"0,-355.5 84,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"42\" y=\"-340.3\">Embedding</text>\n<polyline fill=\"none\" points=\"84,-332.5 84,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"113\" y=\"-363.3\">input:</text>\n<polyline fill=\"none\" points=\"84,-355.5 142,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"113\" y=\"-340.3\">output:</text>\n<polyline fill=\"none\" points=\"142,-332.5 142,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"185.5\" y=\"-351.8\">(None, 128)</text>\n<polyline fill=\"none\" points=\"229,-332.5 229,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"287.5\" y=\"-351.8\">(None, 128, 300)</text>\n</g>\n<!-- 140408155232080&#45;&gt;140409998172688 -->\n<g class=\"edge\" id=\"edge1\">\n<title>140408155232080-&gt;140409998172688</title>\n<path d=\"M173,-415.3799C173,-407.1745 173,-397.7679 173,-388.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"176.5001,-388.784 173,-378.784 169.5001,-388.784 176.5001,-388.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140408174402384 -->\n<g class=\"node\" id=\"node2\">\n<title>140408174402384</title>\n<polygon fill=\"none\" points=\"378,-415.5 378,-461.5 694,-461.5 694,-415.5 378,-415.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"418\" y=\"-446.3\">input_2</text>\n<polyline fill=\"none\" points=\"378,-438.5 458,-438.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"418\" y=\"-423.3\">InputLayer</text>\n<polyline fill=\"none\" points=\"458,-415.5 458,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"487\" y=\"-446.3\">input:</text>\n<polyline fill=\"none\" points=\"458,-438.5 516,-438.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"487\" y=\"-423.3\">output:</text>\n<polyline fill=\"none\" points=\"516,-415.5 516,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"560.5\" y=\"-434.8\">[(None, 16)]</text>\n<polyline fill=\"none\" points=\"605,-415.5 605,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"649.5\" y=\"-434.8\">[(None, 16)]</text>\n</g>\n<!-- 140408154747728 -->\n<g class=\"node\" id=\"node4\">\n<title>140408154747728</title>\n<polygon fill=\"none\" points=\"364,-332.5 364,-378.5 708,-378.5 708,-332.5 364,-332.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"412\" y=\"-363.3\">embedding_1</text>\n<polyline fill=\"none\" points=\"364,-355.5 460,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"412\" y=\"-340.3\">Embedding</text>\n<polyline fill=\"none\" points=\"460,-332.5 460,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"489\" y=\"-363.3\">input:</text>\n<polyline fill=\"none\" points=\"460,-355.5 518,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"489\" y=\"-340.3\">output:</text>\n<polyline fill=\"none\" points=\"518,-332.5 518,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"558\" y=\"-351.8\">(None, 16)</text>\n<polyline fill=\"none\" points=\"598,-332.5 598,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"653\" y=\"-351.8\">(None, 16, 300)</text>\n</g>\n<!-- 140408174402384&#45;&gt;140408154747728 -->\n<g class=\"edge\" id=\"edge2\">\n<title>140408174402384-&gt;140408154747728</title>\n<path d=\"M536,-415.3799C536,-407.1745 536,-397.7679 536,-388.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"539.5001,-388.784 536,-378.784 532.5001,-388.784 539.5001,-388.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140410270989008 -->\n<g class=\"node\" id=\"node5\">\n<title>140410270989008</title>\n<polygon fill=\"none\" points=\"31,-249.5 31,-295.5 349,-295.5 349,-249.5 31,-249.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"59\" y=\"-280.3\">lstm_1</text>\n<polyline fill=\"none\" points=\"31,-272.5 87,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"59\" y=\"-257.3\">LSTM</text>\n<polyline fill=\"none\" points=\"87,-249.5 87,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"116\" y=\"-280.3\">input:</text>\n<polyline fill=\"none\" points=\"87,-272.5 145,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"116\" y=\"-257.3\">output:</text>\n<polyline fill=\"none\" points=\"145,-249.5 145,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"203.5\" y=\"-268.8\">(None, 128, 300)</text>\n<polyline fill=\"none\" points=\"262,-249.5 262,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"305.5\" y=\"-268.8\">(None, 128)</text>\n</g>\n<!-- 140409998172688&#45;&gt;140410270989008 -->\n<g class=\"edge\" id=\"edge3\">\n<title>140409998172688-&gt;140410270989008</title>\n<path d=\"M177.7354,-332.3799C179.4343,-324.0854 181.3846,-314.5633 183.2227,-305.5889\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"186.6532,-306.2829 185.231,-295.784 179.7956,-304.8783 186.6532,-306.2829\" stroke=\"#000000\"/>\n</g>\n<!-- 140410257618000 -->\n<g class=\"node\" id=\"node6\">\n<title>140410257618000</title>\n<polygon fill=\"none\" points=\"375,-249.5 375,-295.5 679,-295.5 679,-249.5 375,-249.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"403\" y=\"-280.3\">lstm_2</text>\n<polyline fill=\"none\" points=\"375,-272.5 431,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"403\" y=\"-257.3\">LSTM</text>\n<polyline fill=\"none\" points=\"431,-249.5 431,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"460\" y=\"-280.3\">input:</text>\n<polyline fill=\"none\" points=\"431,-272.5 489,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"460\" y=\"-257.3\">output:</text>\n<polyline fill=\"none\" points=\"489,-249.5 489,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"544\" y=\"-268.8\">(None, 16, 300)</text>\n<polyline fill=\"none\" points=\"599,-249.5 599,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"639\" y=\"-268.8\">(None, 16)</text>\n</g>\n<!-- 140408154747728&#45;&gt;140410257618000 -->\n<g class=\"edge\" id=\"edge4\">\n<title>140408154747728-&gt;140410257618000</title>\n<path d=\"M533.493,-332.3799C532.6033,-324.1745 531.5833,-314.7679 530.6194,-305.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"534.0825,-305.3484 529.5248,-295.784 527.1233,-306.1031 534.0825,-305.3484\" stroke=\"#000000\"/>\n</g>\n<!-- 140408174390352 -->\n<g class=\"node\" id=\"node7\">\n<title>140408174390352</title>\n<polygon fill=\"none\" points=\"66.5,-166.5 66.5,-212.5 343.5,-212.5 343.5,-166.5 66.5,-166.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"92.5\" y=\"-197.3\">dense</text>\n<polyline fill=\"none\" points=\"66.5,-189.5 118.5,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"92.5\" y=\"-174.3\">Dense</text>\n<polyline fill=\"none\" points=\"118.5,-166.5 118.5,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"147.5\" y=\"-197.3\">input:</text>\n<polyline fill=\"none\" points=\"118.5,-189.5 176.5,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"147.5\" y=\"-174.3\">output:</text>\n<polyline fill=\"none\" points=\"176.5,-166.5 176.5,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"220\" y=\"-185.8\">(None, 128)</text>\n<polyline fill=\"none\" points=\"263.5,-166.5 263.5,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"303.5\" y=\"-185.8\">(None, 16)</text>\n</g>\n<!-- 140410270989008&#45;&gt;140408174390352 -->\n<g class=\"edge\" id=\"edge5\">\n<title>140410270989008-&gt;140408174390352</title>\n<path d=\"M194.1783,-249.3799C195.6612,-241.1745 197.3612,-231.7679 198.9677,-222.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"202.4578,-223.2471 200.792,-212.784 195.5693,-222.0021 202.4578,-223.2471\" stroke=\"#000000\"/>\n</g>\n<!-- 140410347759056 -->\n<g class=\"node\" id=\"node8\">\n<title>140410347759056</title>\n<polygon fill=\"none\" points=\"374,-166.5 374,-212.5 656,-212.5 656,-166.5 374,-166.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"406\" y=\"-197.3\">dense_1</text>\n<polyline fill=\"none\" points=\"374,-189.5 438,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"406\" y=\"-174.3\">Dense</text>\n<polyline fill=\"none\" points=\"438,-166.5 438,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"467\" y=\"-197.3\">input:</text>\n<polyline fill=\"none\" points=\"438,-189.5 496,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"467\" y=\"-174.3\">output:</text>\n<polyline fill=\"none\" points=\"496,-166.5 496,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"536\" y=\"-185.8\">(None, 16)</text>\n<polyline fill=\"none\" points=\"576,-166.5 576,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"616\" y=\"-185.8\">(None, 16)</text>\n</g>\n<!-- 140410257618000&#45;&gt;140410347759056 -->\n<g class=\"edge\" id=\"edge6\">\n<title>140410257618000-&gt;140410347759056</title>\n<path d=\"M523.6573,-249.3799C522.471,-241.1745 521.111,-231.7679 519.8258,-222.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"523.2613,-222.1803 518.3664,-212.784 516.3334,-223.1819 523.2613,-222.1803\" stroke=\"#000000\"/>\n</g>\n<!-- 140408141069648 -->\n<g class=\"node\" id=\"node9\">\n<title>140408141069648</title>\n<polygon fill=\"none\" points=\"162.5,-83.5 162.5,-129.5 545.5,-129.5 545.5,-83.5 162.5,-83.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"205.5\" y=\"-114.3\">concatenate</text>\n<polyline fill=\"none\" points=\"162.5,-106.5 248.5,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"205.5\" y=\"-91.3\">Concatenate</text>\n<polyline fill=\"none\" points=\"248.5,-83.5 248.5,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"277.5\" y=\"-114.3\">input:</text>\n<polyline fill=\"none\" points=\"248.5,-106.5 306.5,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"277.5\" y=\"-91.3\">output:</text>\n<polyline fill=\"none\" points=\"306.5,-83.5 306.5,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"386\" y=\"-102.8\">[(None, 16), (None, 16)]</text>\n<polyline fill=\"none\" points=\"465.5,-83.5 465.5,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"505.5\" y=\"-102.8\">(None, 32)</text>\n</g>\n<!-- 140408174390352&#45;&gt;140408141069648 -->\n<g class=\"edge\" id=\"edge7\">\n<title>140408174390352-&gt;140408141069648</title>\n<path d=\"M246.5048,-166.3799C264.1908,-156.5279 284.9858,-144.9442 303.5493,-134.6034\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"305.4268,-137.564 312.4596,-129.6399 302.0203,-131.4488 305.4268,-137.564\" stroke=\"#000000\"/>\n</g>\n<!-- 140410347759056&#45;&gt;140408141069648 -->\n<g class=\"edge\" id=\"edge8\">\n<title>140410347759056-&gt;140408141069648</title>\n<path d=\"M470.1525,-166.3799C450.8684,-156.4384 428.1636,-144.7334 407.9674,-134.3217\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"409.3781,-131.1113 398.8859,-129.6399 406.1705,-137.3331 409.3781,-131.1113\" stroke=\"#000000\"/>\n</g>\n<!-- 140408141117968 -->\n<g class=\"node\" id=\"node10\">\n<title>140408141117968</title>\n<polygon fill=\"none\" points=\"217,-.5 217,-46.5 491,-46.5 491,-.5 217,-.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"249\" y=\"-31.3\">dense_2</text>\n<polyline fill=\"none\" points=\"217,-23.5 281,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"249\" y=\"-8.3\">Dense</text>\n<polyline fill=\"none\" points=\"281,-.5 281,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"310\" y=\"-31.3\">input:</text>\n<polyline fill=\"none\" points=\"281,-23.5 339,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"310\" y=\"-8.3\">output:</text>\n<polyline fill=\"none\" points=\"339,-.5 339,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"379\" y=\"-19.8\">(None, 32)</text>\n<polyline fill=\"none\" points=\"419,-.5 419,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"455\" y=\"-19.8\">(None, 3)</text>\n</g>\n<!-- 140408141069648&#45;&gt;140408141117968 -->\n<g class=\"edge\" id=\"edge9\">\n<title>140408141069648-&gt;140408141117968</title>\n<path d=\"M354,-83.3799C354,-75.1745 354,-65.7679 354,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"357.5001,-56.784 354,-46.784 350.5001,-56.784 357.5001,-56.784\" stroke=\"#000000\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history3b = model3b.fit([x_train_review_pad_glove, x_train_aspect_pad_glove],\n",
        "                    y_train,\n",
        "                    epochs=200,\n",
        "                    batch_size=512,\n",
        "                    validation_data=([x_dev_review_pad_glove, x_dev_aspect_pad_glove], y_dev),\n",
        "                    verbose=1)\n",
        "\n",
        "results3b = model3b.evaluate([x_test_review_pad_glove, x_test_aspect_pad_glove], y_test)\n",
        "\n",
        "print(\"Loss: {} and Accuracy: {}\".format(results3b[0], results3b[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rE8V7HjI62xx",
        "outputId": "bbf4b6cc-35fd-47e7-a81e-bf53ed027599"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "22/22 [==============================] - 11s 144ms/step - loss: 0.6449 - accuracy: 0.4408 - val_loss: 0.6168 - val_accuracy: 0.4535\n",
            "Epoch 2/200\n",
            "22/22 [==============================] - 2s 84ms/step - loss: 0.6020 - accuracy: 0.4588 - val_loss: 0.5676 - val_accuracy: 0.5383\n",
            "Epoch 3/200\n",
            "22/22 [==============================] - 2s 84ms/step - loss: 0.5542 - accuracy: 0.5438 - val_loss: 0.5352 - val_accuracy: 0.5593\n",
            "Epoch 4/200\n",
            "22/22 [==============================] - 2s 85ms/step - loss: 0.5344 - accuracy: 0.5703 - val_loss: 0.5241 - val_accuracy: 0.5878\n",
            "Epoch 5/200\n",
            "22/22 [==============================] - 2s 85ms/step - loss: 0.5257 - accuracy: 0.5862 - val_loss: 0.5169 - val_accuracy: 0.5953\n",
            "Epoch 6/200\n",
            "22/22 [==============================] - 2s 85ms/step - loss: 0.5214 - accuracy: 0.5924 - val_loss: 0.5118 - val_accuracy: 0.6014\n",
            "Epoch 7/200\n",
            "22/22 [==============================] - 2s 85ms/step - loss: 0.5168 - accuracy: 0.5932 - val_loss: 0.5068 - val_accuracy: 0.6089\n",
            "Epoch 8/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.5138 - accuracy: 0.5932 - val_loss: 0.5037 - val_accuracy: 0.6111\n",
            "Epoch 9/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.5114 - accuracy: 0.5945 - val_loss: 0.5025 - val_accuracy: 0.6081\n",
            "Epoch 10/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.5094 - accuracy: 0.5978 - val_loss: 0.4992 - val_accuracy: 0.6081\n",
            "Epoch 11/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.5070 - accuracy: 0.6023 - val_loss: 0.5000 - val_accuracy: 0.6239\n",
            "Epoch 12/200\n",
            "22/22 [==============================] - 2s 88ms/step - loss: 0.5078 - accuracy: 0.6005 - val_loss: 0.4993 - val_accuracy: 0.6081\n",
            "Epoch 13/200\n",
            "22/22 [==============================] - 2s 89ms/step - loss: 0.5054 - accuracy: 0.6030 - val_loss: 0.4984 - val_accuracy: 0.6119\n",
            "Epoch 14/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.5042 - accuracy: 0.6060 - val_loss: 0.4989 - val_accuracy: 0.6156\n",
            "Epoch 15/200\n",
            "22/22 [==============================] - 2s 85ms/step - loss: 0.5038 - accuracy: 0.6065 - val_loss: 0.4979 - val_accuracy: 0.6119\n",
            "Epoch 16/200\n",
            "22/22 [==============================] - 2s 85ms/step - loss: 0.5028 - accuracy: 0.6039 - val_loss: 0.4994 - val_accuracy: 0.6156\n",
            "Epoch 17/200\n",
            "22/22 [==============================] - 2s 85ms/step - loss: 0.5020 - accuracy: 0.6088 - val_loss: 0.4988 - val_accuracy: 0.6164\n",
            "Epoch 18/200\n",
            "22/22 [==============================] - 2s 85ms/step - loss: 0.5009 - accuracy: 0.6082 - val_loss: 0.4992 - val_accuracy: 0.6141\n",
            "Epoch 19/200\n",
            "22/22 [==============================] - 2s 85ms/step - loss: 0.5010 - accuracy: 0.6063 - val_loss: 0.4989 - val_accuracy: 0.6149\n",
            "Epoch 20/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.5006 - accuracy: 0.6114 - val_loss: 0.4990 - val_accuracy: 0.6186\n",
            "Epoch 21/200\n",
            "22/22 [==============================] - 2s 85ms/step - loss: 0.4993 - accuracy: 0.6137 - val_loss: 0.4991 - val_accuracy: 0.6209\n",
            "Epoch 22/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4993 - accuracy: 0.6109 - val_loss: 0.4998 - val_accuracy: 0.6104\n",
            "Epoch 23/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4983 - accuracy: 0.6128 - val_loss: 0.5007 - val_accuracy: 0.6171\n",
            "Epoch 24/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4983 - accuracy: 0.6092 - val_loss: 0.4993 - val_accuracy: 0.6119\n",
            "Epoch 25/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4970 - accuracy: 0.6136 - val_loss: 0.5016 - val_accuracy: 0.6014\n",
            "Epoch 26/200\n",
            "22/22 [==============================] - 2s 89ms/step - loss: 0.4972 - accuracy: 0.6116 - val_loss: 0.5013 - val_accuracy: 0.6059\n",
            "Epoch 27/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4966 - accuracy: 0.6132 - val_loss: 0.5005 - val_accuracy: 0.6096\n",
            "Epoch 28/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4956 - accuracy: 0.6147 - val_loss: 0.5014 - val_accuracy: 0.6194\n",
            "Epoch 29/200\n",
            "22/22 [==============================] - 2s 89ms/step - loss: 0.4952 - accuracy: 0.6146 - val_loss: 0.4994 - val_accuracy: 0.6149\n",
            "Epoch 30/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4950 - accuracy: 0.6162 - val_loss: 0.5003 - val_accuracy: 0.6194\n",
            "Epoch 31/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4941 - accuracy: 0.6176 - val_loss: 0.5002 - val_accuracy: 0.6126\n",
            "Epoch 32/200\n",
            "22/22 [==============================] - 2s 85ms/step - loss: 0.4942 - accuracy: 0.6168 - val_loss: 0.5009 - val_accuracy: 0.6141\n",
            "Epoch 33/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4930 - accuracy: 0.6211 - val_loss: 0.5000 - val_accuracy: 0.6111\n",
            "Epoch 34/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4935 - accuracy: 0.6168 - val_loss: 0.5002 - val_accuracy: 0.6156\n",
            "Epoch 35/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4932 - accuracy: 0.6132 - val_loss: 0.5020 - val_accuracy: 0.6186\n",
            "Epoch 36/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4937 - accuracy: 0.6142 - val_loss: 0.5033 - val_accuracy: 0.6156\n",
            "Epoch 37/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4919 - accuracy: 0.6183 - val_loss: 0.5028 - val_accuracy: 0.6156\n",
            "Epoch 38/200\n",
            "22/22 [==============================] - 2s 90ms/step - loss: 0.4911 - accuracy: 0.6214 - val_loss: 0.5047 - val_accuracy: 0.6096\n",
            "Epoch 39/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4911 - accuracy: 0.6160 - val_loss: 0.5029 - val_accuracy: 0.6051\n",
            "Epoch 40/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4911 - accuracy: 0.6243 - val_loss: 0.5042 - val_accuracy: 0.6126\n",
            "Epoch 41/200\n",
            "22/22 [==============================] - 2s 89ms/step - loss: 0.4909 - accuracy: 0.6176 - val_loss: 0.5027 - val_accuracy: 0.6104\n",
            "Epoch 42/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4899 - accuracy: 0.6173 - val_loss: 0.5032 - val_accuracy: 0.6179\n",
            "Epoch 43/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4898 - accuracy: 0.6222 - val_loss: 0.5044 - val_accuracy: 0.6134\n",
            "Epoch 44/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4894 - accuracy: 0.6208 - val_loss: 0.5025 - val_accuracy: 0.6149\n",
            "Epoch 45/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4897 - accuracy: 0.6207 - val_loss: 0.5060 - val_accuracy: 0.6066\n",
            "Epoch 46/200\n",
            "22/22 [==============================] - 2s 89ms/step - loss: 0.4889 - accuracy: 0.6180 - val_loss: 0.5051 - val_accuracy: 0.6089\n",
            "Epoch 47/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4884 - accuracy: 0.6239 - val_loss: 0.5065 - val_accuracy: 0.6081\n",
            "Epoch 48/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4883 - accuracy: 0.6224 - val_loss: 0.5047 - val_accuracy: 0.6179\n",
            "Epoch 49/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4883 - accuracy: 0.6210 - val_loss: 0.5050 - val_accuracy: 0.6066\n",
            "Epoch 50/200\n",
            "22/22 [==============================] - 2s 85ms/step - loss: 0.4873 - accuracy: 0.6223 - val_loss: 0.5059 - val_accuracy: 0.6149\n",
            "Epoch 51/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4869 - accuracy: 0.6245 - val_loss: 0.5058 - val_accuracy: 0.6134\n",
            "Epoch 52/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4865 - accuracy: 0.6220 - val_loss: 0.5061 - val_accuracy: 0.6156\n",
            "Epoch 53/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4868 - accuracy: 0.6232 - val_loss: 0.5058 - val_accuracy: 0.6081\n",
            "Epoch 54/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4871 - accuracy: 0.6244 - val_loss: 0.5083 - val_accuracy: 0.6044\n",
            "Epoch 55/200\n",
            "22/22 [==============================] - 2s 85ms/step - loss: 0.4866 - accuracy: 0.6224 - val_loss: 0.5072 - val_accuracy: 0.6051\n",
            "Epoch 56/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4856 - accuracy: 0.6210 - val_loss: 0.5076 - val_accuracy: 0.6111\n",
            "Epoch 57/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4851 - accuracy: 0.6218 - val_loss: 0.5065 - val_accuracy: 0.6066\n",
            "Epoch 58/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4854 - accuracy: 0.6210 - val_loss: 0.5068 - val_accuracy: 0.6126\n",
            "Epoch 59/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4855 - accuracy: 0.6227 - val_loss: 0.5087 - val_accuracy: 0.6096\n",
            "Epoch 60/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4845 - accuracy: 0.6235 - val_loss: 0.5064 - val_accuracy: 0.6111\n",
            "Epoch 61/200\n",
            "22/22 [==============================] - 2s 89ms/step - loss: 0.4840 - accuracy: 0.6261 - val_loss: 0.5094 - val_accuracy: 0.6126\n",
            "Epoch 62/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4841 - accuracy: 0.6241 - val_loss: 0.5105 - val_accuracy: 0.6119\n",
            "Epoch 63/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4847 - accuracy: 0.6257 - val_loss: 0.5103 - val_accuracy: 0.6134\n",
            "Epoch 64/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4843 - accuracy: 0.6252 - val_loss: 0.5075 - val_accuracy: 0.6104\n",
            "Epoch 65/200\n",
            "22/22 [==============================] - 2s 85ms/step - loss: 0.4838 - accuracy: 0.6247 - val_loss: 0.5102 - val_accuracy: 0.6156\n",
            "Epoch 66/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4845 - accuracy: 0.6225 - val_loss: 0.5074 - val_accuracy: 0.6141\n",
            "Epoch 67/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4835 - accuracy: 0.6230 - val_loss: 0.5088 - val_accuracy: 0.6066\n",
            "Epoch 68/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4835 - accuracy: 0.6220 - val_loss: 0.5106 - val_accuracy: 0.6029\n",
            "Epoch 69/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4841 - accuracy: 0.6225 - val_loss: 0.5093 - val_accuracy: 0.6059\n",
            "Epoch 70/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4829 - accuracy: 0.6217 - val_loss: 0.5102 - val_accuracy: 0.6111\n",
            "Epoch 71/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4827 - accuracy: 0.6227 - val_loss: 0.5089 - val_accuracy: 0.6081\n",
            "Epoch 72/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4820 - accuracy: 0.6268 - val_loss: 0.5134 - val_accuracy: 0.6074\n",
            "Epoch 73/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4840 - accuracy: 0.6211 - val_loss: 0.5109 - val_accuracy: 0.6081\n",
            "Epoch 74/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4815 - accuracy: 0.6259 - val_loss: 0.5110 - val_accuracy: 0.6089\n",
            "Epoch 75/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4816 - accuracy: 0.6259 - val_loss: 0.5112 - val_accuracy: 0.6044\n",
            "Epoch 76/200\n",
            "22/22 [==============================] - 2s 85ms/step - loss: 0.4820 - accuracy: 0.6247 - val_loss: 0.5108 - val_accuracy: 0.6044\n",
            "Epoch 77/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4812 - accuracy: 0.6244 - val_loss: 0.5121 - val_accuracy: 0.6081\n",
            "Epoch 78/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4809 - accuracy: 0.6267 - val_loss: 0.5124 - val_accuracy: 0.6074\n",
            "Epoch 79/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4812 - accuracy: 0.6258 - val_loss: 0.5133 - val_accuracy: 0.6066\n",
            "Epoch 80/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4807 - accuracy: 0.6258 - val_loss: 0.5127 - val_accuracy: 0.6066\n",
            "Epoch 81/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4805 - accuracy: 0.6241 - val_loss: 0.5155 - val_accuracy: 0.6096\n",
            "Epoch 82/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4803 - accuracy: 0.6237 - val_loss: 0.5117 - val_accuracy: 0.6096\n",
            "Epoch 83/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4795 - accuracy: 0.6287 - val_loss: 0.5134 - val_accuracy: 0.6074\n",
            "Epoch 84/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4808 - accuracy: 0.6242 - val_loss: 0.5173 - val_accuracy: 0.5953\n",
            "Epoch 85/200\n",
            "22/22 [==============================] - 2s 95ms/step - loss: 0.4826 - accuracy: 0.6209 - val_loss: 0.5166 - val_accuracy: 0.6059\n",
            "Epoch 86/200\n",
            "22/22 [==============================] - 2s 89ms/step - loss: 0.4802 - accuracy: 0.6238 - val_loss: 0.5150 - val_accuracy: 0.6066\n",
            "Epoch 87/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4798 - accuracy: 0.6267 - val_loss: 0.5147 - val_accuracy: 0.6111\n",
            "Epoch 88/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4789 - accuracy: 0.6287 - val_loss: 0.5177 - val_accuracy: 0.6014\n",
            "Epoch 89/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4796 - accuracy: 0.6262 - val_loss: 0.5175 - val_accuracy: 0.6141\n",
            "Epoch 90/200\n",
            "22/22 [==============================] - 2s 91ms/step - loss: 0.4780 - accuracy: 0.6288 - val_loss: 0.5158 - val_accuracy: 0.6014\n",
            "Epoch 91/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4782 - accuracy: 0.6244 - val_loss: 0.5180 - val_accuracy: 0.5991\n",
            "Epoch 92/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4777 - accuracy: 0.6289 - val_loss: 0.5191 - val_accuracy: 0.5968\n",
            "Epoch 93/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4788 - accuracy: 0.6260 - val_loss: 0.5175 - val_accuracy: 0.6051\n",
            "Epoch 94/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4780 - accuracy: 0.6279 - val_loss: 0.5188 - val_accuracy: 0.6096\n",
            "Epoch 95/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4771 - accuracy: 0.6277 - val_loss: 0.5188 - val_accuracy: 0.6044\n",
            "Epoch 96/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4774 - accuracy: 0.6261 - val_loss: 0.5195 - val_accuracy: 0.6029\n",
            "Epoch 97/200\n",
            "22/22 [==============================] - 2s 89ms/step - loss: 0.4774 - accuracy: 0.6261 - val_loss: 0.5193 - val_accuracy: 0.6044\n",
            "Epoch 98/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4772 - accuracy: 0.6290 - val_loss: 0.5203 - val_accuracy: 0.6021\n",
            "Epoch 99/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4767 - accuracy: 0.6286 - val_loss: 0.5202 - val_accuracy: 0.6059\n",
            "Epoch 100/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4770 - accuracy: 0.6214 - val_loss: 0.5211 - val_accuracy: 0.5991\n",
            "Epoch 101/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4772 - accuracy: 0.6290 - val_loss: 0.5191 - val_accuracy: 0.6066\n",
            "Epoch 102/200\n",
            "22/22 [==============================] - 2s 89ms/step - loss: 0.4778 - accuracy: 0.6279 - val_loss: 0.5209 - val_accuracy: 0.6066\n",
            "Epoch 103/200\n",
            "22/22 [==============================] - 2s 89ms/step - loss: 0.4773 - accuracy: 0.6267 - val_loss: 0.5208 - val_accuracy: 0.5998\n",
            "Epoch 104/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4762 - accuracy: 0.6284 - val_loss: 0.5216 - val_accuracy: 0.6074\n",
            "Epoch 105/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4760 - accuracy: 0.6270 - val_loss: 0.5217 - val_accuracy: 0.5991\n",
            "Epoch 106/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4760 - accuracy: 0.6300 - val_loss: 0.5230 - val_accuracy: 0.6014\n",
            "Epoch 107/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4763 - accuracy: 0.6256 - val_loss: 0.5225 - val_accuracy: 0.6096\n",
            "Epoch 108/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4760 - accuracy: 0.6227 - val_loss: 0.5254 - val_accuracy: 0.5991\n",
            "Epoch 109/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4761 - accuracy: 0.6257 - val_loss: 0.5236 - val_accuracy: 0.6021\n",
            "Epoch 110/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4753 - accuracy: 0.6304 - val_loss: 0.5224 - val_accuracy: 0.5998\n",
            "Epoch 111/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4761 - accuracy: 0.6262 - val_loss: 0.5244 - val_accuracy: 0.5976\n",
            "Epoch 112/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4753 - accuracy: 0.6286 - val_loss: 0.5243 - val_accuracy: 0.5991\n",
            "Epoch 113/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4751 - accuracy: 0.6263 - val_loss: 0.5245 - val_accuracy: 0.6059\n",
            "Epoch 114/200\n",
            "22/22 [==============================] - 2s 91ms/step - loss: 0.4758 - accuracy: 0.6217 - val_loss: 0.5252 - val_accuracy: 0.5923\n",
            "Epoch 115/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4752 - accuracy: 0.6312 - val_loss: 0.5232 - val_accuracy: 0.6059\n",
            "Epoch 116/200\n",
            "22/22 [==============================] - 2s 94ms/step - loss: 0.4753 - accuracy: 0.6303 - val_loss: 0.5254 - val_accuracy: 0.6081\n",
            "Epoch 117/200\n",
            "22/22 [==============================] - 2s 88ms/step - loss: 0.4753 - accuracy: 0.6281 - val_loss: 0.5244 - val_accuracy: 0.6089\n",
            "Epoch 118/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4746 - accuracy: 0.6275 - val_loss: 0.5249 - val_accuracy: 0.5998\n",
            "Epoch 119/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4754 - accuracy: 0.6282 - val_loss: 0.5253 - val_accuracy: 0.6036\n",
            "Epoch 120/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4751 - accuracy: 0.6280 - val_loss: 0.5226 - val_accuracy: 0.6044\n",
            "Epoch 121/200\n",
            "22/22 [==============================] - 2s 90ms/step - loss: 0.4750 - accuracy: 0.6257 - val_loss: 0.5269 - val_accuracy: 0.6021\n",
            "Epoch 122/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4743 - accuracy: 0.6302 - val_loss: 0.5271 - val_accuracy: 0.6029\n",
            "Epoch 123/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4757 - accuracy: 0.6286 - val_loss: 0.5275 - val_accuracy: 0.6096\n",
            "Epoch 124/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4744 - accuracy: 0.6294 - val_loss: 0.5279 - val_accuracy: 0.6021\n",
            "Epoch 125/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4752 - accuracy: 0.6271 - val_loss: 0.5268 - val_accuracy: 0.5968\n",
            "Epoch 126/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4753 - accuracy: 0.6211 - val_loss: 0.5258 - val_accuracy: 0.6051\n",
            "Epoch 127/200\n",
            "22/22 [==============================] - 2s 91ms/step - loss: 0.4739 - accuracy: 0.6286 - val_loss: 0.5268 - val_accuracy: 0.6059\n",
            "Epoch 128/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4745 - accuracy: 0.6246 - val_loss: 0.5276 - val_accuracy: 0.6021\n",
            "Epoch 129/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4739 - accuracy: 0.6294 - val_loss: 0.5291 - val_accuracy: 0.5961\n",
            "Epoch 130/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4742 - accuracy: 0.6284 - val_loss: 0.5281 - val_accuracy: 0.6081\n",
            "Epoch 131/200\n",
            "22/22 [==============================] - 2s 89ms/step - loss: 0.4740 - accuracy: 0.6284 - val_loss: 0.5260 - val_accuracy: 0.6074\n",
            "Epoch 132/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4737 - accuracy: 0.6298 - val_loss: 0.5302 - val_accuracy: 0.6096\n",
            "Epoch 133/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4740 - accuracy: 0.6282 - val_loss: 0.5290 - val_accuracy: 0.6029\n",
            "Epoch 134/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4733 - accuracy: 0.6264 - val_loss: 0.5312 - val_accuracy: 0.5938\n",
            "Epoch 135/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4736 - accuracy: 0.6305 - val_loss: 0.5302 - val_accuracy: 0.6051\n",
            "Epoch 136/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4732 - accuracy: 0.6305 - val_loss: 0.5296 - val_accuracy: 0.5983\n",
            "Epoch 137/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4737 - accuracy: 0.6211 - val_loss: 0.5303 - val_accuracy: 0.5998\n",
            "Epoch 138/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4733 - accuracy: 0.6294 - val_loss: 0.5308 - val_accuracy: 0.6036\n",
            "Epoch 139/200\n",
            "22/22 [==============================] - 2s 90ms/step - loss: 0.4739 - accuracy: 0.6295 - val_loss: 0.5293 - val_accuracy: 0.6014\n",
            "Epoch 140/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4737 - accuracy: 0.6305 - val_loss: 0.5326 - val_accuracy: 0.6006\n",
            "Epoch 141/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4737 - accuracy: 0.6306 - val_loss: 0.5309 - val_accuracy: 0.6044\n",
            "Epoch 142/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4732 - accuracy: 0.6284 - val_loss: 0.5298 - val_accuracy: 0.6096\n",
            "Epoch 143/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4731 - accuracy: 0.6280 - val_loss: 0.5299 - val_accuracy: 0.5976\n",
            "Epoch 144/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4737 - accuracy: 0.6235 - val_loss: 0.5311 - val_accuracy: 0.6006\n",
            "Epoch 145/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4729 - accuracy: 0.6316 - val_loss: 0.5313 - val_accuracy: 0.5998\n",
            "Epoch 146/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4729 - accuracy: 0.6293 - val_loss: 0.5325 - val_accuracy: 0.6074\n",
            "Epoch 147/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4729 - accuracy: 0.6298 - val_loss: 0.5317 - val_accuracy: 0.6051\n",
            "Epoch 148/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4726 - accuracy: 0.6305 - val_loss: 0.5319 - val_accuracy: 0.6029\n",
            "Epoch 149/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4727 - accuracy: 0.6281 - val_loss: 0.5323 - val_accuracy: 0.6074\n",
            "Epoch 150/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4726 - accuracy: 0.6302 - val_loss: 0.5329 - val_accuracy: 0.6059\n",
            "Epoch 151/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4723 - accuracy: 0.6302 - val_loss: 0.5313 - val_accuracy: 0.6036\n",
            "Epoch 152/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4731 - accuracy: 0.6281 - val_loss: 0.5344 - val_accuracy: 0.6119\n",
            "Epoch 153/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4734 - accuracy: 0.6266 - val_loss: 0.5325 - val_accuracy: 0.6044\n",
            "Epoch 154/200\n",
            "22/22 [==============================] - 2s 90ms/step - loss: 0.4730 - accuracy: 0.6257 - val_loss: 0.5314 - val_accuracy: 0.6089\n",
            "Epoch 155/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4726 - accuracy: 0.6279 - val_loss: 0.5321 - val_accuracy: 0.6059\n",
            "Epoch 156/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4728 - accuracy: 0.6277 - val_loss: 0.5326 - val_accuracy: 0.6059\n",
            "Epoch 157/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4727 - accuracy: 0.6299 - val_loss: 0.5338 - val_accuracy: 0.6051\n",
            "Epoch 158/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4724 - accuracy: 0.6265 - val_loss: 0.5341 - val_accuracy: 0.6036\n",
            "Epoch 159/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4721 - accuracy: 0.6314 - val_loss: 0.5330 - val_accuracy: 0.6096\n",
            "Epoch 160/200\n",
            "22/22 [==============================] - 2s 90ms/step - loss: 0.4729 - accuracy: 0.6297 - val_loss: 0.5357 - val_accuracy: 0.6029\n",
            "Epoch 161/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4735 - accuracy: 0.6262 - val_loss: 0.5336 - val_accuracy: 0.5976\n",
            "Epoch 162/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4732 - accuracy: 0.6257 - val_loss: 0.5328 - val_accuracy: 0.6051\n",
            "Epoch 163/200\n",
            "22/22 [==============================] - 2s 90ms/step - loss: 0.4722 - accuracy: 0.6291 - val_loss: 0.5362 - val_accuracy: 0.6029\n",
            "Epoch 164/200\n",
            "22/22 [==============================] - 2s 90ms/step - loss: 0.4724 - accuracy: 0.6330 - val_loss: 0.5342 - val_accuracy: 0.6059\n",
            "Epoch 165/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4725 - accuracy: 0.6267 - val_loss: 0.5338 - val_accuracy: 0.6021\n",
            "Epoch 166/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4726 - accuracy: 0.6303 - val_loss: 0.5357 - val_accuracy: 0.6081\n",
            "Epoch 167/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4729 - accuracy: 0.6286 - val_loss: 0.5332 - val_accuracy: 0.6089\n",
            "Epoch 168/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4728 - accuracy: 0.6317 - val_loss: 0.5362 - val_accuracy: 0.6036\n",
            "Epoch 169/200\n",
            "22/22 [==============================] - 2s 90ms/step - loss: 0.4717 - accuracy: 0.6239 - val_loss: 0.5366 - val_accuracy: 0.6036\n",
            "Epoch 170/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4722 - accuracy: 0.6304 - val_loss: 0.5350 - val_accuracy: 0.6051\n",
            "Epoch 171/200\n",
            "22/22 [==============================] - 2s 88ms/step - loss: 0.4722 - accuracy: 0.6294 - val_loss: 0.5353 - val_accuracy: 0.6059\n",
            "Epoch 172/200\n",
            "22/22 [==============================] - 2s 90ms/step - loss: 0.4722 - accuracy: 0.6286 - val_loss: 0.5366 - val_accuracy: 0.5953\n",
            "Epoch 173/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4717 - accuracy: 0.6307 - val_loss: 0.5370 - val_accuracy: 0.6089\n",
            "Epoch 174/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4718 - accuracy: 0.6309 - val_loss: 0.5365 - val_accuracy: 0.6119\n",
            "Epoch 175/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4713 - accuracy: 0.6309 - val_loss: 0.5366 - val_accuracy: 0.6066\n",
            "Epoch 176/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4720 - accuracy: 0.6305 - val_loss: 0.5387 - val_accuracy: 0.6044\n",
            "Epoch 177/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4722 - accuracy: 0.6286 - val_loss: 0.5359 - val_accuracy: 0.5946\n",
            "Epoch 178/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4720 - accuracy: 0.6267 - val_loss: 0.5387 - val_accuracy: 0.6036\n",
            "Epoch 179/200\n",
            "22/22 [==============================] - 2s 90ms/step - loss: 0.4714 - accuracy: 0.6301 - val_loss: 0.5386 - val_accuracy: 0.6089\n",
            "Epoch 180/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4719 - accuracy: 0.6292 - val_loss: 0.5378 - val_accuracy: 0.5901\n",
            "Epoch 181/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4726 - accuracy: 0.6303 - val_loss: 0.5375 - val_accuracy: 0.6014\n",
            "Epoch 182/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4727 - accuracy: 0.6288 - val_loss: 0.5397 - val_accuracy: 0.6051\n",
            "Epoch 183/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4716 - accuracy: 0.6298 - val_loss: 0.5367 - val_accuracy: 0.6014\n",
            "Epoch 184/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4713 - accuracy: 0.6311 - val_loss: 0.5381 - val_accuracy: 0.6089\n",
            "Epoch 185/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4716 - accuracy: 0.6320 - val_loss: 0.5388 - val_accuracy: 0.6021\n",
            "Epoch 186/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4715 - accuracy: 0.6287 - val_loss: 0.5390 - val_accuracy: 0.5961\n",
            "Epoch 187/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4716 - accuracy: 0.6298 - val_loss: 0.5371 - val_accuracy: 0.6104\n",
            "Epoch 188/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4715 - accuracy: 0.6308 - val_loss: 0.5386 - val_accuracy: 0.6036\n",
            "Epoch 189/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4714 - accuracy: 0.6303 - val_loss: 0.5386 - val_accuracy: 0.6126\n",
            "Epoch 190/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4716 - accuracy: 0.6288 - val_loss: 0.5382 - val_accuracy: 0.6059\n",
            "Epoch 191/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4716 - accuracy: 0.6291 - val_loss: 0.5411 - val_accuracy: 0.5991\n",
            "Epoch 192/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4713 - accuracy: 0.6299 - val_loss: 0.5391 - val_accuracy: 0.6021\n",
            "Epoch 193/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4710 - accuracy: 0.6304 - val_loss: 0.5396 - val_accuracy: 0.6036\n",
            "Epoch 194/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4712 - accuracy: 0.6279 - val_loss: 0.5389 - val_accuracy: 0.6014\n",
            "Epoch 195/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4710 - accuracy: 0.6290 - val_loss: 0.5393 - val_accuracy: 0.6021\n",
            "Epoch 196/200\n",
            "22/22 [==============================] - 2s 88ms/step - loss: 0.4711 - accuracy: 0.6321 - val_loss: 0.5402 - val_accuracy: 0.6066\n",
            "Epoch 197/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4706 - accuracy: 0.6294 - val_loss: 0.5389 - val_accuracy: 0.5991\n",
            "Epoch 198/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4707 - accuracy: 0.6300 - val_loss: 0.5416 - val_accuracy: 0.6111\n",
            "Epoch 199/200\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4713 - accuracy: 0.6306 - val_loss: 0.5418 - val_accuracy: 0.6111\n",
            "Epoch 200/200\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.4720 - accuracy: 0.6300 - val_loss: 0.5387 - val_accuracy: 0.6014\n",
            "42/42 [==============================] - 1s 12ms/step - loss: 0.5403 - accuracy: 0.5981\n",
            "Loss: 0.5403147339820862 and Accuracy: 0.5980538725852966\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_PPzd6R6gt8"
      },
      "source": [
        "#  Model 4: Another LSTM model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IC85cIumM9V"
      },
      "source": [
        "If you study the data carefully, you can find that every aspect appears in the review sentence, which means we can extract the aspect information from the sentence. In most cases, the polarity of the aspect is determined by the content near it. Therefore, an LSTM can transfer the information of adjacent context to the aspect. We only need to extract the aspect vector to calculate its polarity, without analyzing the whole sentence.\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "(This is a bit like a simplified version of the \"unknown target\" sequence tagging models discussed in lectures - but we're only trying to tag the one part of the sequence that we know corresponds to the aspect mention.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "pcxqucKN66sy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91f9957a-1b6d-45b0-f5f3-5413a170ebb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "annotation error:\n",
            "['i love the food here, and although it is pricey, the entree comes with rice, naan, dal, and salad, which makes it worthwhile.', 'd al', 'neutral', '24', '28']\n",
            "\n",
            "\n",
            "\n",
            "['the decor is not special at all but their food and amazing prices make up for it.', 'decor', 'negative', '4', '9']\n",
            "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "['the decor is not special at all but their food and amazing prices make up for it.', 'food', 'positive', '42', '46']\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
            "['the decor is not special at all but their food and amazing prices make up for it.', 'prices', 'positive', '59', '65']\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
            "['when tables opened up, the manager sat another party before us.', 'tables', 'neutral', '5', '11']\n",
            "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "['the scene there are two distinct personalities to the place: the loud, seemingly always-crowded bar with hanging paper decorations and dim lighting, and the two main dining areas, where the noise level and decor is notably more subdued.', 'noise level', 'negative', '190', '201']\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "def aspect_mask(reviews, aspects, dataset):\n",
        "  mask = []\n",
        "  for review,aspect,data in zip(reviews, aspects, dataset):\n",
        "    find_aspect = False\n",
        "    for j in range(5):\n",
        "      aspect_num = len(aspect)\n",
        "      aspect_str = \" \".join(aspect)\n",
        "      aspect_len = int(len(aspect_str) - 1) * (j+1)\n",
        "      offset = 0\n",
        "      for i,r in enumerate(review):\n",
        "        if i + aspect_num <= len(review):\n",
        "          r_context = \" \".join(review[i:i+aspect_num])\n",
        "          if r_context == aspect_str and offset + aspect_len >  int(data[3]) and offset + aspect_len <  int(data[4]):\n",
        "            find_aspect = True\n",
        "            sentence_mask = [0] * len(review)\n",
        "            sentence_mask[i:i+aspect_num] = [1] * aspect_num\n",
        "            mask.append(sentence_mask)\n",
        "            break\n",
        "          else:\n",
        "            offset += (len(r) + 1)\n",
        "      if find_aspect:\n",
        "        break\n",
        "\n",
        "    if not find_aspect:\n",
        "      for j in range(5):\n",
        "        aspect_len = int(len(aspect_str) - 1) * (j+1)\n",
        "        offset = 0\n",
        "        for i,r in enumerate(review):\n",
        "          if i + aspect_num <= len(review):\n",
        "            r_context = \" \".join(review[i:i+aspect_num])\n",
        "            if r_context.startswith(aspect_str) and offset + aspect_len >  int(data[3]) and offset + aspect_len <  int(data[4]):\n",
        "              find_aspect = True\n",
        "              sentence_mask = [0] * len(review)\n",
        "              sentence_mask[i:i+aspect_num] = [1] * aspect_num\n",
        "              mask.append(sentence_mask)\n",
        "              break\n",
        "            else:\n",
        "              offset += (len(r) + 1)\n",
        "        if find_aspect:\n",
        "          break\n",
        "\n",
        "    if not find_aspect:\n",
        "      for j in range(5):\n",
        "        aspect_len = int(len(aspect_str) - 1) * (j+1)\n",
        "        offset = 0\n",
        "        for i,r in enumerate(review):\n",
        "          if i + aspect_num <= len(review):\n",
        "            r_context = \" \".join(review[i:i+aspect_num])\n",
        "            if r_context.endswith(aspect_str) and offset + aspect_len >  int(data[3]) and offset + aspect_len <  int(data[4]):\n",
        "              find_aspect = True\n",
        "              sentence_mask = [0] * len(review)\n",
        "              sentence_mask[i:i+aspect_num] = [1] * aspect_num\n",
        "              mask.append(sentence_mask)\n",
        "              break\n",
        "            else:\n",
        "              offset += (len(r) + 1)\n",
        "        if find_aspect:\n",
        "          break\n",
        "\n",
        "    if not find_aspect:\n",
        "      print(\"annotation error:\")\n",
        "      print(data)\n",
        "      sentence_mask = [0] * len(review)\n",
        "      sentence_mask[16] = 1\n",
        "      mask.append(sentence_mask)\n",
        "\n",
        "    # if aspect_num > 1:\n",
        "    #   print(mask[-1])\n",
        "\n",
        "  return mask\n",
        "x_train_aspect_mask = aspect_mask(x_train_review, x_train_aspect, train)\n",
        "x_dev_aspect_mask = aspect_mask(x_dev_review, x_dev_aspect, val)\n",
        "x_test_aspect_mask = aspect_mask(x_test_review, x_test_aspect, test)\n",
        "\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "assert len(x_train_aspect_mask) == len(train)\n",
        "assert len(x_test_aspect_mask) == len(x_test_aspect)\n",
        "\n",
        "print(train[0])\n",
        "print(x_train_aspect_mask[0])\n",
        "print(train[1])\n",
        "print(x_train_aspect_mask[1])\n",
        "print(train[2])\n",
        "print(x_train_aspect_mask[2])\n",
        "print(train[3])\n",
        "print(x_train_aspect_mask[3])\n",
        "print(train[10319])\n",
        "print(x_train_aspect_mask[10319])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "PJK16El2yBQ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40cf1281-797b-4206-c715-20caef8b2d0f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "x_train_aspect_mask_pad = keras.preprocessing.sequence.pad_sequences(x_train_aspect_mask,\n",
        "                                                        value=0,\n",
        "                                                        padding='post',\n",
        "                                                        maxlen=128)\n",
        "x_dev_aspect_mask_pad = keras.preprocessing.sequence.pad_sequences(x_dev_aspect_mask,\n",
        "                                                        value=0,\n",
        "                                                        padding='post',\n",
        "                                                        maxlen=128)\n",
        "x_test_aspect_mask_pad = keras.preprocessing.sequence.pad_sequences(x_test_aspect_mask,\n",
        "                                                        value=0,\n",
        "                                                        padding='post',\n",
        "                                                        maxlen=128)\n",
        "x_train_aspect_mask_pad[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ahUWMflW65jM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23ef6b4a-33ef-46ed-dcbc-1856b9fd759a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " embedding_2 (Embedding)        (None, 128, 300)     120000300   ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " BiLSTM (Bidirectional)         (None, 128, 200)     320800      ['embedding_2[0][0]']            \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " dot (Dot)                      (None, 200)          0           ['BiLSTM[0][0]',                 \n",
            "                                                                  'input_4[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 16)           3216        ['dot[0][0]']                    \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 3)            51          ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 120,324,367\n",
            "Trainable params: 324,067\n",
            "Non-trainable params: 120,000,300\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from keras.layers import Input, Embedding, Dense, concatenate, LSTM, Bidirectional, Dot\n",
        "from keras.models import Model\n",
        "from keras.layers.core import Dense, Reshape\n",
        "\n",
        "\n",
        "maxlen = 200\n",
        "inputlayer1 = Input(shape=(128,), dtype='int32')\n",
        "\n",
        "glove_embedding = createPretrainedEmbeddingLayer(wordToGlove,wordToIndex,isTrainable=False)(inputlayer1)\n",
        "\n",
        "bilstm = Bidirectional(LSTM(100, return_sequences=True), name='BiLSTM', )(glove_embedding)\n",
        "\n",
        "inputlayer2 = Input(shape=(128,), dtype='float32')\n",
        "\n",
        "dot = Dot(axes=1)([bilstm, inputlayer2])\n",
        "\n",
        "hidden1 = Dense(16)(dot)\n",
        "\n",
        "output = Dense(3, activation='softmax')(hidden1)\n",
        "\n",
        "model_4 = Model(inputs=[inputlayer1, inputlayer2], outputs=output)\n",
        "\n",
        "model_4.compile(optimizer='adam', loss='binary_crossentropy', metrics=[\"accuracy\"])\n",
        "\n",
        "model_4.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "f90SrYb4YHj3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "81b57e43-66ad-4588-da1e-ab33a3be8fa4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg height=\"392pt\" viewBox=\"0.00 0.00 786.00 470.00\" width=\"655pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(.8333 .8333) rotate(0) translate(4 466)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" points=\"-4,4 -4,-466 782,-466 782,4 -4,4\" stroke=\"transparent\"/>\n<!-- 140408112382992 -->\n<g class=\"node\" id=\"node1\">\n<title>140408112382992</title>\n<polygon fill=\"none\" points=\"50,-415.5 50,-461.5 380,-461.5 380,-415.5 50,-415.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"90\" y=\"-446.3\">input_3</text>\n<polyline fill=\"none\" points=\"50,-438.5 130,-438.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"90\" y=\"-423.3\">InputLayer</text>\n<polyline fill=\"none\" points=\"130,-415.5 130,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"159\" y=\"-446.3\">input:</text>\n<polyline fill=\"none\" points=\"130,-438.5 188,-438.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"159\" y=\"-423.3\">output:</text>\n<polyline fill=\"none\" points=\"188,-415.5 188,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"236\" y=\"-434.8\">[(None, 128)]</text>\n<polyline fill=\"none\" points=\"284,-415.5 284,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"332\" y=\"-434.8\">[(None, 128)]</text>\n</g>\n<!-- 140407442653008 -->\n<g class=\"node\" id=\"node2\">\n<title>140407442653008</title>\n<polygon fill=\"none\" points=\"36,-332.5 36,-378.5 394,-378.5 394,-332.5 36,-332.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"84\" y=\"-363.3\">embedding_2</text>\n<polyline fill=\"none\" points=\"36,-355.5 132,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"84\" y=\"-340.3\">Embedding</text>\n<polyline fill=\"none\" points=\"132,-332.5 132,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"161\" y=\"-363.3\">input:</text>\n<polyline fill=\"none\" points=\"132,-355.5 190,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"161\" y=\"-340.3\">output:</text>\n<polyline fill=\"none\" points=\"190,-332.5 190,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"233.5\" y=\"-351.8\">(None, 128)</text>\n<polyline fill=\"none\" points=\"277,-332.5 277,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"335.5\" y=\"-351.8\">(None, 128, 300)</text>\n</g>\n<!-- 140408112382992&#45;&gt;140407442653008 -->\n<g class=\"edge\" id=\"edge1\">\n<title>140408112382992-&gt;140407442653008</title>\n<path d=\"M215,-415.3799C215,-407.1745 215,-397.7679 215,-388.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"218.5001,-388.784 215,-378.784 211.5001,-388.784 218.5001,-388.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140408139775184 -->\n<g class=\"node\" id=\"node3\">\n<title>140408139775184</title>\n<polygon fill=\"none\" points=\"0,-249.5 0,-295.5 430,-295.5 430,-249.5 0,-249.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"69\" y=\"-280.3\">BiLSTM(lstm_3)</text>\n<polyline fill=\"none\" points=\"0,-272.5 138,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"69\" y=\"-257.3\">Bidirectional(LSTM)</text>\n<polyline fill=\"none\" points=\"138,-249.5 138,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"167\" y=\"-280.3\">input:</text>\n<polyline fill=\"none\" points=\"138,-272.5 196,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"167\" y=\"-257.3\">output:</text>\n<polyline fill=\"none\" points=\"196,-249.5 196,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"254.5\" y=\"-268.8\">(None, 128, 300)</text>\n<polyline fill=\"none\" points=\"313,-249.5 313,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"371.5\" y=\"-268.8\">(None, 128, 200)</text>\n</g>\n<!-- 140407442653008&#45;&gt;140408139775184 -->\n<g class=\"edge\" id=\"edge2\">\n<title>140407442653008-&gt;140408139775184</title>\n<path d=\"M215,-332.3799C215,-324.1745 215,-314.7679 215,-305.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"218.5001,-305.784 215,-295.784 211.5001,-305.784 218.5001,-305.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140408139761168 -->\n<g class=\"node\" id=\"node5\">\n<title>140408139761168</title>\n<polygon fill=\"none\" points=\"220.5,-166.5 220.5,-212.5 607.5,-212.5 607.5,-166.5 220.5,-166.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"239.5\" y=\"-197.3\">dot</text>\n<polyline fill=\"none\" points=\"220.5,-189.5 258.5,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"239.5\" y=\"-174.3\">Dot</text>\n<polyline fill=\"none\" points=\"258.5,-166.5 258.5,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"287.5\" y=\"-197.3\">input:</text>\n<polyline fill=\"none\" points=\"258.5,-189.5 316.5,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"287.5\" y=\"-174.3\">output:</text>\n<polyline fill=\"none\" points=\"316.5,-166.5 316.5,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"418.5\" y=\"-185.8\">[(None, 128, 200), (None, 128)]</text>\n<polyline fill=\"none\" points=\"520.5,-166.5 520.5,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"564\" y=\"-185.8\">(None, 200)</text>\n</g>\n<!-- 140408139775184&#45;&gt;140408139761168 -->\n<g class=\"edge\" id=\"edge3\">\n<title>140408139775184-&gt;140408139761168</title>\n<path d=\"M270.1683,-249.4901C294.755,-239.2353 323.8812,-227.0872 349.4713,-216.414\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"350.8423,-219.6345 358.7244,-212.5547 348.1476,-213.1739 350.8423,-219.6345\" stroke=\"#000000\"/>\n</g>\n<!-- 140408140344080 -->\n<g class=\"node\" id=\"node4\">\n<title>140408140344080</title>\n<polygon fill=\"none\" points=\"448,-249.5 448,-295.5 778,-295.5 778,-249.5 448,-249.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"488\" y=\"-280.3\">input_4</text>\n<polyline fill=\"none\" points=\"448,-272.5 528,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"488\" y=\"-257.3\">InputLayer</text>\n<polyline fill=\"none\" points=\"528,-249.5 528,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"557\" y=\"-280.3\">input:</text>\n<polyline fill=\"none\" points=\"528,-272.5 586,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"557\" y=\"-257.3\">output:</text>\n<polyline fill=\"none\" points=\"586,-249.5 586,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"634\" y=\"-268.8\">[(None, 128)]</text>\n<polyline fill=\"none\" points=\"682,-249.5 682,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"730\" y=\"-268.8\">[(None, 128)]</text>\n</g>\n<!-- 140408140344080&#45;&gt;140408139761168 -->\n<g class=\"edge\" id=\"edge4\">\n<title>140408140344080-&gt;140408139761168</title>\n<path d=\"M557.8317,-249.4901C533.245,-239.2353 504.1188,-227.0872 478.5287,-216.414\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"479.8524,-213.1739 469.2756,-212.5547 477.1577,-219.6345 479.8524,-213.1739\" stroke=\"#000000\"/>\n</g>\n<!-- 140408139773456 -->\n<g class=\"node\" id=\"node6\">\n<title>140408139773456</title>\n<polygon fill=\"none\" points=\"269.5,-83.5 269.5,-129.5 558.5,-129.5 558.5,-83.5 269.5,-83.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"301.5\" y=\"-114.3\">dense_3</text>\n<polyline fill=\"none\" points=\"269.5,-106.5 333.5,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"301.5\" y=\"-91.3\">Dense</text>\n<polyline fill=\"none\" points=\"333.5,-83.5 333.5,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"362.5\" y=\"-114.3\">input:</text>\n<polyline fill=\"none\" points=\"333.5,-106.5 391.5,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"362.5\" y=\"-91.3\">output:</text>\n<polyline fill=\"none\" points=\"391.5,-83.5 391.5,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"435\" y=\"-102.8\">(None, 200)</text>\n<polyline fill=\"none\" points=\"478.5,-83.5 478.5,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"518.5\" y=\"-102.8\">(None, 16)</text>\n</g>\n<!-- 140408139761168&#45;&gt;140408139773456 -->\n<g class=\"edge\" id=\"edge5\">\n<title>140408139761168-&gt;140408139773456</title>\n<path d=\"M414,-166.3799C414,-158.1745 414,-148.7679 414,-139.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"417.5001,-139.784 414,-129.784 410.5001,-139.784 417.5001,-139.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140408089701520 -->\n<g class=\"node\" id=\"node7\">\n<title>140408089701520</title>\n<polygon fill=\"none\" points=\"277,-.5 277,-46.5 551,-46.5 551,-.5 277,-.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"309\" y=\"-31.3\">dense_4</text>\n<polyline fill=\"none\" points=\"277,-23.5 341,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"309\" y=\"-8.3\">Dense</text>\n<polyline fill=\"none\" points=\"341,-.5 341,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"370\" y=\"-31.3\">input:</text>\n<polyline fill=\"none\" points=\"341,-23.5 399,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"370\" y=\"-8.3\">output:</text>\n<polyline fill=\"none\" points=\"399,-.5 399,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"439\" y=\"-19.8\">(None, 16)</text>\n<polyline fill=\"none\" points=\"479,-.5 479,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"515\" y=\"-19.8\">(None, 3)</text>\n</g>\n<!-- 140408139773456&#45;&gt;140408089701520 -->\n<g class=\"edge\" id=\"edge6\">\n<title>140408139773456-&gt;140408089701520</title>\n<path d=\"M414,-83.3799C414,-75.1745 414,-65.7679 414,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"417.5001,-56.784 414,-46.784 410.5001,-56.784 417.5001,-56.784\" stroke=\"#000000\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "from IPython.display import SVG\n",
        "from keras.utils import vis_utils\n",
        "SVG(vis_utils.model_to_dot(model_4, show_shapes=True, show_layer_names=True, dpi=60).create(prog='dot', format='svg'))  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "MF_x9s6mYbkv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e30f6956-93ef-40e1-f17d-581d4d589e8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "22/22 [==============================] - 8s 194ms/step - loss: 0.5751 - accuracy: 0.5449 - val_loss: 0.5099 - val_accuracy: 0.6389\n",
            "Epoch 2/200\n",
            "22/22 [==============================] - 3s 151ms/step - loss: 0.4863 - accuracy: 0.6572 - val_loss: 0.4710 - val_accuracy: 0.6794\n",
            "Epoch 3/200\n",
            "22/22 [==============================] - 3s 151ms/step - loss: 0.4430 - accuracy: 0.6977 - val_loss: 0.4439 - val_accuracy: 0.7020\n",
            "Epoch 4/200\n",
            "22/22 [==============================] - 3s 150ms/step - loss: 0.4151 - accuracy: 0.7177 - val_loss: 0.4335 - val_accuracy: 0.7185\n",
            "Epoch 5/200\n",
            "22/22 [==============================] - 3s 151ms/step - loss: 0.3848 - accuracy: 0.7448 - val_loss: 0.4135 - val_accuracy: 0.7260\n",
            "Epoch 6/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 0.3583 - accuracy: 0.7697 - val_loss: 0.4112 - val_accuracy: 0.7327\n",
            "Epoch 7/200\n",
            "22/22 [==============================] - 3s 151ms/step - loss: 0.3302 - accuracy: 0.7896 - val_loss: 0.4162 - val_accuracy: 0.7260\n",
            "Epoch 8/200\n",
            "22/22 [==============================] - 3s 151ms/step - loss: 0.3066 - accuracy: 0.8107 - val_loss: 0.4269 - val_accuracy: 0.7267\n",
            "Epoch 9/200\n",
            "22/22 [==============================] - 3s 151ms/step - loss: 0.2857 - accuracy: 0.8279 - val_loss: 0.4257 - val_accuracy: 0.7387\n",
            "Epoch 10/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 0.2575 - accuracy: 0.8467 - val_loss: 0.4530 - val_accuracy: 0.7492\n",
            "Epoch 11/200\n",
            "22/22 [==============================] - 3s 151ms/step - loss: 0.2356 - accuracy: 0.8661 - val_loss: 0.4399 - val_accuracy: 0.7455\n",
            "Epoch 12/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 0.2135 - accuracy: 0.8785 - val_loss: 0.4778 - val_accuracy: 0.7380\n",
            "Epoch 13/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 0.1881 - accuracy: 0.8959 - val_loss: 0.5297 - val_accuracy: 0.7492\n",
            "Epoch 14/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 0.1682 - accuracy: 0.9110 - val_loss: 0.5298 - val_accuracy: 0.7350\n",
            "Epoch 15/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 0.1441 - accuracy: 0.9281 - val_loss: 0.5641 - val_accuracy: 0.7357\n",
            "Epoch 16/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 0.1284 - accuracy: 0.9381 - val_loss: 0.6349 - val_accuracy: 0.7050\n",
            "Epoch 17/200\n",
            "22/22 [==============================] - 3s 151ms/step - loss: 0.1068 - accuracy: 0.9560 - val_loss: 0.6395 - val_accuracy: 0.7222\n",
            "Epoch 18/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 0.0861 - accuracy: 0.9685 - val_loss: 0.7262 - val_accuracy: 0.7185\n",
            "Epoch 19/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 0.0681 - accuracy: 0.9777 - val_loss: 0.7331 - val_accuracy: 0.7312\n",
            "Epoch 20/200\n",
            "22/22 [==============================] - 3s 151ms/step - loss: 0.0532 - accuracy: 0.9852 - val_loss: 0.7682 - val_accuracy: 0.7282\n",
            "Epoch 21/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 0.0439 - accuracy: 0.9895 - val_loss: 0.8324 - val_accuracy: 0.7162\n",
            "Epoch 22/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 0.0389 - accuracy: 0.9906 - val_loss: 0.8873 - val_accuracy: 0.7132\n",
            "Epoch 23/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 0.0344 - accuracy: 0.9915 - val_loss: 0.9169 - val_accuracy: 0.7065\n",
            "Epoch 24/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 0.0258 - accuracy: 0.9947 - val_loss: 0.9868 - val_accuracy: 0.7140\n",
            "Epoch 25/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 0.0221 - accuracy: 0.9962 - val_loss: 0.9985 - val_accuracy: 0.7095\n",
            "Epoch 26/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 0.0196 - accuracy: 0.9964 - val_loss: 1.0328 - val_accuracy: 0.7147\n",
            "Epoch 27/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 0.0201 - accuracy: 0.9965 - val_loss: 1.0371 - val_accuracy: 0.7230\n",
            "Epoch 28/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 0.0264 - accuracy: 0.9938 - val_loss: 1.0022 - val_accuracy: 0.7192\n",
            "Epoch 29/200\n",
            "22/22 [==============================] - 3s 151ms/step - loss: 0.0205 - accuracy: 0.9961 - val_loss: 1.0294 - val_accuracy: 0.7230\n",
            "Epoch 30/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 0.0145 - accuracy: 0.9978 - val_loss: 1.0787 - val_accuracy: 0.7177\n",
            "Epoch 31/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 0.0101 - accuracy: 0.9985 - val_loss: 1.0896 - val_accuracy: 0.7177\n",
            "Epoch 32/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 0.0065 - accuracy: 0.9991 - val_loss: 1.0978 - val_accuracy: 0.7222\n",
            "Epoch 33/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 0.0057 - accuracy: 0.9991 - val_loss: 1.1553 - val_accuracy: 0.7095\n",
            "Epoch 34/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 0.0063 - accuracy: 0.9990 - val_loss: 1.1578 - val_accuracy: 0.7162\n",
            "Epoch 35/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 0.0062 - accuracy: 0.9995 - val_loss: 1.1911 - val_accuracy: 0.7170\n",
            "Epoch 36/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 0.0068 - accuracy: 0.9991 - val_loss: 1.2194 - val_accuracy: 0.7200\n",
            "Epoch 37/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 0.0053 - accuracy: 0.9992 - val_loss: 1.1867 - val_accuracy: 0.7252\n",
            "Epoch 38/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 0.0064 - accuracy: 0.9990 - val_loss: 1.2315 - val_accuracy: 0.7237\n",
            "Epoch 39/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 0.0056 - accuracy: 0.9993 - val_loss: 1.2847 - val_accuracy: 0.7162\n",
            "Epoch 40/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 0.0241 - accuracy: 0.9928 - val_loss: 1.2424 - val_accuracy: 0.7185\n",
            "Epoch 41/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 0.0477 - accuracy: 0.9831 - val_loss: 1.0653 - val_accuracy: 0.7207\n",
            "Epoch 42/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 0.0262 - accuracy: 0.9937 - val_loss: 1.1452 - val_accuracy: 0.7230\n",
            "Epoch 43/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 0.0133 - accuracy: 0.9979 - val_loss: 1.1596 - val_accuracy: 0.7305\n",
            "Epoch 44/200\n",
            "22/22 [==============================] - 3s 151ms/step - loss: 0.0091 - accuracy: 0.9986 - val_loss: 1.1692 - val_accuracy: 0.7335\n",
            "Epoch 45/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 0.0060 - accuracy: 0.9994 - val_loss: 1.1826 - val_accuracy: 0.7425\n",
            "Epoch 46/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 0.0035 - accuracy: 0.9995 - val_loss: 1.2157 - val_accuracy: 0.7357\n",
            "Epoch 47/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 1.2554 - val_accuracy: 0.7327\n",
            "Epoch 48/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 0.0023 - accuracy: 0.9996 - val_loss: 1.2797 - val_accuracy: 0.7237\n",
            "Epoch 49/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 0.0022 - accuracy: 0.9995 - val_loss: 1.3008 - val_accuracy: 0.7245\n",
            "Epoch 50/200\n",
            "22/22 [==============================] - 3s 154ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 1.3166 - val_accuracy: 0.7245\n",
            "Epoch 51/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 0.0019 - accuracy: 0.9994 - val_loss: 1.3432 - val_accuracy: 0.7335\n",
            "Epoch 52/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 1.3432 - val_accuracy: 0.7245\n",
            "Epoch 53/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 1.3656 - val_accuracy: 0.7215\n",
            "Epoch 54/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 1.3744 - val_accuracy: 0.7207\n",
            "Epoch 55/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 0.0025 - accuracy: 0.9996 - val_loss: 1.3698 - val_accuracy: 0.7282\n",
            "Epoch 56/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 1.4031 - val_accuracy: 0.7290\n",
            "Epoch 57/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 0.0013 - accuracy: 0.9996 - val_loss: 1.3893 - val_accuracy: 0.7252\n",
            "Epoch 58/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 0.0014 - accuracy: 0.9995 - val_loss: 1.4054 - val_accuracy: 0.7207\n",
            "Epoch 59/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 0.0015 - accuracy: 0.9994 - val_loss: 1.4190 - val_accuracy: 0.7222\n",
            "Epoch 60/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 0.0015 - accuracy: 0.9995 - val_loss: 1.4319 - val_accuracy: 0.7185\n",
            "Epoch 61/200\n",
            "22/22 [==============================] - 3s 154ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 1.4349 - val_accuracy: 0.7177\n",
            "Epoch 62/200\n",
            "22/22 [==============================] - 3s 154ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 1.4504 - val_accuracy: 0.7260\n",
            "Epoch 63/200\n",
            "22/22 [==============================] - 3s 154ms/step - loss: 0.0015 - accuracy: 0.9995 - val_loss: 1.4461 - val_accuracy: 0.7200\n",
            "Epoch 64/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 1.4533 - val_accuracy: 0.7155\n",
            "Epoch 65/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 0.0012 - accuracy: 0.9995 - val_loss: 1.4666 - val_accuracy: 0.7170\n",
            "Epoch 66/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 0.0015 - accuracy: 0.9995 - val_loss: 1.4739 - val_accuracy: 0.7230\n",
            "Epoch 67/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 0.0015 - accuracy: 0.9993 - val_loss: 1.4832 - val_accuracy: 0.7147\n",
            "Epoch 68/200\n",
            "22/22 [==============================] - 3s 151ms/step - loss: 0.0012 - accuracy: 0.9996 - val_loss: 1.4886 - val_accuracy: 0.7230\n",
            "Epoch 69/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 0.0013 - accuracy: 0.9996 - val_loss: 1.4914 - val_accuracy: 0.7192\n",
            "Epoch 70/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 0.0017 - accuracy: 0.9995 - val_loss: 1.5125 - val_accuracy: 0.7170\n",
            "Epoch 71/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 0.0012 - accuracy: 0.9995 - val_loss: 1.4956 - val_accuracy: 0.7215\n",
            "Epoch 72/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 0.0013 - accuracy: 0.9995 - val_loss: 1.5157 - val_accuracy: 0.7147\n",
            "Epoch 73/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 0.0011 - accuracy: 0.9996 - val_loss: 1.5285 - val_accuracy: 0.7282\n",
            "Epoch 74/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 0.0012 - accuracy: 0.9996 - val_loss: 1.5336 - val_accuracy: 0.7162\n",
            "Epoch 75/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 0.0012 - accuracy: 0.9996 - val_loss: 1.5199 - val_accuracy: 0.7177\n",
            "Epoch 76/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 0.0012 - accuracy: 0.9995 - val_loss: 1.5349 - val_accuracy: 0.7170\n",
            "Epoch 77/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 0.0011 - accuracy: 0.9995 - val_loss: 1.5401 - val_accuracy: 0.7177\n",
            "Epoch 78/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 0.0012 - accuracy: 0.9996 - val_loss: 1.5434 - val_accuracy: 0.7140\n",
            "Epoch 79/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 0.0011 - accuracy: 0.9994 - val_loss: 1.5560 - val_accuracy: 0.7147\n",
            "Epoch 80/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 0.0012 - accuracy: 0.9996 - val_loss: 1.5573 - val_accuracy: 0.7162\n",
            "Epoch 81/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 0.0014 - accuracy: 0.9995 - val_loss: 1.5711 - val_accuracy: 0.7170\n",
            "Epoch 82/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 1.5738 - val_accuracy: 0.7215\n",
            "Epoch 83/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 0.0012 - accuracy: 0.9995 - val_loss: 1.5626 - val_accuracy: 0.7170\n",
            "Epoch 84/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 9.3594e-04 - accuracy: 0.9996 - val_loss: 1.5711 - val_accuracy: 0.7192\n",
            "Epoch 85/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 0.0017 - accuracy: 0.9995 - val_loss: 1.5899 - val_accuracy: 0.7125\n",
            "Epoch 86/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 9.9773e-04 - accuracy: 0.9996 - val_loss: 1.5667 - val_accuracy: 0.7177\n",
            "Epoch 87/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 9.6242e-04 - accuracy: 0.9996 - val_loss: 1.5863 - val_accuracy: 0.7162\n",
            "Epoch 88/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 0.0011 - accuracy: 0.9996 - val_loss: 1.5937 - val_accuracy: 0.7117\n",
            "Epoch 89/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 0.0010 - accuracy: 0.9996 - val_loss: 1.5855 - val_accuracy: 0.7117\n",
            "Epoch 90/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 0.0013 - accuracy: 0.9995 - val_loss: 1.5998 - val_accuracy: 0.7140\n",
            "Epoch 91/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 9.2844e-04 - accuracy: 0.9996 - val_loss: 1.6295 - val_accuracy: 0.7252\n",
            "Epoch 92/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 9.0865e-04 - accuracy: 0.9996 - val_loss: 1.6198 - val_accuracy: 0.7155\n",
            "Epoch 93/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 0.0012 - accuracy: 0.9996 - val_loss: 1.6156 - val_accuracy: 0.7170\n",
            "Epoch 94/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 0.0010 - accuracy: 0.9994 - val_loss: 1.6197 - val_accuracy: 0.7132\n",
            "Epoch 95/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 0.0010 - accuracy: 0.9995 - val_loss: 1.6249 - val_accuracy: 0.7110\n",
            "Epoch 96/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 8.7208e-04 - accuracy: 0.9996 - val_loss: 1.6387 - val_accuracy: 0.7207\n",
            "Epoch 97/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 8.1045e-04 - accuracy: 0.9996 - val_loss: 1.6316 - val_accuracy: 0.7192\n",
            "Epoch 98/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 0.0011 - accuracy: 0.9996 - val_loss: 1.6681 - val_accuracy: 0.7215\n",
            "Epoch 99/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 8.8755e-04 - accuracy: 0.9996 - val_loss: 1.6450 - val_accuracy: 0.7125\n",
            "Epoch 100/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 9.8856e-04 - accuracy: 0.9995 - val_loss: 1.6559 - val_accuracy: 0.7177\n",
            "Epoch 101/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 0.0013 - accuracy: 0.9995 - val_loss: 1.6418 - val_accuracy: 0.7155\n",
            "Epoch 102/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 9.2599e-04 - accuracy: 0.9995 - val_loss: 1.6521 - val_accuracy: 0.7140\n",
            "Epoch 103/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 8.1215e-04 - accuracy: 0.9996 - val_loss: 1.6599 - val_accuracy: 0.7147\n",
            "Epoch 104/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 9.9376e-04 - accuracy: 0.9996 - val_loss: 1.6539 - val_accuracy: 0.7132\n",
            "Epoch 105/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 9.8258e-04 - accuracy: 0.9995 - val_loss: 1.6722 - val_accuracy: 0.7110\n",
            "Epoch 106/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 7.0932e-04 - accuracy: 0.9996 - val_loss: 1.6675 - val_accuracy: 0.7110\n",
            "Epoch 107/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 0.0011 - accuracy: 0.9996 - val_loss: 1.6707 - val_accuracy: 0.7207\n",
            "Epoch 108/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 8.3086e-04 - accuracy: 0.9996 - val_loss: 1.6987 - val_accuracy: 0.7117\n",
            "Epoch 109/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 0.0012 - accuracy: 0.9995 - val_loss: 1.6742 - val_accuracy: 0.7192\n",
            "Epoch 110/200\n",
            "22/22 [==============================] - 3s 154ms/step - loss: 0.0010 - accuracy: 0.9996 - val_loss: 1.6915 - val_accuracy: 0.7132\n",
            "Epoch 111/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 9.1404e-04 - accuracy: 0.9996 - val_loss: 1.6825 - val_accuracy: 0.7125\n",
            "Epoch 112/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 7.9314e-04 - accuracy: 0.9995 - val_loss: 1.6900 - val_accuracy: 0.7117\n",
            "Epoch 113/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 8.6738e-04 - accuracy: 0.9996 - val_loss: 1.6895 - val_accuracy: 0.7147\n",
            "Epoch 114/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 9.7524e-04 - accuracy: 0.9996 - val_loss: 1.7124 - val_accuracy: 0.7170\n",
            "Epoch 115/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 0.0010 - accuracy: 0.9996 - val_loss: 1.7050 - val_accuracy: 0.7140\n",
            "Epoch 116/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 7.6782e-04 - accuracy: 0.9996 - val_loss: 1.6969 - val_accuracy: 0.7177\n",
            "Epoch 117/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 0.0010 - accuracy: 0.9996 - val_loss: 1.7108 - val_accuracy: 0.7132\n",
            "Epoch 118/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 6.4495e-04 - accuracy: 0.9997 - val_loss: 1.7065 - val_accuracy: 0.7140\n",
            "Epoch 119/200\n",
            "22/22 [==============================] - 3s 154ms/step - loss: 0.0012 - accuracy: 0.9994 - val_loss: 1.7058 - val_accuracy: 0.7117\n",
            "Epoch 120/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 8.2692e-04 - accuracy: 0.9995 - val_loss: 1.7190 - val_accuracy: 0.7170\n",
            "Epoch 121/200\n",
            "22/22 [==============================] - 3s 154ms/step - loss: 9.3868e-04 - accuracy: 0.9994 - val_loss: 1.7163 - val_accuracy: 0.7102\n",
            "Epoch 122/200\n",
            "22/22 [==============================] - 3s 154ms/step - loss: 8.0051e-04 - accuracy: 0.9994 - val_loss: 1.7168 - val_accuracy: 0.7132\n",
            "Epoch 123/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 8.9736e-04 - accuracy: 0.9996 - val_loss: 1.7249 - val_accuracy: 0.7125\n",
            "Epoch 124/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 0.0014 - accuracy: 0.9995 - val_loss: 1.7261 - val_accuracy: 0.7162\n",
            "Epoch 125/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 7.9345e-04 - accuracy: 0.9996 - val_loss: 1.7423 - val_accuracy: 0.7125\n",
            "Epoch 126/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 8.8189e-04 - accuracy: 0.9996 - val_loss: 1.7365 - val_accuracy: 0.7132\n",
            "Epoch 127/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 0.0013 - accuracy: 0.9996 - val_loss: 1.7346 - val_accuracy: 0.7117\n",
            "Epoch 128/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 0.0011 - accuracy: 0.9996 - val_loss: 1.7129 - val_accuracy: 0.7170\n",
            "Epoch 129/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 9.4061e-04 - accuracy: 0.9996 - val_loss: 1.7395 - val_accuracy: 0.7132\n",
            "Epoch 130/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 0.0010 - accuracy: 0.9994 - val_loss: 1.7375 - val_accuracy: 0.7095\n",
            "Epoch 131/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 7.9829e-04 - accuracy: 0.9996 - val_loss: 1.7588 - val_accuracy: 0.7102\n",
            "Epoch 132/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 9.3090e-04 - accuracy: 0.9995 - val_loss: 1.7305 - val_accuracy: 0.7140\n",
            "Epoch 133/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 8.6053e-04 - accuracy: 0.9994 - val_loss: 1.7407 - val_accuracy: 0.7177\n",
            "Epoch 134/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 6.8616e-04 - accuracy: 0.9996 - val_loss: 1.7575 - val_accuracy: 0.7125\n",
            "Epoch 135/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 8.3830e-04 - accuracy: 0.9996 - val_loss: 1.7685 - val_accuracy: 0.7065\n",
            "Epoch 136/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 9.2183e-04 - accuracy: 0.9995 - val_loss: 1.7613 - val_accuracy: 0.7170\n",
            "Epoch 137/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 9.4709e-04 - accuracy: 0.9996 - val_loss: 1.7533 - val_accuracy: 0.7147\n",
            "Epoch 138/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 6.1834e-04 - accuracy: 0.9995 - val_loss: 1.7751 - val_accuracy: 0.7110\n",
            "Epoch 139/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 7.7530e-04 - accuracy: 0.9996 - val_loss: 1.7623 - val_accuracy: 0.7117\n",
            "Epoch 140/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 0.0011 - accuracy: 0.9994 - val_loss: 1.7760 - val_accuracy: 0.7072\n",
            "Epoch 141/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 0.0011 - accuracy: 0.9995 - val_loss: 1.7717 - val_accuracy: 0.7125\n",
            "Epoch 142/200\n",
            "22/22 [==============================] - 3s 154ms/step - loss: 9.4532e-04 - accuracy: 0.9995 - val_loss: 1.7691 - val_accuracy: 0.7117\n",
            "Epoch 143/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 0.0011 - accuracy: 0.9996 - val_loss: 1.7780 - val_accuracy: 0.7170\n",
            "Epoch 144/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 5.9584e-04 - accuracy: 0.9995 - val_loss: 1.7795 - val_accuracy: 0.7125\n",
            "Epoch 145/200\n",
            "22/22 [==============================] - 3s 154ms/step - loss: 9.2415e-04 - accuracy: 0.9994 - val_loss: 1.7716 - val_accuracy: 0.7140\n",
            "Epoch 146/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 7.3635e-04 - accuracy: 0.9996 - val_loss: 1.7876 - val_accuracy: 0.7117\n",
            "Epoch 147/200\n",
            "22/22 [==============================] - 3s 154ms/step - loss: 8.3995e-04 - accuracy: 0.9996 - val_loss: 1.7762 - val_accuracy: 0.7147\n",
            "Epoch 148/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 6.9859e-04 - accuracy: 0.9994 - val_loss: 1.8002 - val_accuracy: 0.7110\n",
            "Epoch 149/200\n",
            "22/22 [==============================] - 3s 154ms/step - loss: 7.8075e-04 - accuracy: 0.9996 - val_loss: 1.7840 - val_accuracy: 0.7110\n",
            "Epoch 150/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 8.4342e-04 - accuracy: 0.9994 - val_loss: 1.7975 - val_accuracy: 0.7087\n",
            "Epoch 151/200\n",
            "22/22 [==============================] - 3s 154ms/step - loss: 9.7151e-04 - accuracy: 0.9995 - val_loss: 1.7874 - val_accuracy: 0.7162\n",
            "Epoch 152/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 6.6459e-04 - accuracy: 0.9996 - val_loss: 1.8088 - val_accuracy: 0.7102\n",
            "Epoch 153/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 7.0450e-04 - accuracy: 0.9996 - val_loss: 1.8125 - val_accuracy: 0.7050\n",
            "Epoch 154/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 6.1218e-04 - accuracy: 0.9995 - val_loss: 1.7977 - val_accuracy: 0.7147\n",
            "Epoch 155/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 9.3240e-04 - accuracy: 0.9994 - val_loss: 1.8079 - val_accuracy: 0.7110\n",
            "Epoch 156/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 6.7073e-04 - accuracy: 0.9997 - val_loss: 1.8168 - val_accuracy: 0.7102\n",
            "Epoch 157/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 6.9170e-04 - accuracy: 0.9995 - val_loss: 1.8284 - val_accuracy: 0.7042\n",
            "Epoch 158/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 8.5815e-04 - accuracy: 0.9996 - val_loss: 1.8165 - val_accuracy: 0.7162\n",
            "Epoch 159/200\n",
            "22/22 [==============================] - 3s 154ms/step - loss: 7.0967e-04 - accuracy: 0.9995 - val_loss: 1.8318 - val_accuracy: 0.7102\n",
            "Epoch 160/200\n",
            "22/22 [==============================] - 3s 154ms/step - loss: 7.7229e-04 - accuracy: 0.9996 - val_loss: 1.8272 - val_accuracy: 0.7035\n",
            "Epoch 161/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 9.2936e-04 - accuracy: 0.9996 - val_loss: 1.8235 - val_accuracy: 0.7155\n",
            "Epoch 162/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 8.2283e-04 - accuracy: 0.9996 - val_loss: 1.8386 - val_accuracy: 0.7087\n",
            "Epoch 163/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 8.4456e-04 - accuracy: 0.9995 - val_loss: 1.8084 - val_accuracy: 0.7102\n",
            "Epoch 164/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 5.1484e-04 - accuracy: 0.9997 - val_loss: 1.8425 - val_accuracy: 0.7087\n",
            "Epoch 165/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 9.3779e-04 - accuracy: 0.9996 - val_loss: 1.8308 - val_accuracy: 0.7155\n",
            "Epoch 166/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 7.0383e-04 - accuracy: 0.9993 - val_loss: 1.8462 - val_accuracy: 0.7110\n",
            "Epoch 167/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 9.1586e-04 - accuracy: 0.9994 - val_loss: 1.8371 - val_accuracy: 0.7080\n",
            "Epoch 168/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 8.5905e-04 - accuracy: 0.9996 - val_loss: 1.8438 - val_accuracy: 0.7147\n",
            "Epoch 169/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 7.3405e-04 - accuracy: 0.9994 - val_loss: 1.8501 - val_accuracy: 0.7095\n",
            "Epoch 170/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 5.8528e-04 - accuracy: 0.9995 - val_loss: 1.8448 - val_accuracy: 0.7140\n",
            "Epoch 171/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 6.7776e-04 - accuracy: 0.9996 - val_loss: 1.8555 - val_accuracy: 0.7102\n",
            "Epoch 172/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 6.9156e-04 - accuracy: 0.9995 - val_loss: 1.8456 - val_accuracy: 0.7132\n",
            "Epoch 173/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 7.2534e-04 - accuracy: 0.9996 - val_loss: 1.8604 - val_accuracy: 0.7080\n",
            "Epoch 174/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 7.0960e-04 - accuracy: 0.9995 - val_loss: 1.8625 - val_accuracy: 0.7087\n",
            "Epoch 175/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 0.0012 - accuracy: 0.9995 - val_loss: 1.8541 - val_accuracy: 0.7057\n",
            "Epoch 176/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 9.0851e-04 - accuracy: 0.9996 - val_loss: 1.8369 - val_accuracy: 0.7162\n",
            "Epoch 177/200\n",
            "22/22 [==============================] - 3s 154ms/step - loss: 5.5662e-04 - accuracy: 0.9996 - val_loss: 1.8841 - val_accuracy: 0.7095\n",
            "Epoch 178/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 7.7220e-04 - accuracy: 0.9996 - val_loss: 1.8640 - val_accuracy: 0.7087\n",
            "Epoch 179/200\n",
            "22/22 [==============================] - 3s 154ms/step - loss: 5.8212e-04 - accuracy: 0.9995 - val_loss: 1.8713 - val_accuracy: 0.7072\n",
            "Epoch 180/200\n",
            "22/22 [==============================] - 3s 154ms/step - loss: 7.0902e-04 - accuracy: 0.9996 - val_loss: 1.8714 - val_accuracy: 0.7095\n",
            "Epoch 181/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 8.8685e-04 - accuracy: 0.9995 - val_loss: 1.8702 - val_accuracy: 0.7072\n",
            "Epoch 182/200\n",
            "22/22 [==============================] - 3s 154ms/step - loss: 5.0274e-04 - accuracy: 0.9996 - val_loss: 1.8899 - val_accuracy: 0.7080\n",
            "Epoch 183/200\n",
            "22/22 [==============================] - 3s 154ms/step - loss: 7.8778e-04 - accuracy: 0.9996 - val_loss: 1.8782 - val_accuracy: 0.7080\n",
            "Epoch 184/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 7.2649e-04 - accuracy: 0.9995 - val_loss: 1.8810 - val_accuracy: 0.7087\n",
            "Epoch 185/200\n",
            "22/22 [==============================] - 3s 154ms/step - loss: 6.2340e-04 - accuracy: 0.9995 - val_loss: 1.8788 - val_accuracy: 0.7065\n",
            "Epoch 186/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 7.6095e-04 - accuracy: 0.9994 - val_loss: 1.8825 - val_accuracy: 0.7072\n",
            "Epoch 187/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 7.6737e-04 - accuracy: 0.9995 - val_loss: 1.8877 - val_accuracy: 0.7117\n",
            "Epoch 188/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 5.9765e-04 - accuracy: 0.9995 - val_loss: 1.8991 - val_accuracy: 0.7102\n",
            "Epoch 189/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 7.0694e-04 - accuracy: 0.9995 - val_loss: 1.8923 - val_accuracy: 0.7065\n",
            "Epoch 190/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 6.3438e-04 - accuracy: 0.9996 - val_loss: 1.8954 - val_accuracy: 0.7117\n",
            "Epoch 191/200\n",
            "22/22 [==============================] - 3s 154ms/step - loss: 7.5100e-04 - accuracy: 0.9995 - val_loss: 1.8888 - val_accuracy: 0.7102\n",
            "Epoch 192/200\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 5.1082e-04 - accuracy: 0.9996 - val_loss: 1.9084 - val_accuracy: 0.7065\n",
            "Epoch 193/200\n",
            "22/22 [==============================] - 3s 154ms/step - loss: 7.6642e-04 - accuracy: 0.9995 - val_loss: 1.8984 - val_accuracy: 0.7087\n",
            "Epoch 194/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 7.0821e-04 - accuracy: 0.9996 - val_loss: 1.9016 - val_accuracy: 0.7035\n",
            "Epoch 195/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 7.2464e-04 - accuracy: 0.9995 - val_loss: 1.9027 - val_accuracy: 0.7065\n",
            "Epoch 196/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 7.0588e-04 - accuracy: 0.9996 - val_loss: 1.8854 - val_accuracy: 0.7080\n",
            "Epoch 197/200\n",
            "22/22 [==============================] - 3s 153ms/step - loss: 7.1443e-04 - accuracy: 0.9996 - val_loss: 1.9038 - val_accuracy: 0.7095\n",
            "Epoch 198/200\n",
            "22/22 [==============================] - 3s 154ms/step - loss: 5.6815e-04 - accuracy: 0.9996 - val_loss: 1.9020 - val_accuracy: 0.7050\n",
            "Epoch 199/200\n",
            "22/22 [==============================] - 3s 154ms/step - loss: 9.2475e-04 - accuracy: 0.9996 - val_loss: 1.9013 - val_accuracy: 0.7050\n",
            "Epoch 200/200\n",
            "22/22 [==============================] - 3s 154ms/step - loss: 0.0541 - accuracy: 0.9886 - val_loss: 2.2623 - val_accuracy: 0.6577\n",
            "42/42 [==============================] - 1s 18ms/step - loss: 2.1852 - accuracy: 0.6654\n",
            "Loss: 2.1852259635925293 and Accuracy: 0.6654191613197327\n"
          ]
        }
      ],
      "source": [
        "history4 = model_4.fit([x_train_review_pad_glove, x_train_aspect_mask_pad], y_train,\n",
        "                    epochs=200,\n",
        "                    batch_size=512,\n",
        "                    validation_data=([x_dev_review_pad_glove, x_dev_aspect_mask_pad], y_dev),\n",
        "                    verbose=1)\n",
        "\n",
        "results4 = model_4.evaluate([x_test_review_pad_glove, x_test_aspect_mask_pad], y_test)\n",
        "\n",
        "print(\"Loss: {} and Accuracy: {}\".format(results4[0], results4[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0DwhCH4l4Np"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "NN & NLP LAB 4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}